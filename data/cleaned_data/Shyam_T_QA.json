[
    {
        "X": "What is the name of the pruning method that globally prunes tensors corresponding to all parameters inparameters?",
        "Y": "prune.global_unstructured",
        "Z": "Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Prunes tensor corresponding to parameter callednamein?",
        "Y": "prune.custom_from_mask",
        "Z": "Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is removed from current unpruned units selected at random?",
        "Y": "specifiedamountof",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What prune.global_unstructured is globally prunes tensors corresponding to all parameters inparameters?",
        "Y": "prune.global_unstructured",
        "Z": "prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the pruning method used in container holding a sequence of pruning methods for?",
        "Y": "iterative pruning",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask?",
        "Y": "prune.custom_from_mask",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying what?",
        "Y": "pre-computed mask inmask",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is applied to prune.global_unstructured Globally prunes tensors corresponding to all parameters inparameters?",
        "Y": "specifiedpruning_method",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What removes the pruning reparameterization from a module and the pruning method?",
        "Y": "prune.remove",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "At what rate do prune.RandomUnstructured Prune units in a tensor?",
        "Y": "random",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.l1_unstructured Prunes tensor removing the specifiedamountof (currently unpruned",
        "Y": "L1-norm",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are currently unpruned channels along the specifieddimselected at random?",
        "Y": "specifiedamountof",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying what?",
        "Y": "specifiedpruning_method",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is applied to prune.custom_from_mask?",
        "Y": "pre-computed mask inmask",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What check whethermoduleis pruned by looking forforward?",
        "Y": "prune.is_pruned",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prune (currently unpruned) units in a tensor at what?",
        "Y": "random",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest what?",
        "Y": "L1-norm",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prune entire (currently unpruned) channels in a tensor at what?",
        "Y": "random",
        "Z": "Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prune entire (currently unpruned) channels in a tensor based on what?",
        "Y": "Ln-norm",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is used to prune tensors corresponding to all parameters inparameters?",
        "Y": "specifiedpruning_method",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What check whethermoduleis pruned by looking forforward_pre_hooksin its modules?",
        "Y": "prune.is_pruned",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What do prune.is_pruned modules inherit from?",
        "Y": "theBasePruningMethod",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Applies what to a parameter based on their Ln-norm?",
        "Y": "weight normalization to a parameter",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How does prune.is_pruned check whethermoduleis pruned?",
        "Y": "by looking forforward_pre_hooks",
        "Z": "prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Applies what to a parameter in the given module?",
        "Y": "weight normalization",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.remove do?",
        "Y": "prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What removes the spectral normalization?",
        "Y": "Removes the spectral",
        "Z": "prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.",
        "Y": "prune.ln_structured",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lowest Ln-norm?",
        "Y": "L1-norm",
        "Z": "Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What method is used to prune tensors corresponding to all parameters inparameters?",
        "Y": "specifiedpruning_method",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module. Utility functions in other modules nn.utils.rnn.PackedSequence",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.is_pruned check whethermoduleis pruned by?",
        "Y": "looking forforward_pre_hooks",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Removes what from a module?",
        "Y": "spectral normalization reparameterization",
        "Z": "Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of _unstructured Prunes tensor is prune?",
        "Y": "random",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is pruned by in modules that inherit from theBasePruningMethod?",
        "Y": "looking forforward_pre_hooks",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of normalization reparameterization is removed from a module?",
        "Y": "spectral",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module. Utility functions in other modules nn.utils.rnn.PackedSequence",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is implemented using the new parametrization functionality intorch.nn.utils?",
        "Y": "Parametrizations",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.CustomFromMask prune.identity Applies to the tensor corresponding to the parameter callednameinmodul",
        "Y": "pruning reparametrization",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of _unstructured Prunes tensor is pruned?",
        "Y": "random",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prune channels based on their Ln-norm. prune.global_unstructured Globally prunes tensors",
        "Y": "specifiedpruning_method",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is implemented using the new parametrization functionality intorch.nn.utils.parameterize.register_paramete?",
        "Y": "Parametrizations",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of reparametrization does prune.customFromMask apply?",
        "Y": "pruning",
        "Z": "prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How does prune.is_pruned check whethermodule is pruned?",
        "Y": "by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.is_pruned Applies to a parameter in the given module?",
        "Y": "weight normalization",
        "Z": "prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.customFromMask apply to a parameter in the given module?",
        "Y": "spectral normalization",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What reparameterization does prune.remove remove from a module?",
        "Y": "spectral normalization",
        "Z": "prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Parametrizations implemented using the new parametrization functionality?",
        "Y": "intorch.nn.utils.parameterize.register_parametrization()",
        "Z": "prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does.spectral_norm apply spectral normalization to a parameter in the given module?",
        "Y": "parametrizations",
        "Z": "prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.identity apply to the tensor corresponding to the parameter callednameinmodule without actually pruning any units?",
        "Y": "pruning reparametrization",
        "Z": "prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMeth",
        "Y": "prune.is_pruned",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is applied to a parameter in the given module?",
        "Y": "spectral normalization",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does intorch.nn.utils.parameterize.register_parametrization() implement?",
        "Y": "parametrizations",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is implemented using the new parametrization functionality intorch.nn.utils.parameterize.register_parametrization()",
        "Y": "Parametrizations",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module. Utility functions in other modules nn.utils.rnn.PackedSequence",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is currently unpruned channels along the specifieddimselected at random?",
        "Y": "specifiedamountof",
        "Z": "prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules. Utility",
        "Y": "parametrizations.spectral_norm",
        "Z": "prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is used to parametrize Tensors on existing Modules?",
        "Y": "Utility functions",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module. Utility functions in other modules nn.utils.rnn.PackedSequence",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Utility functions to parametrize Tensors on existing Modules can be used to what?",
        "Y": "parametrize a given Parameter or Buffer",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module. Utility functions in other modules nn.utils.rnn.PackedSequence",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What would transform an object into a parameter?",
        "Y": "parameterizations",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module. Utility functions in other modules nn.utils.rnn.PackedSequence",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "For more information on how to implement your own parametrizations, see what?",
        "Y": "Parametrizationstutorial",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module. Utility functions in other modules nn.utils.rnn.PackedSequence",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prunes tensor removing the specifiedamountof (currently unpruned) units with the lowest what?",
        "Y": "L1-norm",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules. What",
        "Y": "parametrizations.spectral_norm",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What parametrize.register_paramet?",
        "Y": "parametrize.register_paramet",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Applies what normalization to a parameter in the given module?",
        "Y": "spectral",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the Context manager that enables the caching system within parametrizations registered withregister_parametrization()?",
        "Y": "parametrize.cached",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module. Utility functions in other modules nn.utils.rnn.PackedSequence",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What parametrize.is_parametrized ReturnsTrueif module has an active parametrization?",
        "Y": "parametrize.is_parametrized ReturnsTrueif module has an active parametrization",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.is_pruned do?",
        "Y": "prune.is_pruned",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does parametrize.is_parametrized return?",
        "Y": "ReturnsTrueif module has an active parametrization",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module. Utility functions in other modules nn.utils.rnn.PackedSequence",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are Utility functions in other modules?",
        "Y": "Utility functions in other modules",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module. Utility functions in other modules nn.utils.rnn.PackedSequence",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Pytorch Hub is a pre-trained model repository designed to facilitate what?",
        "Y": "research reproducibility",
        "Z": "Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility. Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load().",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How does the code snippet specify an entrypoint forresnet18model?",
        "Y": "if we expand the implementation inpytorch/vision/hubconf.py",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "In most cases importing the right function what is sufficient?",
        "Y": "inhubconf.py",
        "Z": "Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility. Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load().",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Why did we use the expanded version as an example?",
        "Y": "to show how it works",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What are the allowed arguments in the model?",
        "Y": "positional/keyword arguments",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is highly recommended to add to Pytorch Hub?",
        "Y": "a few examples",
        "Z": "Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility. Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load().",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Callables prefixed with underscore are considered as what?",
        "Y": "helper functions",
        "Z": "Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where can pretrained weights be stored?",
        "Y": "github repo",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the minimum size of a pre-trained model?",
        "Y": "2GB",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "The published models should be at least in a what?",
        "Y": "branch/tag",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Pytorch Hub can't be a what?",
        "Y": "random commit",
        "Z": "Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility. Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load().",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What provides convenient APIs to explore all available models in hub throughtt?",
        "Y": "Pytorch Hub",
        "Z": "Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility. Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load().",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Who supports publishing pre-trained models to a github repository?",
        "Y": "Pytorch Hub",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does the code snippet specifies if we expand the implementation inpytorch/vision/hubconf.py?",
        "Y": "an entrypoint forresnet18model",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Why do we use the expanded version as an example?",
        "Y": "to show how it works",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is an example of an auxiliary tool to make the user workflow smoother?",
        "Y": "tokenizers",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the minimum size of a pretrained model?",
        "Y": "2GB",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Pytorch Hub supports publishing pre-trained models to a github repository. It can't be a what?",
        "Y": "random commit",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "hubconf.pycan have what?",
        "Y": "multiple entrypoints",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Docstring of the function works as a help message. It explains what are the allowed?",
        "Y": "positional/keyword arguments",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from",
        "Y": "Pretrained weights",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If less than 2GB, it\u2019s recommended to what?",
        "Y": "attach it to aproject release",
        "Z": "Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Can't be a branch/tag?",
        "Y": "random commit",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What provides convenient APIs to explore all available models in hub?",
        "Y": "Pytorch Hub",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "List all entrypoints available which in github hubconf?",
        "Y": "in github hubconf",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is github(string) \u2013 a string?",
        "Y": "github(string) \u2013 a string",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What might be slightly different from dependencies variable?",
        "Y": "dependencies required for training a model",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Entrypoint function can return a model(nn.module), or what to make the user workflow smoother?",
        "Y": "auxiliary tools",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "The published models should be at least in a branch/tag. It can\u2019t be a what?",
        "Y": "random commit",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "List all entrypoints available what?",
        "Y": "in github hubconf",
        "Z": "argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel.",
        "Y": "Default is False",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Example Show the docstring of what?",
        "Y": "entrypointmodel",
        "Z": "Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a string with format repo_owner/repo_name[:tag_name]?",
        "Y": "github(string)",
        "Z": "argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Docstring of the function explains what are the allowed?",
        "Y": "positional/keyword arguments",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is highly recommended to add to the Docstring?",
        "Y": "a few examples",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "List what available in github hubconf?",
        "Y": "all entrypoints",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where is the default branch ismasterif not specified?",
        "Y": "github",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the function that determines whether to discard the existing cache and force a fresh download?",
        "Y": "force_reload(bool,optional)",
        "Z": "argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a list of available entrypoint names entrypoints?",
        "Y": "Default is False",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a string of entry?",
        "Y": "model(string)",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can't be a branch/tag?",
        "Y": "random commit",
        "Z": "Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What function does Pytorch use to force a fresh download?",
        "Y": "force_reload",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional)",
        "Y": "model(string)",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool",
        "Y": "is False",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is it recommended to do if the weight is less than 2GB?",
        "Y": "attach it to aproject release",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Can\u2019t be a what?",
        "Y": "random commit",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where can you find a string with format repo_owner/repo_name[:tag_name]>?",
        "Y": "github",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does if sourceis'github' mean?",
        "Y": "if sourceis'github'",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for a github repo or a local directory?",
        "Y": "is False",
        "Z": "github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "if sourceis'local',repo_or_diris expected to be a what?",
        "Y": "path to a local directory",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "repo_or_dir(string) \u2013 what is repo_owner/repo_name[:tag_name]?",
        "Y": "repo name",
        "Z": "github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(boo",
        "Y": "source(string,optional) \u2013'github'|'local'",
        "Z": "entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does source(string,optional) -'github'|'local' specify?",
        "Y": "how repo_or_dir is to be interpreted",
        "Z": "a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default for a github repo?",
        "Y": "github",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the function to force a fresh download of the github repo unconditionally?",
        "Y": "force_reload",
        "Z": "Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does verbose(bool,optional) mute messages about?",
        "Y": "hitting local caches",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "The docstring of what entrypoint Example Show the docstring of. github(string) \u2013 a string with format",
        "Y": "entrypointmodel",
        "Z": "entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does source(string,optional) Specify?",
        "Y": "how repo_or_dir is to be interpreted",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for force_reload(bool,optional)?",
        "Y": "Default is False",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does the verbose(bool,optional) do?",
        "Y": "mute messages about hitting local caches",
        "Z": "entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the message about hitting local caches?",
        "Y": "first",
        "Z": "entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does source(string,optional) \u2013'github'|'local'?",
        "Y": "source(string,optional) \u2013'github'|'local'",
        "Z": "Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What cannot be muted?",
        "Y": "first download",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does source(string,optional) \u2013 'github'|'local'?",
        "Y": "source(string,optional) \u2013'github'|'local'",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What default is 'github'?",
        "Y": "Default",
        "Z": "github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What default is used to force a fresh download of the github repo unconditionally?",
        "Y": "is False",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does verbose(bool,optional) do?",
        "Y": "mute messages about hitting local caches",
        "Z": "github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for model(string)?",
        "Y": "is False",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Example Load a model from what?",
        "Y": "a github repo or a local directory",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "repo_or_dir(string) \u2013 what does repo_owner/repo_name[:tag_name] mean?",
        "Y": "repo name",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Default is True. verbose(bool,optional) \u2013 If False, mute messages about hitting local",
        "Y": "local",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What type of object can be found at the given URL?",
        "Y": "Example Download object",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does source(string,optional) Specifies how repo_or_dir is to be interpreted?",
        "Y": "source(string,optional) \u2013'github'|'local'",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the option to force a fresh download of the github repo unconditionally?",
        "Y": "force_reload",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Force_reload(bool,optional) \u2013 whether to force a fresh download of the github repo unconditionally. What",
        "Y": "Does not have any effect if source='local'",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Default is what?",
        "Y": "True",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What object at the given URL to a local path?",
        "Y": "Example Download object",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for progress(bool,optional) \u2013 whether or not to display a progress bar to stderr",
        "Y": "None",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If downloaded file is a what, it will be automatically decompressed. If the object is already present inmodel_dir, it\u2019s des",
        "Y": "zip file",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the function that can be used to force a fresh download of the github repo unconditionally?",
        "Y": "force_reload",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "url(string) \u2013 URL of the object to download dst(string) \u2013 what?",
        "Y": "Full path where object will be saved",
        "Z": "url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If downloaded file is a what?",
        "Y": "zip file",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Example Download object at what?",
        "Y": "given URL to a local path",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Default: None what \u2013 whether or not to display a progress bar to stderr Default: True Example Loads",
        "Y": "progress(bool,optional)",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Check_hash(bool,optional) \u2013 whether or not to display a progress bar to stderr. Default",
        "Y": "True",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the option to force a fresh download of the github repo unconditionally?",
        "Y": "force_reload",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What do verbose(bool,optional) mute messages about?",
        "Y": "hitting local caches",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does not have any effect if source='local'?",
        "Y": "message about first download cannot be muted",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Does not have any effect if source='local'. Default is False. verbose(bool,optional)",
        "Y": "local",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for progress(bool,optional)?",
        "Y": "Default: None",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If downloaded file is a what, it will be automatically decompressed?",
        "Y": "zip file",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the directory in which to save the object map_location(optional)?",
        "Y": "url(string) \u2013 URL of the object to download model_dir(string,optional)",
        "Z": "The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does themodelcallable have when called with the given*argsand**kwargs?",
        "Y": "output",
        "Z": "The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If the object is already present, it\u2019s deserialized and returned. The default value ofmodel_dirishub_dir>/",
        "Y": "inmodel_dir",
        "Z": "url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the filename-sha256>.extwheresha256>?",
        "Y": "first eight or more digits",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Default: False file_name(string,optional) \u2013 name for the downloaded file.",
        "Y": "Filename fromurlwill be used if not set",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is instantiated by*argsand**kwargsintorch.hub.load()?",
        "Y": "model",
        "Z": "The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "After you have loaded a model, what should you do?",
        "Y": "how can you find out what you can do with the model",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a suggested workflow to see all available methods?",
        "Y": "dir(model)",
        "Z": "The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What object is at the given URL to a local path?",
        "Y": "Download object",
        "Z": "Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(",
        "Y": "url(string)",
        "Z": "Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Filename fromurlwill be used if not set?",
        "Y": "Filename fromurlwill be used if not set",
        "Z": "Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is instantiated by *argsand**kwargsintorch.hub.load()?",
        "Y": "model",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "help(model.foo) to check what argumentsmodel.footakes to run?",
        "Y": "to check what argumentsmodel.footakes to run",
        "Z": "Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What type of object is instantiated by *argsand**kwargsintorch.hub.load()?",
        "Y": "model",
        "Z": "Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the document that specifies how to remap storage locations?",
        "Y": "a function or a dict",
        "Z": "url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for file_name(string,optional) \u2013 name for the downloaded file?",
        "Y": "False",
        "Z": "url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What arguments does model.footakes have to run?",
        "Y": "model.footakes to run",
        "Z": "url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "dst(string) \u2013 what?",
        "Y": "Full path where object will be saved",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is used to check what argumentsmodel.footakes to run?",
        "Y": "model.foo",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Docstring of the function works as a help message. It explains what are the allowed arguments?",
        "Y": "positional/keyword",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If less than 2GB, what is it recommended to do?",
        "Y": "attach it to aproject release",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "The locations are used in the order of what?",
        "Y": "Callinghub.set_dir",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release. Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is set if environment variableXDG_CACHE_HOMEis set?",
        "Y": "$XDG_CACHE_HOME/torch/hub",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release. Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Ifset_dir()is not called, what is the default path?",
        "Y": "$TORCH_HOME/hub",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release. Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Optionally set the what directory used to save downloaded models & weights?",
        "Y": "Torch Hub directory",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release. Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "d(string) \u2013 what is used to save downloaded models & weights?",
        "Y": "path to a local folder",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release. Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "By default, we don\u2019t do what after loading it?",
        "Y": "clean up files",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release. Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Hub uses the cache what if it already exists in the directory returned byget_dir()?",
        "Y": "by default",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release. Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Users can force a reload by what?",
        "Y": "callinghub.load",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release. Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does callinghub.load(...,force_reload=True) do?",
        "Y": "delete the existing github folder and downloaded weights",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release. Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What works by a github branch?",
        "Y": "Torch hub",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release. Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where are the indices located?",
        "Y": "Find the indices",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note If your use case is always 1-D sorted sequence,torch.bucketize()is preferred,\nbecause it has fewer dimension checks resulting in slightly better performance. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If rightis False (default), then the left boundary ofsorted_sequenceis closed.",
        "Y": "Ifrightis False",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note If your use case is always 1-D sorted sequence,torch.bucketize()is preferred,\nbecause it has fewer dimension checks resulting in slightly better performance. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is the name of the right returned index that satisfies 1-D False sorted_sequence?",
        "Y": "sorted_sequence",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note If your use case is always 1-D sorted sequence,torch.bucketize()is preferred,\nbecause it has fewer dimension checks resulting in slightly better performance. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What do values(TensororScalar) contain?",
        "Y": "search value(s)",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note If your use case is always 1-D sorted sequence,torch.bucketize()is preferred,\nbecause it has fewer dimension checks resulting in slightly better performance. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Return the last such index, if False, what?",
        "Y": "return the last such index",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note If your use case is always 1-D sorted sequence,torch.bucketize()is preferred,\nbecause it has fewer dimension checks resulting in slightly better performance. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Return the last such index. If False, return the first suitable location that is found. If True, return the last such index?",
        "Y": "If",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note If your use case is always 1-D sorted sequence,torch.bucketize()is preferred,\nbecause it has fewer dimension checks resulting in slightly better performance. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Which backend controls if TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aintthat controls what of the cuFFT plan?",
        "Y": "cache capacity",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What methods are used to get a new tensor with the data ininputfake quantized per channel?",
        "Y": "scale,zero_point,quant_minandquant_max",
        "Z": "Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis. input(Tensor) \u2013 the input value(s), intorch.float32. scale(Tensor) \u2013 quantization scale, per channel zero_point(Tensor) \u2013 quantization zero_point, per channel axis(int32) \u2013 channel axis quant_min(int64) \u2013 lower bound of the quantized domain quant_max(int64) \u2013 upper bound of the quantized domain A newly fake_quantized per channel tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine"
    },
    {
        "X": "What is the input value(s) of intorch.float32?",
        "Y": "input(Tensor)",
        "Z": "Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis. input(Tensor) \u2013 the input value(s), intorch.float32. scale(Tensor) \u2013 quantization scale, per channel zero_point(Tensor) \u2013 quantization zero_point, per channel axis(int32) \u2013 channel axis quant_min(int64) \u2013 lower bound of the quantized domain quant_max(int64) \u2013 upper bound of the quantized domain A newly fake_quantized per channel tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine"
    },
    {
        "X": "What is the lower bound of the quantized domain?",
        "Y": "quantized domain quant_max(int64)",
        "Z": "Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis. input(Tensor) \u2013 the input value(s), intorch.float32. scale(Tensor) \u2013 quantization scale, per channel zero_point(Tensor) \u2013 quantization zero_point, per channel axis(int32) \u2013 channel axis quant_min(int64) \u2013 lower bound of the quantized domain quant_max(int64) \u2013 upper bound of the quantized domain A newly fake_quantized per channel tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine"
    },
    {
        "X": "What is returned with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis. input(Tensor) \u2013 the input value(s), intorch.float32. scale(Tensor) \u2013 quantization scale, per channel zero_point(Tensor) \u2013 quantization zero_point, per channel axis(int32) \u2013 channel axis quant_min(int64) \u2013 lower bound of the quantized domain quant_max(int64) \u2013 upper bound of the quantized domain A newly fake_quantized per channel tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine"
    },
    {
        "X": "What is the input value(s) in intorch.float32?",
        "Y": "input(Tensor)",
        "Z": "Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis. input(Tensor) \u2013 the input value(s), intorch.float32. scale(Tensor) \u2013 quantization scale, per channel zero_point(Tensor) \u2013 quantization zero_point, per channel axis(int32) \u2013 channel axis quant_min(int64) \u2013 lower bound of the quantized domain quant_max(int64) \u2013 upper bound of the quantized domain A newly fake_quantized per channel tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine"
    },
    {
        "X": "What is a new tensor with the data ininputfake quantized per channel?",
        "Y": "fake_quantized per channel tensor Tensor Example:",
        "Z": "Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis. input(Tensor) \u2013 the input value(s), intorch.float32. scale(Tensor) \u2013 quantization scale, per channel zero_point(Tensor) \u2013 quantization zero_point, per channel axis(int32) \u2013 channel axis quant_min(int64) \u2013 lower bound of the quantized domain quant_max(int64) \u2013 upper bound of the quantized domain A newly fake_quantized per channel tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine"
    },
    {
        "X": "Returns a 1-dimensional view of each input tensor with what dimensions?",
        "Y": "zero",
        "Z": "Returns a 1-dimensional view of each input tensor with zero dimensions.\nInput tensors with one or more dimensions are returned as-is. input(Tensororlist of Tensors) \u2013 output (Tensor or tuple of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_1d.html#torch.atleast_1d"
    },
    {
        "X": "How are input tensors with one or more dimensions returned?",
        "Y": "as-is",
        "Z": "Returns a 1-dimensional view of each input tensor with zero dimensions.\nInput tensors with one or more dimensions are returned as-is. input(Tensororlist of Tensors) \u2013 output (Tensor or tuple of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_1d.html#torch.atleast_1d"
    },
    {
        "X": "What is computed for each element ininput?",
        "Y": "Heaviside step function",
        "Z": "Computes the Heaviside step function for each element ininput.\nThe Heaviside step function is defined as: input(Tensor) \u2013 the input tensor. values(Tensor) \u2013 The values to use whereinputis zero. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside"
    },
    {
        "X": "What is the Heaviside step function defined as?",
        "Y": "input(Tensor) \u2013 the input tensor",
        "Z": "Computes the Heaviside step function for each element ininput.\nThe Heaviside step function is defined as: input(Tensor) \u2013 the input tensor. values(Tensor) \u2013 The values to use whereinputis zero. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside"
    },
    {
        "X": "values(Tensor) \u2013 The values to use what?",
        "Y": "whereinputis zero",
        "Z": "Computes the Heaviside step function for each element ininput.\nThe Heaviside step function is defined as: input(Tensor) \u2013 the input tensor. values(Tensor) \u2013 The values to use whereinputis zero. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside"
    },
    {
        "X": "What is an example of a Heaviside step function?",
        "Y": "Example:",
        "Z": "Computes the Heaviside step function for each element ininput.\nThe Heaviside step function is defined as: input(Tensor) \u2013 the input tensor. values(Tensor) \u2013 The values to use whereinputis zero. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside"
    },
    {
        "X": "What type of storage object is ifobj a storage object?",
        "Y": "PyTorch",
        "Z": "Returns True ifobjis a PyTorch storage object. obj(Object) \u2013 Object to test",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_storage.html#torch.is_storage"
    },
    {
        "X": "Returns what ifobjis a PyTorch storage object?",
        "Y": "True",
        "Z": "Returns True ifobjis a PyTorch storage object. obj(Object) \u2013 Object to test",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_storage.html#torch.is_storage"
    },
    {
        "X": "What type of storage object does ifobjis return True?",
        "Y": "PyTorch",
        "Z": "Returns True ifobjis a PyTorch storage object. obj(Object) \u2013 Object to test",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_storage.html#torch.is_storage"
    },
    {
        "X": "obj(what) returns true ifobjis a PyTorch storage object?",
        "Y": "Object",
        "Z": "Returns True ifobjis a PyTorch storage object. obj(Object) \u2013 Object to test",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_storage.html#torch.is_storage"
    },
    {
        "X": "What is the name of the object to test?",
        "Y": "obj(Object)",
        "Z": "Returns True ifobjis a PyTorch storage object. obj(Object) \u2013 Object to test",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_storage.html#torch.is_storage"
    },
    {
        "X": "What algorithm is this function a front-end to?",
        "Y": "LOBPCG",
        "Z": "Find the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized\neigenvalue problem using matrix-free LOBPCG methods. This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "A less robust method may fail when what is applied to singular input?",
        "Y": "Cholesky",
        "Z": "This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Who introduced the LOBPCG method?",
        "Y": "Andrew Knyazev",
        "Z": "method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When does a less robust method fail?",
        "Y": "when Cholesky is applied to singular input",
        "Z": "This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the front-end to?",
        "Y": "LOBPCG",
        "Z": "This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the name of the algorithm that may fail when applied to a singular input?",
        "Y": "Cholesky",
        "Z": "This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the LOBPCG method with?",
        "Y": "orthogonal basis selection",
        "Z": "method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What type of method is the orthogonal basis selection method?",
        "Y": "robust",
        "Z": "This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What types of inputs are supported by the LOBPCG method?",
        "Y": "dense, sparse, and batches of dense matrices",
        "Z": "method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is applied to singular input?",
        "Y": "Cholesky",
        "Z": "method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What type of method is the LOBPCG method with orthogonal basis selection?",
        "Y": "robust",
        "Z": "method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the name of the method that supports dense, sparse, and batches of dense matrices?",
        "Y": "robust method",
        "Z": "method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does the LOBPCG method have?",
        "Y": "orthogonal basis selection",
        "Z": "method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What type of method is supported by dense, sparse, and batches of dense matrices?",
        "Y": "robust",
        "Z": "method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What types of inputs are supported by the robust method?",
        "Y": "dense, sparse, and batches of dense matrices",
        "Z": "method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "How much time does the basic method spend per iteration?",
        "Y": "least time",
        "Z": "Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Which method converges much faster and is more stable?",
        "Y": "robust methods",
        "Z": "In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is generally not recommended but there are cases where the basic method may be preferred?",
        "Y": "the usage of the basic method",
        "Z": "Find the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized\neigenvalue problem using matrix-free LOBPCG methods. This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are the inputs supported by the LOBPCG method?",
        "Y": "dense, sparse, and batches of dense matrices",
        "Z": "method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "How much time does the basic method spend?",
        "Y": "least time per iteration",
        "Z": "In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the advantage of using the basic method over the robust method?",
        "Y": "the robust methods converge much faster and are more stable",
        "Z": "method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is generally not recommended but there are cases where the use of the basic method may be preferred?",
        "Y": "the usage of the basic method",
        "Z": "method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are the supported inputs?",
        "Y": "dense, sparse, and batches of dense matrices",
        "Z": "Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Which methods converge much faster and are more stable?",
        "Y": "robust methods",
        "Z": "Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is generally not recommended?",
        "Y": "usage of the basic method",
        "Z": "In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does the backward method not support?",
        "Y": "sparse and complex inputs",
        "Z": "Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When does the backward method work?",
        "Y": "whenBis not provided",
        "Z": "The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are we actively working on?",
        "Y": "extensions",
        "Z": "The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Which method spends least time per iteration?",
        "Y": "the basic method",
        "Z": "Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What type of inputs does the backward method not support?",
        "Y": "sparse and complex inputs",
        "Z": "The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "The backward method does not support what?",
        "Y": "sparse and complex inputs",
        "Z": "The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is assumed about A.gradis?",
        "Y": "symmetric",
        "Z": "Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What should be done in first-order optimization routines before runninglobpcgwe do the following symmetrization map?",
        "Y": "A - t * A.gradis symmetric",
        "Z": "While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When is the map performed?",
        "Y": "when theArequires gradients",
        "Z": "While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When not specified,Bis interpereted as what?",
        "Y": "identity matrix",
        "Z": "The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is assumed to be symmetric?",
        "Y": "A.gradis",
        "Z": "While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "In what routines do we make sure thatA - t * A.gradis symmetric?",
        "Y": "first-order optimization routines",
        "Z": "In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the input tensor of size(,m,m)(*, m, m)(,m,",
        "Y": "iK(tensor,optional)",
        "Z": "The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Is A.gradis symmetric or symmetric?",
        "Y": "symmetric",
        "Z": "While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What type of tensor must X be?",
        "Y": "dense",
        "Z": "The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When specified, the input tensor of size(,m,m)(*, m, m)(,m",
        "Y": "preconditioner",
        "Z": "While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "iK(tensor,optional) \u2013 the input tensor of size(,m,m)(*,",
        "Y": "preconditioner",
        "Z": "Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When specified, the input tensor of size(,m,m)(*,m, m)(,m,",
        "Y": "preconditioner",
        "Z": "X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the number of requested eigenpairs?",
        "Y": "k(integer,optional)",
        "Z": "iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default number of XXXcolumns?",
        "Y": "Default",
        "Z": "While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value of k(integer,optional)?",
        "Y": "number ofXXXcolumns",
        "Z": "iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When specified, the input tensor of size will be used as what?",
        "Y": "preconditioner",
        "Z": "iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value for the number ofXXXcolumns?",
        "Y": "Default",
        "Z": "iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What happens to the size of the generated random approximation of eigenvectors?",
        "Y": "ifXXXis not specified",
        "Z": "X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value of the generated random approximation of eigenvectors?",
        "Y": "fornisk",
        "Z": "B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What must be the number ofXXXcolumns if XXXis specified?",
        "Y": "the value ofn",
        "Z": "iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What specifies the size of the generated random approximation of eigenvectors?",
        "Y": "n(integer,optional)",
        "Z": "n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What must the value ofn(when specified) be?",
        "Y": "the number ofXXXcolumns",
        "Z": "n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the residual tolerance for stopping criterion?",
        "Y": "tol(float,optional)",
        "Z": "iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value of isfeps?",
        "Y": "0.5",
        "Z": "tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the smallest non-zero floating-point number of the given input tensorAdata type?",
        "Y": "largest",
        "Z": "tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does largest(bool,optional) solve the eigenproblem for?",
        "Y": "smallest eigenvalues",
        "Z": "iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default to solve the eigenproblem for the smallest eigenvalues?",
        "Y": "True",
        "Z": "tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does method(str,optional) do?",
        "Y": "select LOBPCG method",
        "Z": "n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Where can you find the LOBPCG method?",
        "Y": "the description of the function above",
        "Z": "n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default name of the LOBPCG method?",
        "Y": "ortho",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Tol(float,optional) \u2013 what for stopping criterion?",
        "Y": "residual tolerance",
        "Z": "tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default floating-point number of the given input tensorAdata type?",
        "Y": "Default isfeps",
        "Z": "tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the eigenproblem for the largest eigenvalues?",
        "Y": "largest",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value of largest(bool,optional)?",
        "Y": "Default is True",
        "Z": "tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What method does method(str,optional) select?",
        "Y": "LOBPCG method",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value of the LOBPCG method?",
        "Y": "ortho",
        "Z": "n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the name of the eigenproblem for the largest eigenvalues?",
        "Y": "largest",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the smallest eigenvalue?",
        "Y": "smallest",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value to solve the eigenproblem for the smallest eigenvalues?",
        "Y": "True",
        "Z": "iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the maximum number of iterations?",
        "Y": "niter",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When the maximum number of iterations is reached, the iteration process is what?",
        "Y": "hard-stopped",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "For infinite iteration but until convergence criteria is met, what is the default?",
        "Y": "use-1",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value of the largest(bool,optional)?",
        "Y": "Default",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Where is the LOBPCG method described?",
        "Y": "the description of the function above",
        "Z": "tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is niter?",
        "Y": "maximum number of iterations",
        "Z": "niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What happens when the maximum number of iterations is reached?",
        "Y": "the iteration process is hard-stopped and the current approximation of eigenpairs is returned",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When does use-1 for infinite iteration?",
        "Y": "until convergence criteria is met",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When the maximum number of iterations is reached, what happens to the iteration process?",
        "Y": "hard-stopped",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the name of the function that can be called?",
        "Y": "tracker",
        "Z": "n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does method(str,optional) select?",
        "Y": "LOBPCG method",
        "Z": "method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Where can you find out more about the LOBPCG method?",
        "Y": "the description of the function above",
        "Z": "method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When is the iteration process hard-stopped and the current approximation of eigenpairs returned?",
        "Y": "until convergence criteria is met",
        "Z": "method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the name of the function that can be used for infinite iteration?",
        "Y": "tracker",
        "Z": "method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "For infinite iteration but until convergence criteria is met, what is used?",
        "Y": "use-1",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is a function for tracing the iteration process?",
        "Y": "tracker",
        "Z": "tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is used as an argument for tracker?",
        "Y": "LOBPCG instance",
        "Z": "tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What holds the full state of the iteration process in the following attributes?",
        "Y": "LOBPCG instance",
        "Z": "tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When is use-1 used for infinite iteration?",
        "Y": "until convergence criteria is met",
        "Z": "niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is a function for tracing the iteration process called?",
        "Y": "tracker",
        "Z": "n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What instance holds the full state of the iteration process in the following attributes?",
        "Y": "LOBPCG",
        "Z": "niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is an argument for tracker?",
        "Y": "LOBPCG instance",
        "Z": "niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is a function for?",
        "Y": "tracing the iteration process",
        "Z": "a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is an argument for a function for tracing the iteration process?",
        "Y": "LOBPCG instance",
        "Z": "a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "A,B,iK- input what?",
        "Y": "Tensor arguments",
        "Z": "n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is LOBPCG a function for?",
        "Y": "tracing the iteration process",
        "Z": "a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "The LOBPCG instance holds the full state of the iteration process in what attributes?",
        "Y": "iparams,fparams,bparams",
        "Z": "a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does A,B,iK stand for?",
        "Y": "input Tensor arguments",
        "Z": "tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883 [DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming\nGu. (2018) A Robust and Efficient Implementation of LOBPCG.\nSIAM J. Sci. Comput., 40(5), C655-C676. (22 pages)https://epubs.siam.org/doi/abs/10.1137/17M1129830",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are dictionaries of integer, float, and boolean valued input parameters?",
        "Y": "iparams,fparams,bparams",
        "Z": "iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are iteration Tensor variables?",
        "Y": "E,X,S,R",
        "Z": "niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is an example of a Tensor valued iteration variables?",
        "Y": "For instance",
        "Z": "iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "ivars,fvars,bvars,tvars are dictionaries of what types of variables?",
        "Y": "integer, float, boolean, and Tensor valued iteration variables",
        "Z": "ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are the input Tensor arguments?",
        "Y": "A,B,iK",
        "Z": "ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does rerr stand for?",
        "Y": "current state of convergence criteria",
        "Z": "ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are dictionaries of integer, float, boolean, and Tensor valued iteration variables?",
        "Y": "ivars,fvars,bvars,tvars",
        "Z": "ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the current number of converged eigenpairstvars?",
        "Y": "current state of convergence criteria",
        "Z": "ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "E,X,S,R- iteration of what?",
        "Y": "Tensor variables",
        "Z": "A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the current iteration stepX?",
        "Y": "ivars",
        "Z": "For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does E,X,S,R refer to?",
        "Y": "iteration Tensor variables",
        "Z": "E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When does tracker make copies of Tensor objects?",
        "Y": "whentrackerstores Tensor objects from the LOBPCG instance",
        "Z": "For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the iteration of E,X,S,R?",
        "Y": "Tensor variables",
        "Z": "E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When must tracker make copies of Tensor objects?",
        "Y": "whentrackerstores Tensor objects from the LOBPCG instance",
        "Z": "E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the iteration process will be what?",
        "Y": "hard-stopped",
        "Z": "A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Iftrackersetsbvars[\u201cforce_stop\u201d] = True, what will be hard-stopped?",
        "Y": "the iteration process",
        "Z": "ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the current approximation of eigenvectorsE?",
        "Y": "current iteration stepX",
        "Z": "iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the current approximation of eigenvaluesR?",
        "Y": "the current residualivars",
        "Z": "ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the current approximation of?",
        "Y": "eigenvalues",
        "Z": "ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When must it make copies of Tensor objects?",
        "Y": "whentrackerstores Tensor objects from the LOBPCG instance",
        "Z": "Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are the various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d?",
        "Y": "ortho_fparams, ortho_bparams(ortho_iparams,)",
        "Z": "A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Whentrackerstores Tensor objects from the LOBPCG instance, it must what?",
        "Y": "make copies",
        "Z": "Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Iftrackersetsbvars = True, the iteration process will be hard-stopped.",
        "Y": "Iftrackersetsbvars",
        "Z": "a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does ortho_fparams, ortho_bparams(ortho_iparams) represent?",
        "Y": "various parameters",
        "Z": "Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the tensor of eigenvalues of size(,k)(*, k)(,k)",
        "Y": "X",
        "Z": "tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883 [DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming\nGu. (2018) A Robust and Efficient Implementation of LOBPCG.\nSIAM J. Sci. Comput., 40(5), C655-C676. (22 pages)https://epubs.siam.org/doi/abs/10.1137/17M1129830",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Iftrackersetsbvars[\u201cforce_stop\u201d] = what, the iteration process will be hard-stopped?",
        "Y": "True",
        "Z": "Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the name of the parameter to the LOBPCG algorithm when usingmethod=\u201dortho\u201d?",
        "Y": "ortho_fparams",
        "Z": "Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the name of the iteration process that will be hard-stopped?",
        "Y": "Iftrackersetsbvars",
        "Z": "Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does ortho_fparams, ortho_bparams(ortho_iparams) mean?",
        "Y": "various parameters to LOBPCG algorithm",
        "Z": "Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Returns a 2-dimensional view of each input tensor with what dimensions?",
        "Y": "zero",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.\nInput tensors with two or more dimensions are returned as-is. input(Tensororlist of Tensors) \u2013 output (Tensor or tuple of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_2d.html#torch.atleast_2d"
    },
    {
        "X": "How are input tensors with two or more dimensions returned?",
        "Y": "as-is",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.\nInput tensors with two or more dimensions are returned as-is. input(Tensororlist of Tensors) \u2013 output (Tensor or tuple of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_2d.html#torch.atleast_2d"
    },
    {
        "X": "What is used to find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of",
        "Y": "matrix-free LOBPCG methods",
        "Z": "Find the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized\neigenvalue problem using matrix-free LOBPCG methods. This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the name of the method used to find the k largest eigenvalues and the corresponding eigenvectors?",
        "Y": "robust method",
        "Z": "Find the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized\neigenvalue problem using matrix-free LOBPCG methods. This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are supported inputs?",
        "Y": "dense, sparse, and batches of dense matrices",
        "Z": "Find the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized\neigenvalue problem using matrix-free LOBPCG methods. This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is generally not recommended but there are cases where the usage of the basic method may be preferred?",
        "Y": "usage of the basic method",
        "Z": "Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What type of method supports dense, sparse, and batches of dense matrices?",
        "Y": "robust",
        "Z": "method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Which method does not support sparse and complex inputs?",
        "Y": "The backward method",
        "Z": "Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is assumed thatAis symmetric?",
        "Y": "A.gradis not",
        "Z": "The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What do we do to make sure thatA - t * A.gradis symmetric?",
        "Y": "first-order optimization routines",
        "Z": "While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What converges much faster and is more stable?",
        "Y": "robust methods",
        "Z": "Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "In what routines is A - t * A.gradis symmetric?",
        "Y": "first-order optimization routines",
        "Z": "The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What must X be?",
        "Y": "dense tensor",
        "Z": "In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does k(integer,optional) default to when specified?",
        "Y": "the number ofXXXcolumns",
        "Z": "While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value for the number of requested eigenpairs?",
        "Y": "number ofXXXcolumns",
        "Z": "Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does nSpecify if XXXis not specified?",
        "Y": "nspecifies the size of the generated random approximation of eigenvectors",
        "Z": "A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value for the size of the generated random approximation of eigenvectors?",
        "Y": "Default value fornisk",
        "Z": "A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "If XXXis specified, the value ofn(when specified) must be what?",
        "Y": "the number ofXXXcolumns",
        "Z": "The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is tol(float,optional) for stopping criterion?",
        "Y": "residual tolerance",
        "Z": "A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default isfeps?",
        "Y": "smallest non-zero floating-point number of the given input tensorAdata type",
        "Z": "A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Tol(float,optional) \u2013 what is the stopping criterion?",
        "Y": "residual tolerance",
        "Z": "tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value for largest(bool,optional)?",
        "Y": "True",
        "Z": "n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When does use-1. tracker(callable,optional) \u2013 a function for tracing the iteration process?",
        "Y": "until convergence criteria is met",
        "Z": "niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is a function for tracing the iteration process called at each iteration step with LOBPCG instance as an argument?",
        "Y": "tracker",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does the LOBPCG instance hold the full state of the iteration process in?",
        "Y": "The LOBPCG instance holds the full state of the iteration process in the following attributes",
        "Z": "tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default value of the largest(bool,optional) function?",
        "Y": "Default",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When niter(int,optional) is reached, what happens to the iteration process?",
        "Y": "the iteration process is hard-stopped",
        "Z": "niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the name of the iteration variable that holds the full state of the iteration process?",
        "Y": "instance",
        "Z": "niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are the iteration Tensor variables?",
        "Y": "E,X,S,R",
        "Z": "ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairst",
        "Y": "current state of convergence criteria",
        "Z": "tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "ivars[\u201cistep\u201d] is what?",
        "Y": "current iteration step",
        "Z": "a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "ivars[\u201cistep\u201d]- the current approximation of eigenvectorsE- the current approxim",
        "Y": "current iteration stepX",
        "Z": "iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Iftrackersetsbvars = True, the iteration process will be hard-stopped?",
        "Y": "Iftrackersetsbvars",
        "Z": "A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are ortho_fparams and ortho_bparams?",
        "Y": "various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d",
        "Z": "A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are the dictionaries of integer, float, boolean, and Tensor valued iteration variables?",
        "Y": "ivars,fvars,bvars,tvars",
        "Z": "ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "ivars[\u201cistep\u201d]- what?",
        "Y": "current iteration stepX",
        "Z": "ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "E,X,S,R- what?",
        "Y": "iteration Tensor variables",
        "Z": "A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "The current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars",
        "Y": "current state of convergence criteria",
        "Z": "A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive",
        "Y": "matrix-free LOBPCG methods",
        "Z": "Find the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized\neigenvalue problem using matrix-free LOBPCG methods. This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What type of method does the LOBPCG method with orthogonal basis selection use?",
        "Y": "robust method",
        "Z": "Find the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized\neigenvalue problem using matrix-free LOBPCG methods. This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Supported inputs are what?",
        "Y": "dense, sparse, and batches of dense matrices",
        "Z": "Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are the advantages of the robust methods?",
        "Y": "converge much faster and are more stable",
        "Z": "This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Is the use of the basic method recommended or recommended?",
        "Y": "generally not recommended",
        "Z": "Find the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized\neigenvalue problem using matrix-free LOBPCG methods. This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Is Ais symmetric or not?",
        "Y": "A.gradis not",
        "Z": "Find the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized\neigenvalue problem using matrix-free LOBPCG methods. This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is performed only when theArequires gradients?",
        "Y": "symmetrization map",
        "Z": "This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When is the map performed only?",
        "Y": "when theArequires gradients",
        "Z": "The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the input tensor of size(optional)?",
        "Y": "X(tensor,optional)",
        "Z": "Find the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized\neigenvalue problem using matrix-free LOBPCG methods. This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the LOBPCG method with orthogonal basis selection?",
        "Y": "robust method",
        "Z": "This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the usage of the basic method?",
        "Y": "generally not recommended",
        "Z": "This function is a front-end to the following LOBPCG algorithms\nselectable viamethodargument: method=\u201dbasic\u201d- the LOBPCG method introduced by Andrew\nKnyazev, see [Knyazev2001]. A less robust method, may fail when\nCholesky is applied to singular input. method=\u201dortho\u201d- the LOBPCG method with orthogonal basis\nselection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is it assumed thatAis symmetric is?",
        "Y": "A.gradis not",
        "Z": "Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Where is A - t * A.gradis symmetric?",
        "Y": "first-order optimization routines",
        "Z": "Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "X must be a what?",
        "Y": "dense tensor",
        "Z": "Supported inputs are dense, sparse, and batches of dense matrices. Note In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the default number of requested eigenpairs?",
        "Y": "number ofXXXcolumns",
        "Z": "In general, the basic method spends least time per\niteration. However, the robust methods converge much faster and\nare more stable. So, the usage of the basic method is generally\nnot recommended but there exist cases where the usage of the\nbasic method may be preferred. Warning The backward method does not support sparse and complex inputs.\nIt works only whenBis not provided (i.e.B == None).\nWe are actively working on extensions, and the details of\nthe algorithms are going to be published promptly. Warning While it is assumed thatAis symmetric,A.gradis not.\nTo make sure thatA.gradis symmetric, so thatA - t * A.gradis symmetric\nin first-order optimization routines, prior to runninglobpcgwe do the following symmetrization map:A -> (A + A.t()) / 2.\nThe map is performed only when theArequires gradients. A(Tensor) \u2013 the input tensor of size(\u2217,m,m)(*, m, m)(\u2217,m,m) B(Tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When not specified,Bis interpereted as\nidentity matrix. X(tensor,optional) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)wherek <= n <= m. When specified, it is used as\ninitial approximation of eigenvectors. X must be a\ndense tensor. iK(tensor,optional) \u2013 the input tensor of size(\u2217,m,m)(*, m,\nm)(\u2217,m,m). When specified, it will be used as preconditioner. k(integer,optional) \u2013 the number of requested\neigenpairs. Default is the number ofXXXcolumns (when specified) or1. n(integer,optional) \u2013 ifXXXis not specified thennspecifies the size of the generated random\napproximation of eigenvectors. Default value fornisk. IfXXXis specified, the value ofn(when specified) must be the number ofXXXcolumns. tol(float,optional) \u2013 residual tolerance for stopping\ncriterion. Default isfeps ** 0.5wherefepsis\nsmallest non-zero floating-point number of the given\ninput tensorAdata type. largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When True, solve the eigenproblem for the largest eigenvalues. Otherwise, solve the eigenproblem for smallest e",
        "Y": "largest(bool,optional)",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Largest(bool,optional) \u2013 when True, solve the eigenproblem for what?",
        "Y": "smallest eigenvalues",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is required to select LOBPCG method?",
        "Y": "description of the function above",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "niter(int,optional) \u2013 what?",
        "Y": "maximum number of iterations",
        "Z": "niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When reached, the iteration process is hard-stopped and what is returned?",
        "Y": "the current approximation of eigenpairs is returned",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Tracker(callable,optional) \u2013 a function for what?",
        "Y": "tracing the iteration process",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Tracker is called at each iteration step with what as an argument?",
        "Y": "LOBPCG instance",
        "Z": "niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are the dictionaries of integer, float, and boolean valued input parameters?",
        "Y": "iparams,fparams,bparams",
        "Z": "tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883 [DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming\nGu. (2018) A Robust and Efficient Implementation of LOBPCG.\nSIAM J. Sci. Comput., 40(5), C655-C676. (22 pages)https://epubs.siam.org/doi/abs/10.1137/17M1129830",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the current iteration stepX- the current approximation of eigenvectorsE- the current approximation",
        "Y": "ivars",
        "Z": "A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the iteration process will be hard-stopped.",
        "Y": "force_stop",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What are the various parameters to LOBP?",
        "Y": "ortho_fparams, ortho_bparams(ortho_iparams,)",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "LOBPCG method \u2013 select LOBPCG method. See what?",
        "Y": "description of the function above",
        "Z": "method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When the maximum number of iterations is reached, what happens?",
        "Y": "the iteration process is hard-stopped",
        "Z": "method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the iteration process will be hard-stopped. ortho",
        "Y": "force_stop",
        "Z": "method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is a tensor of eigenvalues?",
        "Y": "tensor of eigenvalues",
        "Z": "method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is returned when the iteration process is hard-stopped?",
        "Y": "the current approximation of eigenpairs",
        "Z": "niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Iftrackersetsbvars[\u201cforce_stop?\u201d] = True, the iteration process will be hard-stopped. ortho",
        "Y": "force_stop",
        "Z": "tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883 [DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming\nGu. (2018) A Robust and Efficient Implementation of LOBPCG.\nSIAM J. Sci. Comput., 40(5), C655-C676. (22 pages)https://epubs.siam.org/doi/abs/10.1137/17M1129830",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What does tracker call at each iteration step with as an argument?",
        "Y": "LOBPCG instance",
        "Z": "largest(bool,optional) \u2013 when True, solve the eigenproblem for\nthe largest eigenvalues. Otherwise, solve the\neigenproblem for smallest eigenvalues. Default is True. method(str,optional) \u2013 select LOBPCG method. See the\ndescription of the function above. Default is\n\u201cortho\u201d. niter(int,optional) \u2013 maximum number of iterations. When\nreached, the iteration process is hard-stopped and\nthe current approximation of eigenpairs is returned.\nFor infinite iteration but until convergence criteria\nis met, use-1. tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "When was Andrew V. Knyazev published?",
        "Y": "2001",
        "Z": "tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883 [DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming\nGu. (2018) A Robust and Efficient Implementation of LOBPCG.\nSIAM J. Sci. Comput., 40(5), C655-C676. (22 pages)https://epubs.siam.org/doi/abs/10.1137/17M1129830",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "Who published the Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method?",
        "Y": "SIAM J. Sci.",
        "Z": "tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883 [DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming\nGu. (2018) A Robust and Efficient Implementation of LOBPCG.\nSIAM J. Sci. Comput., 40(5), C655-C676. (22 pages)https://epubs.siam.org/doi/abs/10.1137/17M1129830",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What is the name of the computer that tracks the iteration process?",
        "Y": "Comput",
        "Z": "tracker(callable,optional) \u2013 a function for tracing the\niteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an\nargument. The LOBPCG instance holds the full state of\nthe iteration process in the following attributes: iparams,fparams,bparams- dictionaries of\ninteger, float, and boolean valued input\nparameters, respectively ivars,fvars,bvars,tvars- dictionaries\nof integer, float, boolean, and Tensor valued\niteration variables, respectively. A,B,iK- input Tensor arguments. E,X,S,R- iteration Tensor variables. For instance: ivars[\u201cistep\u201d]- the current iteration stepX- the current approximation of eigenvectorsE- the current approximation of eigenvaluesR- the current residualivars[\u201cconverged_count\u201d]- the current number of converged eigenpairstvars[\u201crerr\u201d]- the current state of convergence criteria Note that whentrackerstores Tensor objects from\nthe LOBPCG instance, it must make copies of these. Iftrackersetsbvars[\u201cforce_stop\u201d] = True, the\niteration process will be hard-stopped. ortho_fparams, ortho_bparams(ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when usingmethod=\u201dortho\u201d.  tensor of eigenvalues of size(\u2217,k)(*, k)(\u2217,k) X (Tensor): tensor of eigenvectors of size(\u2217,m,k)(*, m, k)(\u2217,m,k) E (Tensor) References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal\nPreconditioned Eigensolver: Locally Optimal Block Preconditioned\nConjugate Gradient Method. SIAM J. Sci. Comput., 23(2),\n517-541. (25 pages)https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng\nWu. (2002) A Block Orthogonalization Procedure with Constant\nSynchronization Requirements. SIAM J. Sci. Comput., 23(6),\n2165-2182. (18 pages)https://epubs.siam.org/doi/10.1137/S1064827500370883 [DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming\nGu. (2018) A Robust and Efficient Implementation of LOBPCG.\nSIAM J. Sci. Comput., 40(5), C655-C676. (22 pages)https://epubs.siam.org/doi/abs/10.1137/17M1129830",
        "source": "https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg"
    },
    {
        "X": "What documentation does totorch.use_deterministic_algorithms() refer to for more details?",
        "Y": "totorch.use_deterministic_algorithms()",
        "Z": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled"
    },
    {
        "X": "What does torch.isfinite do?",
        "Y": "Returns a new tensor with boolean elements representing if each element is finite or not",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    {
        "X": "What are finite when both their real and imaginary parts are finite?",
        "Y": "Complex values",
        "Z": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    {
        "X": "What type of distribution does fillselftensor with elements drawn from with torch.Tensor.geometric_?",
        "Y": "geometric distribution",
        "Z": "Fillsselftensor with elements drawn from the geometric distribution:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"
    },
    {
        "X": "In what direction does the flipud return a new tensor?",
        "Y": "Flip the entries in each column in the up/down direction",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud.",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    {
        "X": "What are preserved, but appear in a different order than before when flipud is used?",
        "Y": "Rows",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud.",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    {
        "X": "What is requirement on tensor when using flipud?",
        "Y": "2-D",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud.",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    {
        "X": "What is requirement on tensor when using fliplr?",
        "Y": "2-D",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr.",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    {
        "X": "Does torch.flip make a copy of input's data?",
        "Y": "Yes torch.flip makes a copy ofinput\u2019s data",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flip makes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    {
        "X": "What is the difference between torch.flipudis and numpy.flipud?",
        "Y": "torch.flipud is slower than numpy.flipud",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud.",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    {
        "X": "What does flip tensor in the up/down direction return?",
        "Y": "torch.flipud",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud.",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    {
        "X": "In which direction do the entries in each column flip?",
        "Y": "up/down",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud.",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    {
        "X": "What is required for the tensor to be at least?",
        "Y": "1-D",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud.",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    {
        "X": "What returns a view in constant time?",
        "Y": "NumPy\u2019snp.flip",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    {
        "X": "Why is copying a tensor's data slower than viewing that data?",
        "Y": "more work",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr.",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    {
        "X": "How is the tensor's dtype inferred with torch.full?",
        "Y": "from fill_value",
        "Z": "Creates a tensor of sizesizefilled withfill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    {
        "X": "What is the value to fill the output tensor with torch.full?",
        "Y": "fill_value(Scalar)",
        "Z": "fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    {
        "X": "What  type of tensor is created with torch.full?",
        "Y": "a tensor of sizesizefilled withfill_value",
        "Z": "Creates a tensor of size filled with fill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    {
        "X": "What is inferred from fill_value?",
        "Y": "datatype",
        "Z": "Creates a tensor of sizesizefilled withfill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    {
        "X": "What defines the shape of the output tensor with torch.ones?",
        "Y": "a sequence of integers",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    {
        "X": "How is the desired layout of returned Tensor with torch.ones?",
        "Y": "with torch.layout",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    {
        "X": "What is the name of the layout of returned Tensor with torch.ones?",
        "Y": "Default:torch.strided",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    {
        "X": "What is the desired data type of returned Tensor with torch.zeros_like?",
        "Y": "dtype, if specified",
        "Z": "Returns a tensor filled with the scalar value0, with the same size as input.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is the default layout of returned Tensor with torch.zeros_like?",
        "Y": "Default:torch.strided",
        "Z": "out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    {
        "X": "How are numbers sampled from the continuous uniform distribution?",
        "Y": "using torch.Tensor.uniform_",
        "Z": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"
    },
    {
        "X": "What is filled with numbers sampled from the continuous uniform distribution?",
        "Y": "self.tensor",
        "Z": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"
    },
    {
        "X": "What is the current state of the generator?",
        "Y": "a torch.ByteTensor",
        "Z": "Gets the current device of the generator. Example: Returns the Generator state as a torch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does atorch.ByteTensor get?",
        "Y": "current device",
        "Z": "Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits?",
        "Y": "Avoid having many 0 bits in the seed",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is the pupose of initial seed for generating random numbers?",
        "Y": "Sets the seed for generating random numbers",
        "Z": "Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is the Generator state returned as?",
        "Y": "a torch.ByteTensor",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What Sets the seed for generating random numbers return?",
        "Y": "a torch.Generator object",
        "Z": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What should you do when setting a large seed?",
        "Y": "Avoid having many 0 bits in the seed",
        "Z": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does Atorch.ByteTensor do?",
        "Y": "Sets the seed for generating random numbers",
        "Z": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is recommended to set a number that has a good balance of 0 and 1 bits?",
        "Y": "large seed",
        "Z": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What  returns the initial seed for generating random numbers?",
        "Y": "a torch.Generator object",
        "Z": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What should you avoid in the seed?",
        "Y": "Avoid having many 0 bits",
        "Z": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does it return for generating random numbers?",
        "Y": "Returns the initial seed",
        "Z": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is the object that sets the seed for generating random numbers?",
        "Y": "torch.Generator",
        "Z": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is an example of a torch.Generator object?",
        "Y": "Generator",
        "Z": "Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is returned if inference mode is enabled?",
        "Y": "True",
        "Z": "Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled"
    },
    {
        "X": "What is reduced with torch.var_mean?",
        "Y": "dimension",
        "Z": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction.",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "How to specify that output tensor should retain the dimension or not?",
        "Y": "by specifying keepdim(bool)",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "What is the name of correction that will be used to calculate the variance?",
        "Y": "Bessel's correction",
        "Z": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean.",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "What does the tuple contain when using torch.var_mean?",
        "Y": "variance and mean",
        "Z": "out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). A tuple (var, mean) containing the variance and mean. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "What is calculated by a tuple containing the variance and mean of all elements in the inputtensor with torch.var_mean?",
        "Y": "the variance and mean of all elements in theinputtensor with torch.var_mean",
        "Z": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "When is Bessel's correction applied",
        "Y": "If unbiased is True",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "If funbiasedisTrue, what will be used?",
        "Y": "Bessel's correction is used",
        "Z": "out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). A tuple (var, mean) containing the variance and mean. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "What is used to determine whether to use Bessel's correction?",
        "Y": "unbiased as True",
        "Z": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1).",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "What type of storage does Everytorch.Tensor cast?",
        "Y": "bfloat16",
        "Z": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does Everytorch.Tensor return a copy of?",
        "Y": "char",
        "Z": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens if the object is already in CUDA memory and on the correct device?",
        "Y": "no copy is performed and the original object is returned",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does Casts this storage to byte type Return?",
        "Y": "a copy",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the object is already in CUDA memory and on the correct device, what happens?",
        "Y": "no copy is performed",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does Casts this storage return?",
        "Y": "a copy",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of what type of storage?",
        "Y": "float type",
        "Z": "Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Casts this storage to complex what type?",
        "Y": "float",
        "Z": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Where is a copy of this object stored?",
        "Y": "CUDA memory",
        "Z": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the object is already in CUDA memory and on the correct device, what is performed?",
        "Y": "no copy",
        "Z": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "The destination GPU id Defaults to what?",
        "Y": "current device",
        "Z": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What if true and the source is in pinned memory, the copy will be asynchronous with respect to the host?",
        "Y": "non_blocking(bool)",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, what happens?",
        "Y": "the argument has no effect",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Where does the object return a copy of?",
        "Y": "CUDA memory",
        "Z": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "For compatibility, may contain what place of thenon_blockingargument?",
        "Y": "keyasyncin",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the object is already in what?",
        "Y": "CUDA memory",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does the keyasyncin cast?",
        "Y": "double type",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does bool stand for?",
        "Y": "non_blocking",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does double type cast?",
        "Y": "float type",
        "Z": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of argument has no effect if the source is in pinned memory?",
        "Y": "non_blocking(bool)",
        "Z": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage is cast to double type?",
        "Y": "float type",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens when memory is shared between all processes?",
        "Y": "All changes are written to the file",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage is casts this storage to?",
        "Y": "float type",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens when a storage is cast to float type IfsharedisTrue?",
        "Y": "memory is shared between all processes",
        "Z": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage is casts?",
        "Y": "float type",
        "Z": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the changes on the storage do not affect the file, what is the name of the type of storage?",
        "Y": "IfsharedisFalse",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does filename(str) - file name to map?",
        "Y": "filename(str) \u2013 file name to map",
        "Z": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens when a float type is shared between all processes?",
        "Y": "All changes are written to the file",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory?",
        "Y": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is shared between all processes?",
        "Y": "memory",
        "Z": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does filename(str) \u2013 file name to map shared(bool)?",
        "Y": "filename(str) \u2013 file name to map shared(bool)",
        "Z": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Copies the storage to what if it's not already pinned?",
        "Y": "pinned memory",
        "Z": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of return does CUDA return?",
        "Y": "self",
        "Z": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does this no-op do for storages already in shared memory and for CUDA storages?",
        "Y": "Moves the storage to shared memory",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for what storage?",
        "Y": "CUDA",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is the name of the number of elements in a storage?",
        "Y": "size(int)",
        "Z": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for what type of storage?",
        "Y": "CUDA",
        "Z": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What cannot be resized?",
        "Y": "Storages in shared memory",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, otherwise casts this object to the",
        "Y": "self Casts this storage to short type",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If this is already of the correct type, no copy is performed and what is returned?",
        "Y": "the original object is returned",
        "Z": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens if the copy is performed asynchronously with respect to the host?",
        "Y": "the argument has no effect",
        "Z": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If true how copy is performed what with respect to the host?",
        "Y": "asynchronously",
        "Z": "dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage is the async arg deprecated?",
        "Y": "bfloat16",
        "Z": "Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If no copy is performed and the original object is returned, what happens?",
        "Y": "If this is already of the correct type",
        "Z": "Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does Theasyncarg cast this storage to?",
        "Y": "bfloat16 type",
        "Z": "Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "IfTrue and the source are in pinned memory and destination is on the GPU or vice versa, how is copy performed?",
        "Y": "asynchronously",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is the name of the function that performs asynchronously with respect to the host?",
        "Y": "non_blocking(bool)",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does the keyasyncin cast this storage to?",
        "Y": "bfloat16 type",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does the keyasyncarg cast this storage to?",
        "Y": "bfloat16",
        "Z": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What part of a complex tensor is equal to real?",
        "Y": "real",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    {
        "X": "What is the real part of the complex tensor?",
        "Y": "real(Tensor)",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    {
        "X": "Real(Tensor) \u2013 The real part of the complex tensor. Must be what?",
        "Y": "float or double",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    {
        "X": "What is the imaginary part of the complex tensor?",
        "Y": "imag",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    {
        "X": "What must the imag(Tensor) be?",
        "Y": "same dtype asreal",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    {
        "X": "If the inputs are torch.float64, ouput tensor must be what?",
        "Y": "be torch.complex128",
        "Z": "Constructs a complex tensor with its real part equal torealand its\nimaginary part equal toimag. real(Tensor) \u2013 The real part of the complex tensor. Must be float or double. imag(Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype\nasreal. out(Tensor) \u2013 If the inputs aretorch.float32, must betorch.complex64. If the inputs aretorch.float64, must betorch.complex128. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    {
        "X": "What does solve AX = b assume A to be?",
        "Y": "upper-triangular",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What does solve AX = b  solve?",
        "Y": "a system of equations with a triangular coefficient matrix",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What does solve solve AX = b  assume A is?",
        "Y": "upper-triangular",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the name for multiple right-hand sides of size(,m,k)(*, m, k)(,",
        "Y": "b(Tensor)",
        "Z": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the name of the input triangular coefficient matrix of size(,m,k)(*, m, k)(",
        "Y": "b(Tensor)",
        "Z": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "Supports input of float, double, cfloat and what other data type?",
        "Y": "cdouble",
        "Z": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the default value for the upper-triangular system of equations?",
        "Y": "True",
        "Z": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the input triangular coefficient matrix of size(,m,m)(*, m, k)(,m",
        "Y": "b(Tensor)",
        "Z": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the default value of transpose(bool,optional)?",
        "Y": "False",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the default matrix system of equations?",
        "Y": "upper triangular",
        "Z": "upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the default value for transpose?",
        "Y": "Default:False",
        "Z": "transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the default value of unitriangular(bool,optional)?",
        "Y": "Default:False",
        "Z": "upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "If True, the diagonal elements of A are assumed to be 1 and not referenced from A..",
        "Y": "Default:False",
        "Z": "transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is a solution, cloned_coefficient?",
        "Y": "namedtuple",
        "Z": "transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. unitriangular(bool,optional) \u2013 whetherAAAis unit triangular.\nIf True, the diagonal elements ofAAAare assumed to be\n1 and not referenced fromAAA. Default:False. A namedtuple(solution, cloned_coefficient)wherecloned_coefficientis a clone ofAAAandsolutionis the solutionXXXtoAX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the error function of input do?",
        "Y": "Computes the complementary error function ofinput",
        "Z": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    {
        "X": "What is the complementary error function of input do?",
        "Y": "Computes the complementary error function ofinput",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    {
        "X": "What is computed for each element of input with torch.special.expit?",
        "Y": "exponentially scaled zeroth order modified Bessel function",
        "Z": "Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    {
        "X": "What does input return with each of the elements of inputrounded to the closest integer?",
        "Y": "Returns a new tensor",
        "Z": "Returns a new tensor with each of the elements ofinputrounded\nto the closest integer. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.round.html#torch.round"
    },
    {
        "X": "What is the nearly optimal approximation of a singular value decomposition of a centered matrix?",
        "Y": "namedtuple(U,S,V)",
        "Z": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What do theVVVcolumns represent?",
        "Y": "the principal directions",
        "Z": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References:",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What is the size of returned matrices?",
        "Y": "UUUis m x q matrix",
        "Z": "The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "matmul(A,V[:,:k])projects data to what?",
        "Y": "first k principal components",
        "Z": "Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References:",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What does S2/(m1)S ** 2 / (m - 1)S2/(m1)contains?",
        "Y": "eigenvalues",
        "Z": "Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References:",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What does the pseudorandom number generator do to obtain repeatable results?",
        "Y": "reset the seed",
        "Z": "S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What is the name of the inputtensor that returns a new tensor that is a narrowed version ofinputten",
        "Y": "dimensiondimis input fromstarttostart+length",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    {
        "X": "Input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow what",
        "Y": "start",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    {
        "X": "What kind of (sub)gradients does this function produce?",
        "Y": "deterministic",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin).",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "Returns a namedtuple(values,indices) wherevaluesis the minimum value of each row of the inputtensor",
        "Y": "wherevaluesis the minimum value of each row of theinputtensor in the given dimensiondim",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin).",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "What is the value of each row of the inputtensor in the given dimensiondim?",
        "Y": "the minimum value of each row of theinputtensor in the given dimensiondim",
        "Z": "Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin).",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "If the output tensors are of the same size as input except in the dimensiondimwhere they are of size 1 what is the default",
        "Y": "IfkeepdimisTrue",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum().",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "If keepdimisTrue, the output tensors are of the same size asinput except in the dimensiondim where they",
        "Y": "1 fewer dimension",
        "Z": "Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "If keepdimisTrue, the output tensors are of the same size as input except in the dimensiondimwhere they are",
        "Y": "1 fewer dimension",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum().",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "What are returned if there are multiple minimal values in a reduced row?",
        "Y": "the indices of the first minimal value",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum().",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "What is the function called for calling LAPACK's geqrf directly?",
        "Y": "low-level function",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf.",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    {
        "X": "What must the rows ofinput be?",
        "Y": "non-negative, finite and have a non-zero sum",
        "Z": "Returns a tensor where each row containsnum_samplesindices sampled\nfrom the multinomial probability distribution located in the corresponding row\nof tensorinput. Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples.",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "Indices are ordered from left to right according to when each was sampled.",
        "Y": "first samples are placed in first column",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "How many rows of inputdo not need to sum to one?",
        "Y": "rows ofinputdo not need to sum to one",
        "Z": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement.",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "Ifinputis a vector,outis a matrix of what?",
        "Y": "matrix withmrows",
        "Z": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement.",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What must the rows of inputdo not need to sum to one?",
        "Y": "non-negative, finite and have a non-zero sum",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement.",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "Ifinputis a matrix withmrows,outis a matrix of shape(mnum_samples)(m",
        "Y": "Ifinputis a vector",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement.",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "How are samples ordered from left to right?",
        "Y": "first samples are placed in first column",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "Ifinputis a matrix,outis a matrix of shape(mnum_samples)(m times",
        "Y": "matrix withmrows",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "If a sample index is drawn for what, it cannot be drawn again for that row?",
        "Y": "a row",
        "Z": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What is the difference between a vector and a matrix of sizenum_samples?",
        "Y": "Note",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What is outputis a vector of sizenum_samples?",
        "Y": "Ifinputis a vector",
        "Z": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "If what is true, samples are drawn with replacement. If not, they are drawn without replacement?",
        "Y": "If replacement isTrue",
        "Z": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "When a sample index is drawn without replacement, it cannot be drawn again for what row?",
        "Y": "a row",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "If num_samples is lower than the min number of non-zero elements in each row of input, what is it?",
        "Y": "matrix",
        "Z": "If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "num_samples must be lower than the min number of non-zero elements in each row of input if it is a",
        "Y": "matrix",
        "Z": "Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What is an example of a pseudorandom number generator?",
        "Y": "Example",
        "Z": "Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What does PyTorch use to represent neural networks?",
        "Y": "modules",
        "Z": "PyTorch uses modules to represent neural networks. Modules are: Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are the building blocks of stateful computation?",
        "Y": "Modules",
        "Z": "PyTorch uses modules to represent neural networks. Modules are: Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How does PyTorch integrate with its autogradsystem?",
        "Y": "Tightly integrated",
        "Z": "PyTorch uses modules to represent neural networks. Modules are: Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What type of networks can PyTorch modules allow for?",
        "Y": "multi-layer neural networks",
        "Z": "Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How does PyTorch'sautogradsystem integrate?",
        "Y": "Tightly integrated",
        "Z": "Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are some of PyTorch's modules able to do?",
        "Y": "prune, quantize",
        "Z": "Building blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is PyTorch's Optimizers tightly integrated with?",
        "Y": "PyTorch\u2019sautogradsystem",
        "Z": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are modules easy to work with and do?",
        "Y": "transform",
        "Z": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How are modules to save and restore?",
        "Y": "straightforward",
        "Z": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are so fundamental to PyTorch?",
        "Y": "modules",
        "Z": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are the Building Blocks of Neural Network Training?",
        "Y": "Modules Module State",
        "Z": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What type of training does A Simple Custom Module Modules provide?",
        "Y": "Neural Network Training",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What module applies an affine transformation to its input?",
        "Y": "PyTorch\u2019sLinearmodule",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features?",
        "Y": "Simple Custom Module Modules",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the name of the advanced features of Neural Network Training?",
        "Y": "Neural Network Training with Modules Module State Module Hooks Advanced Features",
        "Z": "Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What module uses an affine transformation to its input?",
        "Y": "PyTorch\u2019sLinearmodule",
        "Z": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What should subclassModule for composability with other modules?",
        "Y": "module",
        "Z": "It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What do random-initializedweightandbiastensors define?",
        "Y": "affine transformation",
        "Z": "This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is each of the randomly-initializedweightandbiastensors defined as?",
        "Y": "aParameter",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Parameters can be considered what?",
        "Y": "the \u201clearnable\u201d aspects of the module\u2019s computation",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are not required to have state, and can also be stateless?",
        "Y": "modules",
        "Z": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "The input is matrix-multiplied with what?",
        "Y": "theweightparameter",
        "Z": "It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can theforward()implementation for a module perform?",
        "Y": "arbitrary computation involving any number of inputs and outputs",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are aspects of a module's computation that should be learned?",
        "Y": "the parameters registered by a module",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does PyTorch use to update parameters?",
        "Y": "Optimizers",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can contain other modules, making them useful building blocks for developing more elaborate functionality?",
        "Y": "Modules",
        "Z": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto():",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What allows us to chain together multiple modules?",
        "Y": "theSequentialmodule",
        "Z": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto():",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does Sequential feed the output of the first MyLinearmodule as input into?",
        "Y": "theReLU",
        "Z": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does Sequentialautomatically feed the output of the first MyLinearmodule as input into?",
        "Y": "theReLU",
        "Z": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the limit of Sequentialautomatically feeding the output of the firstMyLinearmodule as input into the secondMyLinearmodule?",
        "Y": "in-order chaining of modules",
        "Z": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Why is it recommended to define a custom module for anything beyond the simplest use cases?",
        "Y": "full flexibility on how submodules are used for a module\u2019s computation",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does defining a custom module give you?",
        "Y": "full flexibility",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are the two submodules of a neural network called?",
        "Y": "children",
        "Z": "For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can be iterated through via a call tochildren() ornamed_children()?",
        "Y": "children",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What do TheModuleListandModuleDictmodules do?",
        "Y": "register submodules from a list or dict",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Sometimes, it\u2019s necessary for a module to do what?",
        "Y": "dynamically define submodules",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What do calls toparameters() andnamed_parameters() do?",
        "Y": "recursively include child parameters",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children(): To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are the parameters of a module?",
        "Y": "its direct parameters as well as the parameters of all submodules",
        "Z": "For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto():",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What provides a large library of performant modules within thetorch.nnnamespace?",
        "Y": "PyTorch",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What do we give in the next section of training a neural network?",
        "Y": "a full example",
        "Z": "For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can you find on PyTorch's website?",
        "Y": "more information",
        "Z": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can be used to optimize a neural network?",
        "Y": "PyTorch\u2019s Optimizers",
        "Z": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can be used to optimize a network's parameters?",
        "Y": "PyTorch\u2019s Optimizers",
        "Z": "For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can be used to optimize a neural net?",
        "Y": "PyTorch\u2019s Optimizers",
        "Z": "Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the name of the optimizer that is created?",
        "Y": "stochastic gradient descent optimizer",
        "Z": "An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does the network do?",
        "Y": "computes a loss",
        "Z": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected):",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is an example of training a neural network?",
        "Y": "Using Optimizers",
        "Z": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d):",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does loss.backward() call to apply the gradients to the parameters?",
        "Y": "optimizer.step()",
        "Z": "calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Who computes a loss, zeros the network's parameters' gradients, calls loss.backward() to update the parameters' gradients",
        "Y": "runs the network",
        "Z": "runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can sometimes be tricky?",
        "Y": "Training neural networks",
        "Z": "zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How many layers does _layer_net_optim.html have?",
        "Y": "two",
        "Z": "Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is called to apply gradients to the parameters?",
        "Y": "optimizer.step()",
        "Z": "calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How many layers of a neural network can be optimized?",
        "Y": "two",
        "Z": "calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the value ofl1\u2019sweightparameter now closer to?",
        "Y": "0",
        "Z": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What type of training can be tricky?",
        "Y": "Neural network training",
        "Z": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can we save if we want to save a model to disk?",
        "Y": "itsstate_dict",
        "Z": "In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d):",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is itsstate_dict?",
        "Y": "state dictionary",
        "Z": "In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d):",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does a module'sstate_dict contain?",
        "Y": "state that affects its computation",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "A module'sstate_dict includes state that affects its computation. This includes, but is not limited to, what?",
        "Y": "the module\u2019s parameters",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "For some modules, it may be useful to have state beyond parameters that affects module computation but is what?",
        "Y": "not learnable",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does PyTorch provide for state beyond parameters that affects module computation but is not learnable?",
        "Y": "buffers",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Parameters are contained within what?",
        "Y": "thestate_dict",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Where are learnable aspects of computation contained?",
        "Y": "thestate_dict",
        "Z": "Parameters: learnable aspects of computation; contained within thestate_dict Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are non-learnable aspects of computation?",
        "Y": "Persistentbuffers",
        "Z": "Parameters: learnable aspects of computation; contained within thestate_dict Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are non-persistentbuffers?",
        "Y": "not contained within thestate_dict",
        "Z": "Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Non-persistentbuffers are left out of what?",
        "Y": "serialization",
        "Z": "Parameters: learnable aspects of computation; contained within thestate_dict Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Where are Persistentbuffers contained?",
        "Y": "thestate_dict",
        "Z": "Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Why are non-persistentbuffers not contained within thestate_dict?",
        "Y": "left out of serialization",
        "Z": "Buffers: non-learnable aspects of computation Persistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading) Non-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the purpose of the current value of the running mean?",
        "Y": "it will be restored when loading a serialized form of the module",
        "Z": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is considered part of the module\u2019sstate_dictand?",
        "Y": "the current value of the running mean",
        "Z": "Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "For what do you need to check out:",
        "Y": "more information",
        "Z": "Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can be iterated over usingbuffers() ornamed_buffers()?",
        "Y": "Buffers of a module",
        "Z": "Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html What is a state dict?https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "For more information, check out what?",
        "Y": "Saving and loading",
        "Z": "Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html What is a state dict?https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "When are backward hooks called?",
        "Y": "when the backward for this Module has been computed",
        "Z": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What allows the user to return an updated value that will be used throughout the remaining computation?",
        "Y": "hooks",
        "Z": "All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What company provides several more advanced features that are designed to work with modules?",
        "Y": "PyTorch",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What happens to PyTorch's functionalities when writing a new module?",
        "Y": "inherited",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What type of discussion of PyTorch's advanced features can be found in the links below?",
        "Y": "In-depth",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is another name for profiling?",
        "Y": "Pruning",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Exporting modules to what?",
        "Y": "TorchScript",
        "Z": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What language is TorchScript used for?",
        "Y": "C++",
        "Z": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is Atorch.Tensoris?",
        "Y": "multi-dimensional matrix",
        "Z": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "What type of tensoris float64?",
        "Y": "double",
        "Z": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "What are some examples of atorch.Tensor attributes?",
        "Y": "thetorch.dtype,torch.device, andtorch.layoutattributes",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of what",
        "Y": "atorch.Tensor",
        "Z": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "What does current implementation oftorch.Tensor introduce?",
        "Y": "memory overhead",
        "Z": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "What Returns a new Tensor withdataas the tensor data?",
        "Y": "new_tensor",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "What is the order of a tensor along a given axis in dims?",
        "Y": "n-D",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    {
        "X": "Why is copying a tensor's data slower than numPy'snp.flip?",
        "Y": "more work",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    {
        "X": "What does dims(a listortuple) represent?",
        "Y": "axis to flip on",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    {
        "X": "Computes what of either a matrix or batch of matricesinput?",
        "Y": "singular value decomposition",
        "Z": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the singular value decomposition represented as?",
        "Y": "namedtuple(U, S, V)",
        "Z": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the transpose ofVfor?",
        "Y": "real inputs",
        "Z": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "Ifinputis a batch of what, thenU,S, andVare also batched with the same batch dimensions asinput?",
        "Y": "matrices",
        "Z": "Computes the singular value decomposition of either a matrix or batch of\nmatricesinput. The singular value decomposition is represented as a\nnamedtuple(U, S, V), such thatinput= U diag(S) V\u1d34.\nwhereV\u1d34is the transpose ofVfor real inputs,\nand the conjugate transpose ofVfor complex inputs.\nIfinputis a batch of matrices, thenU,S, andVare also\nbatched with the same batch dimensions asinput. IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the default value of the method that returns the reduced singular value decomposition?",
        "Y": "IfsomeisTrue",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "If someisTrue(default), the returnedUandVmatrices will contain what?",
        "Y": "onlymin(n, m)orthonormal columns",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "Ifcompute_uvisFalse, the returnedUandVwill be what?",
        "Y": "zero-filled matrices",
        "Z": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What effect does argumentsome have whencompute_uvisFalse?",
        "Y": "argumentsomehas no effect whencompute_uvisFalse",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What data type is supported by float, double, cfloat and cfloat?",
        "Y": "cdouble",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "The dtypes ofUandVare the same asinput\u2019s.Swill always be what?",
        "Y": "real-valued",
        "Z": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is deprecated in favor of oftorch.linalg.svd()?",
        "Y": "torch.svd()",
        "Z": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "Along with cfloat and cfloat, what data type is supported by Torch?",
        "Y": "cdouble",
        "Z": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd():",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "The dtypes ofUandVare will always be what?",
        "Y": "real-valued",
        "Z": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd():",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What should be replaced with Note Differences?",
        "Y": "withtorch.linalg.svd()",
        "Z": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd():",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What should be replaced with _,S,_=torch.svd(A,some=some,compute_uv",
        "Y": "U,S,V",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the default value for both oftorch.linalg.svd()'sfull_matrices?",
        "Y": "default value for both isTrue",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What should be replaced with withtorch.linalg.svd()?",
        "Y": "Note Differences",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What does the implementation oftorch.linalg.svd() on CPU use instead of?gesvdfor speed?",
        "Y": "LAPACK\u2019s routine?gesdd",
        "Z": "The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What version of CUDA does cuSOLVER's routinesgesvdjandgesvdjBatchedon?",
        "Y": "10.1.243",
        "Z": "The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What happens when a matrix is represented as a column-major matrix?",
        "Y": "returnedUwill not be contiguous",
        "Z": "Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "The matrix (or batch of matrices) will be represented as what?",
        "Y": "column-major matrix",
        "Z": "Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "The matrix (or batch of matrices) will be represented as a column-major matrix (i.e. what?",
        "Y": "Fortran-contiguous",
        "Z": "Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the name of the warning that the matrix will not be contiguous?",
        "Y": "Warning",
        "Z": "The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What does returnedUnot be contiguous?",
        "Y": "returnedUwill not be contiguous",
        "Z": "Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "When will the gradients with respect toUandVonly be finite?",
        "Y": "when the input does not have zero nor repeated singular values",
        "Z": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "When do the gradients with respect toUandVbe numerically unstable?",
        "Y": "when the matrix has small singular values",
        "Z": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "When do gradients depend onS1?",
        "Y": "when the matrix has small singular values",
        "Z": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the warning if the distance between singular values is close to zero?",
        "Y": "Warning",
        "Z": "Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What can be multiplied byUandV?",
        "Y": "arbitrary phase factor",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What can be used to multiply the columns of the spanning subspace inUandV?",
        "Y": "a rotation matrix",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "Different platforms, like NumPy, or inputs on different device types, may produce what?",
        "Y": "differentUandVtensors",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is used to multiply the columns of the spanning subspace inUandV?",
        "Y": "a rotation matrix",
        "Z": "For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the input tensor of size(*, m, n)where*is zero or more batch dimensions consisting of(",
        "Y": "input(Tensor)",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What controls whether to compute the reduced or full decomposition?",
        "Y": "some(bool,optional)",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the default value of some(bool,optional) that controls whether to compute the reduced or full decomposition?",
        "Y": "Default:True",
        "Z": "input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the name of the function that controls whether to computeUandV?",
        "Y": "compute_uv",
        "Z": "input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the default value of compute_uv(bool,optional)?",
        "Y": "Default:True",
        "Z": "input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the output tuple of tensors?",
        "Y": "out(tuple,optional)",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What does SciPy do with the following cases?",
        "Y": "Computesinput*log(other)",
        "Z": "Computesinput*log(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlogy. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy"
    },
    {
        "X": "Computesinput*log(other)with the following cases. Similar to what?",
        "Y": "SciPy\u2019sscipy.special.xlogy",
        "Z": "Computesinput*log(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlogy. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy"
    },
    {
        "X": "At least one of inputorothermust be what?",
        "Y": "a tensor",
        "Z": "Computesinput*log(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlogy. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy"
    },
    {
        "X": "Decodes a DLPack to what?",
        "Y": "a tensor",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once.",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    {
        "X": "What is a PyCapsule object with the dltensor?",
        "Y": "dlpack",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once.",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    {
        "X": "How many times can a DLPack be consumed?",
        "Y": "once",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once.",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    {
        "X": "What is the tensor represented by?",
        "Y": "DLPack",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once.",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    {
        "X": "Converts a float tensor to what?",
        "Y": "a per-channel quantized tensor",
        "Z": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    {
        "X": "What is input(Tensor) to quantize scales?",
        "Y": "float tensor",
        "Z": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    {
        "X": "What must be used to convert a float tensor to a per-channel quantized tensor?",
        "Y": "one of the quantized dtypes",
        "Z": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    {
        "X": "What is input(Tensor) used to quantize scales?",
        "Y": "float tensor",
        "Z": "input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    {
        "X": "What is an example of a quantized tensor Tensor?",
        "Y": "newly quantized tensor Tensor Example:",
        "Z": "input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    {
        "X": "What does the context-manager do if it has been disabled viano_gradorset_grad_enabled?",
        "Y": "Enables gradient calculation",
        "Z": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad"
    },
    {
        "X": "Why is this context manager thread local?",
        "Y": "it will not affect computation in other threads",
        "Z": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad"
    },
    {
        "X": "What does this context manager function as?",
        "Y": "decorator",
        "Z": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad"
    },
    {
        "X": "Make sure to instantiate with what?",
        "Y": "parenthesis",
        "Z": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled viano_gradorset_grad_enabled. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note enable_grad is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad"
    },
    {
        "X": "What might be complex?",
        "Y": "eigenvalues and eigenvectors",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "When input is on CUDA torch.eig()causes what?",
        "Y": "host-device synchronization",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does it mean to split a tensor into a specific number of chunks?",
        "Y": "Concatenates the given sequence ofseqtensors in the given dimension",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension. Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the difference between a tensor and a specific number of chunks?",
        "Y": "Splits a tensor into a specific number of chunks",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor with three or more dimensions?",
        "Y": "Splitsinput",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "How does Splitsinput create a new tensor?",
        "Y": "horizontally stacking",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Stack tensors in sequence depthwise?",
        "Y": "Gathers values along an axis specified bydim",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What axis does Splitsinput gather values along?",
        "Y": "bydim",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "How many chunks does a tensor split into?",
        "Y": "chunks",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "How many dimensions does Splitsinput have?",
        "Y": "one or more dimensions",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the tensor that Stacks tensors in sequence depthwise?",
        "Y": "Gathers values along an axis specified bydim",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is Splitsinput?",
        "Y": "a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "How do tensors stack in sequence?",
        "Y": "horizontally",
        "Z": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "How does a tensor create a new tensor?",
        "Y": "horizontally stacking",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Stack tensors in sequence depthwise mean?",
        "Y": "third axis",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is acolumn wise?",
        "Y": "Stack tensors in sequence horizontally",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the tensor that indexes theinputtensor along dimensiondimusing the entries inindex?",
        "Y": "aLongTensor",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the length of Stack tensors in sequence depthwise?",
        "Y": "third axis",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Stack tensors in sequence depthwise gather?",
        "Y": "Gathers values along an axis specified bydim",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "How do Stack tensors in sequence?",
        "Y": "horizontally",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the new tensor that indexes theinputtensor along dimensiondimusing the entries inindex?",
        "Y": "aLongTensor",
        "Z": "Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the new tensor that indexes theinputtensor according to the boolean maskmask?",
        "Y": "1-D",
        "Z": "Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What type of tensor is in sequence horizontally?",
        "Y": "Stack",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a new tensor that indexes theinputtensor along dimensiondimusing the entries inindex?",
        "Y": "aLongTensor",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination?",
        "Y": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is dimensiondimusing the entries inindex?",
        "Y": "aLongTensor",
        "Z": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a new tensor that indexes theinputtensor according to the boolean maskmask?",
        "Y": "1-D",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the tensor that is a narrowed version ofinputtensor?",
        "Y": "Alias oftorch.vstack()",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What type of version ofinputtensor is Alias fortorch.movedim()?",
        "Y": "narrowed",
        "Z": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What version ofinputtensor is Alias fortorch.movedim()?",
        "Y": "narrowed",
        "Z": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the new tensor that indexes theinputtensor according to the boolean maskmask",
        "Y": "1-D",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the tensor that indexes theinputtensor according to the boolean maskmask?",
        "Y": "Alias oftorch.vstack()",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a tensor have with the same data and number of elements asinput?",
        "Y": "the specified shape",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Alias fortorch.movedim() have?",
        "Y": "the specified shape",
        "Z": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor with all the dimensions ofinput?",
        "Y": "size1removed",
        "Z": "Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What happens to a sequence of tensors?",
        "Y": "Concatenates a sequence of tensors along a new dimension",
        "Z": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What type of version ofinputtensor is a new tensor?",
        "Y": "narrowed",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a new tensor have?",
        "Y": "arcsine",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the tensor that returns a tensor with the same data and number of elements asinput?",
        "Y": "Alias oftorch.vstack()",
        "Z": "Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a tensor concatenate?",
        "Y": "a sequence of tensors along a new dimension",
        "Z": "Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the generator object generate?",
        "Y": "pseudo random numbers",
        "Z": "Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does atorch.ByteTensor mean?",
        "Y": "Sets the seed for generating random numbers",
        "Z": "Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located",
        "Y": "Draws binary random numbers (0 or 1) from a Bernoulli distribution",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is atorch.ByteTensor?",
        "Y": "random number generator state",
        "Z": "Sets the seed for generating random numbers to a non-deterministic random number.   Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What are given to a tensor of random numbers drawn from separate normal distributions?",
        "Y": "mean and standard deviation",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the state that draws binary random numbers from a Bernoulli distribution?",
        "Y": "Sets the random number generator state",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a Bernoulli distribution draw?",
        "Y": "Draws binary random numbers",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What parameter is given by the corresponding element ininputi?",
        "Y": "rate parameter given by the corresponding element ininputi",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a tensor return with each element sampled from a Poisson distribution with rate parameter given by the corresponding element inin",
        "Y": "a tensor of the same size asinput",
        "Z": "Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Where is a tensor filled with random numbers?",
        "Y": "uniform distribution",
        "Z": "Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Where is a tensor filled with random integers generated?",
        "Y": "betweenlow(inclusive) andhigh(exclusive)",
        "Z": "Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor with the same size asinput filled with random numbers?",
        "Y": "uniform distribution",
        "Z": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh?",
        "Y": "exclusive",
        "Z": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "Y": "tensor",
        "Z": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "Y": "same shape",
        "Z": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the same shape as Tensorinputfilled with random integers generated?",
        "Y": "uniformly",
        "Z": "Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a random permutation of integers from0ton-1?",
        "Y": "mean 0 and variance 1.",
        "Z": "Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor filled with random numbers from a normal distribution with mean0and variance1?",
        "Y": "random permutation of integers from0ton-1",
        "Z": "Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a random permutation of integers from0ton-1 mean?",
        "Y": "mean 0 and variance 1.",
        "Z": "Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What are some more in-place random sampling functions defined on?",
        "Y": "Tensors",
        "Z": "Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Where are there more in-place random sampling functions defined?",
        "Y": "Tensors",
        "Z": "There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What type of distribution does tensor.log_normal_() originate from?",
        "Y": "log-normal distribution",
        "Z": "There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What type of distribution did the numbers sample from?",
        "Y": "discrete uniform distribution",
        "Z": "torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is saved to a disk file?",
        "Y": "an object",
        "Z": "Saves an object to a disk file.   Loads an object saved withtorch.save()from a file.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Loads an object saved from a file?",
        "Y": "withtorch.save()",
        "Z": "Saves an object to a disk file.   Loads an object saved withtorch.save()from a file.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the number of threads used for inter-op parallelism on CPU?",
        "Y": "the number of threads used for interop parallelism",
        "Z": "Returns the number of threads used for parallelizing CPU operations   Sets the number of threads used for intraop parallelism on CPU.   Returns the number of threads used for inter-op parallelism on CPU (e.g.   Sets the number of threads used for interop parallelism (e.g.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a Context-manager?",
        "Y": "disabled gradient calculation",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Context-manager enable?",
        "Y": "gradient calculation",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Context-manager set gradient calculation to?",
        "Y": "on or off",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does inference mode return if inference mode is currently enabled?",
        "Y": "True",
        "Z": "Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Who returns a new tensor with the inverse hyperbolic cosine of the elements ofinput?",
        "Y": "Alias fortorch.acosh()",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Alias fortorch.abs() return a new tensor?",
        "Y": "inverse hyperbolic cosine",
        "Z": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the scalarotherto each element of inputinput return a new resulting tensor?",
        "Y": "Alias fortorch.acosh()",
        "Z": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Alias fortorch.abs() compute the inverse cosine of each element ininput?",
        "Y": "Alias fortorch.acos()",
        "Z": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Alias fortorch.acos() compute?",
        "Y": "inverse cosine",
        "Z": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What type of cosine does Alias fortorch.acosh() return a new tensor?",
        "Y": "hyperbolic",
        "Z": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the cosine of elements ofinput?",
        "Y": "hyperbolic",
        "Z": "Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Alias fortorch.acosh() perform?",
        "Y": "the element-wise multiplication",
        "Z": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the inverse of the elements ofinput?",
        "Y": "hyperbolic sine",
        "Z": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does add the scalarotherto each element of the inputinputand returns a new resulting tensor?",
        "Y": "Alias fortorch.acosh()",
        "Z": "Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the scalarotherto multiply the result by the scalarvalueand add it toinput?",
        "Y": "element-wise division",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the element-wise division oftensor1bytensor2 perform?",
        "Y": "the element-wise multiplication",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the element-wise angle of the giveninputtensor?",
        "Y": "radians",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the elements ofinput?",
        "Y": "arcsine",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does each element of inputinput return a new resulting tensor?",
        "Y": "scalarotherto",
        "Z": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Who returns a new tensor with the arcsine of the elements ofinput?",
        "Y": "Alias fortorch.asin()",
        "Z": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Performs what division of oftensor1bytensor2?",
        "Y": "element-wise division",
        "Z": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the element-wise division oftensor1bytensor2 multiply the result by?",
        "Y": "scalarvalue",
        "Z": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Alias fortorch.asin() have?",
        "Y": "inverse hyperbolic sine",
        "Z": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the element-wise angle of the elements ofinput?",
        "Y": "arcsine",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Alias fortorch.atan() have?",
        "Y": "arctangent",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the new tensor with the arctangent of the elements ofinput?",
        "Y": "Alias fortorch.asinh()",
        "Z": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Who returns a new tensor with the arcsine of elements ofinput?",
        "Y": "Alias fortorch.asin",
        "Z": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the inverse hyperbolic sine of elements ofinput?",
        "Y": "arctangent",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the tensor that returns a new tensor with the inverse hyperbolic sine of the elements of",
        "Y": "Alias fortorch.atanh()",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the element ofinput that returns a new tensor?",
        "Y": "arcsine",
        "Z": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the tensor that returns a new tensor with the arctangent of the elements ofinput",
        "Y": "Alias fortorch.atan",
        "Z": "Computes the absolute value of each element ininput.   Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the new tensor with the inverse hyperbolic tangent of the elements ofinput?",
        "Y": "Alias fortorch.atanh",
        "Z": "Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Alias fortorch.atanh() have?",
        "Y": "inverse hyperbolic tangent",
        "Z": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the element-wise arctangent ofinputi/otheritextinput_i",
        "Y": "Alias fortorch.atanh",
        "Z": "Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the tangent of elements ofinput?",
        "Y": "hyperbolic",
        "Z": "Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Alias fortorch.asinh() do?",
        "Y": "Computes the bitwise OR ofinputandother",
        "Z": "Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the indices of the maximum value of all elements in?",
        "Y": "theinputtensor",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the minimum value of a flattened tensor?",
        "Y": "the flattened tensor or along a dimension",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What returns the indices of the minimum value of each slice of theinputtensor in the given dimension(s)dim?",
        "Y": "the minimum value of each slice of theinputtensor in the given dimension(s)dim",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Where is the minimum value of all elements in theinputtensor?",
        "Y": "theinputtensor",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the return of (input-other) elements in theinputtensor?",
        "Y": "p-norm",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the return of the maximum value of each slice of theinputtensor in the given dimension(s)dim?",
        "Y": "the minimum value of each slice of theinputtensor in the given dimension(s)dim",
        "Z": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the input tensor test if all elements ininputevaluate to?",
        "Y": "Tests if all elements ininputevaluate toTrue",
        "Z": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the minimum value of each slice of theinputtensor in the given dimension(s)dim?",
        "Y": "the flattened tensor or along a dimension",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the minimum value of all elements in?",
        "Y": "theinputtensor",
        "Z": "Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "How do all elements ininputevaluate toTrue?",
        "Y": "Tests",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the mean value of all elements in theinputtensor?",
        "Y": "median",
        "Z": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does ignoringNaNvalues mean?",
        "Y": "the median of the values ininput",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does theinputtensor return?",
        "Y": "the product of all elements in theinputtensor",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the input tensor ignore?",
        "Y": "the median of the values ininput",
        "Z": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the return of the median of the values ininput?",
        "Y": "the mean value of all elements in theinputtensor",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the return of the log of summed exponentials of each row of theinputtensor in the given dimensiondim?",
        "Y": "the mean value of all elements in theinputtensor",
        "Z": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the mean value of all elements in?",
        "Y": "theinputtensor",
        "Z": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does ignoringNaNvalues mean value in theinputtensor?",
        "Y": "the median of the values ininput",
        "Z": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of a value in a given dimensiondim?",
        "Y": "a namedtuple",
        "Z": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a given tensor return?",
        "Y": "matrix norm or vector norm",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Not a Numbers (NaNs) is treated as what?",
        "Y": "zero",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does not a Numbers return?",
        "Y": "the product of all elements in theinputtensor",
        "Z": "Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the return of the values ininput?",
        "Y": "median",
        "Z": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the quantile of each row of theinputtensor?",
        "Y": "q-th",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "In what order does the indices sort a tensor along a given dimension?",
        "Y": "ascending order by value",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Computes element-wise equality?",
        "Y": "Trueif two tensors have the same size and elements",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the name of the tensor?",
        "Y": "Alias fortorch.gt()",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "How do two tensors have the same size and elements?",
        "Y": "Trueif two tensors have the same size and elements",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the tensor that has the same size and elements?",
        "Y": "Alias fortorch.ge()",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Alias fortorch.ge() use to return a new tensor with boolean elements representing if",
        "Y": "Computesinput",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Who returns a new tensor with boolean elements representing if each element isfiniteor not?",
        "Y": "Alias fortorch.gt()",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What represents if each element ofinputis close to the corresponding element ofother?",
        "Y": "boolean elements",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of a new tensor with boolean elements representing if each element isfiniteor not?",
        "Y": "Alias fortorch.le()",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of Alias fortorch.ge()?",
        "Y": "Computesinput",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a new tensor with boolean elements representing if each element ofinputis close to the corresponding element of",
        "Y": "Alias fortorch.gt()",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a test if each element ofinputis positive or negative infinity?",
        "Y": "infinite",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the test if each element ofinputis infinite?",
        "Y": "positive infinity",
        "Z": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What type of input does Alias fortorch.gt() use?",
        "Y": "Computesinput",
        "Z": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a new tensor with boolean elements representing if each element isfiniteor not?",
        "Y": "Alias fortorch.gt()",
        "Z": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding",
        "Y": "Alias fortorch.gt()",
        "Z": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What represents if each element ofinputis \u201cclose\u201d to the corresponding element ofother?",
        "Y": "boolean elements",
        "Z": "Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the test if each element ofinputis infinite or not?",
        "Y": "positive infinity",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What test does each element ofinputis negative infinity or not?",
        "Y": "Tests",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a positive or negative infinity test?",
        "Y": "infinite",
        "Z": "Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a new tensor with boolean elements represent if each element ofinputis negative infinity or not?",
        "Y": "Tests",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What represents if each element ofinputis NaN or not?",
        "Y": "boolean elements",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What represents if each element ofinputis real-valued or not?",
        "Y": "boolean elements",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a new tensor with boolean elements represent if each element ofinputis real-valued or not?",
        "Y": "Tests",
        "Z": "Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Tests if each element ofinputis is what?",
        "Y": "positive infinity",
        "Z": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the test if each element ofinputis positive infinity or not?",
        "Y": "negative infinity",
        "Z": "Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is thekth smallest element of each row of theinputtensor in the given dimensiondim?",
        "Y": "a namedtuple(values,indices)",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What happens if each element ofinputis negative infinity or not?",
        "Y": "Tests",
        "Z": "Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the tensor that returns a new tensor with boolean elements representing if each element of",
        "Y": "Alias fortorch.gt()",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the short time Fourier Transform?",
        "Y": "Inverse",
        "Z": "Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What type of window function is Hamming window function?",
        "Y": "Blackman",
        "Z": "Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the blackman window function?",
        "Y": "Hamming",
        "Z": "Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the window function that computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta?",
        "Y": "Hann",
        "Z": "Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the Kaiser window have?",
        "Y": "window lengthwindow_lengthand shape parameterbeta",
        "Z": "Short-time Fourier transform (STFT).   Inverse short time Fourier Transform.   Bartlett window function.   Blackman window function.   Hamming window function.   Hann window function.   Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a view of each input tensor with zero dimensions?",
        "Y": "3-dimensional",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a block diagonal matrix from provided tensors?",
        "Y": "Create a block diagonal matrix",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Broadcastsinputto the shapeshape is similar tobroadcast_tensors() but for what?",
        "Y": "shapes",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "How many dimensions does a 3-dimensional view of each input tensor have?",
        "Y": "zero",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does Count each value in an array of non-negative ints?",
        "Y": "frequency",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of a block diagonal matrix from provided tensors?",
        "Y": "Create a block diagonal matrix",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the same asbroadcast_tensors()?",
        "Y": "shapes",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What are the indices of the buckets to which each value in theinputbelongs?",
        "Y": "the boundaries of the buckets are set byboundaries",
        "Z": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What doesbroadcast_tensors() look for?",
        "Y": "shapes",
        "Z": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What distance does a cartesian product batched between each pair of two collections of row vectors?",
        "Y": "p-norm distance",
        "Z": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a cartesian product of the given sequence of tensors return?",
        "Y": "copy ofinput",
        "Z": "Broadcasts the given tensors according toBroadcasting semantics.   Broadcastsinputto the shapeshape.   Similar tobroadcast_tensors()but for shapes.   Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the cumulative minimum of elements ofinputin the dimensiondim?",
        "Y": "a namedtuple",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the cumulative sum of elements ofinputin the dimensiondim?",
        "Y": "cumulative product",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a reduced matrix-matrix product of matrices stored inbatch1andbatch2?",
        "Y": "add step",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the batch matrix-matrix product of matrices stored inbatch1andbatch2 perform?",
        "Y": "matrix multiplication",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the vectorvec perform?",
        "Y": "matrix-vector product",
        "Z": "Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the matrix-vector product perform?",
        "Y": "batch matrix-matrix product of matrices inbatch1andbatch2",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the matrices inbatch1andbatch2?",
        "Y": "batch matrix-matrix product",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the matricesmat1andmat2 perform?",
        "Y": "matrix multiplication",
        "Z": "Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the matrix-vector product of vectorsvec1andvec2?",
        "Y": "outer-product",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of the batch matrix-matrix product of matrices stored ininputandmat2?",
        "Y": "Alias",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a batch matrix-matrix product of matrices stored?",
        "Y": "ininputandmat2",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices?",
        "Y": "Cholesky",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the outer-product of?",
        "Y": "vectorsvec1andvec2",
        "Z": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a batch matrix-matrix product of matrices inbatch1andbatch2 perform?",
        "Y": "batch matrix-matrix product of matrices inbatch1andbatch2",
        "Z": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices?",
        "Y": "Cholesky",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?",
        "Y": "Cholesky factor matrixuuu",
        "Z": "Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuuu",
        "Y": "Solves",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What matrix product does the Cholesky decomposition of a symmetric positive-definite matrixAAAor return?",
        "Y": "NNN2-D tensors",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu?",
        "Y": "Solves",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu?",
        "Y": "dot product of two 1D tensors",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the Cholesky decomposition of for batches of symmetric positive-definite matrices?",
        "Y": "symmetric positive-definite matrixAAAor",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the inverse of a symmetric positive-definite matrixAAAor?",
        "Y": "returns matrixinv",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the Cholesky factor matrixuuu?",
        "Y": "dot product of two 1D tensors",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a real square matrix?",
        "Y": "eigenvalues and eigenvectors",
        "Z": "Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matricesinputandmat2.   Performs a matrix-vector product of the matrixinputand the vectorvec.   Alias fortorch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product ofinputandvec2.   Alias fortorch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does AAAusing's Cholesky factor matrixuuu compute?",
        "Y": "dot product of two 1D tensors",
        "Z": "Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a real square matrix have?",
        "Y": "eigenvalues and eigenvectors",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a low-level function for calling directly?",
        "Y": "LAPACK\u2019s geqrf",
        "Z": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a positive semidefinite matrix to be inverted?",
        "Y": "Cholesky factor matrixuuu",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Computes what dot product of two 1D tensors?",
        "Y": "dot product of two 1D tensors",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a low-level function for calling LAPACK's geqrf?",
        "Y": "Alias oftorch.outer()",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of Alias fortorch.linalg.det()?",
        "Y": "Alias fortorch.linalg.inv",
        "Z": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What are the numbers of a real square matrix?",
        "Y": "eigenvalues and eigenvectors",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does this function do for calling LAPACK's geqrf directly?",
        "Y": "low-level function",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "Who calculates log determinant of a square matrix or batches of square matrices?",
        "Y": "Alias fortorch.linalg.slogdet",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu: returns matrixinv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrixuuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the eigenvalues and eigenvectors of a real square matrix?",
        "Y": "low-level function",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "As of 0.4, this function does not support what?",
        "Y": "anoutkeyword",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is equivalent totorch.zeros?",
        "Y": "oldtorch.zeros_like",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is the size ofinput that determines the size of the output tensor?",
        "Y": "input(Tensor)",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is equivalent to totorch.zeros(input.size(),out=output)?",
        "Y": "oldtorch.zeros_like",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "Warning As of 0.4, this function does not support what?",
        "Y": "anoutkeyword",
        "Z": "Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is the oldtorch.zeros_like equivalent?",
        "Y": "totorch.zeros",
        "Z": "As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is the size ofinput?",
        "Y": "input(Tensor)",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is the default to the dtype ofinput?",
        "Y": "ifNone",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is the desired layout of tensor?",
        "Y": "layout",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is the default to the layout ofinput?",
        "Y": "ifNone",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "As of what date does this function not support anoutkeyword?",
        "Y": "0.4",
        "Z": "As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is equivalent totorch.zeros(input.size(),out=output)?",
        "Y": "oldtorch.zeros_like",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is the default to the device ofinput?",
        "Y": "ifNone",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is the default for a tensor?",
        "Y": "False",
        "Z": "Returns a tensor filled with the scalar value0, with the same size asinput.torch.zeros_like(input)is equivalent totorch.zeros(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). Warning As of 0.4, this function does not support anoutkeyword. As an alternative,\nthe oldtorch.zeros_like(input,out=output)is equivalent totorch.zeros(input.size(),out=output). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "What is the name of the object that gets the current device of a Generator?",
        "Y": "torch.Generator object",
        "Z": "An torch.Generator object. Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "Generator.device -> device Gets what of the generator?",
        "Y": "current device",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does atorch.Generator object do?",
        "Y": "Sets the seed for generating random numbers",
        "Z": "An torch.Generator object. Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What are remapped to positive values with the formula0xfff_fff_fff_fff + seed?",
        "Y": "Negative inputs",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is an example of a generator that gets a non-deterministic random number from std::random_device?",
        "Y": "torch.Generator object",
        "Z": "An torch.Generator object. Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does a generator get a non-deterministic random number from?",
        "Y": "std::random_device or the current time",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does the generator do?",
        "Y": "Sets the Generator state",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What object is used to seed a Generator?",
        "Y": "torch.Generator",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does it do when a generator is seeded?",
        "Y": "Sets the Generator state",
        "Z": "Generator Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does generator.device -> device get?",
        "Y": "current device",
        "Z": "Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What object is used to get a non-deterministic random number from std::random_device or the current time?",
        "Y": "torch.Generator",
        "Z": "Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does a Generator get a non-deterministic random number from?",
        "Y": "std::random_device or the current time",
        "Z": "Example: Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is the desired state?",
        "Y": "new_state",
        "Z": "Generator.device -> device Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. seed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed. An torch.Generator object. Generator Example: Gets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator. Example: Sets the Generator state. new_state(torch.ByteTensor) \u2013 The desired state.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "IfTrueand the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no",
        "Y": "non_blocking(bool)",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does this storage cast to?",
        "Y": "float type",
        "Z": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Everytorch.Tensorcasts this storage to what type of type?",
        "Y": "bfloat16",
        "Z": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does everytorch.Tensorcast?",
        "Y": "float type",
        "Z": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Casts this storage to what type Casts this storage to bool type Casts this storage to bool type Casts this storage to",
        "Y": "bfloat16",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does Casts this storage to bool type Casts this storage to char type Returns?",
        "Y": "a copy",
        "Z": "Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "IfsharedisTrue, what happens?",
        "Y": "memory is shared between all processes",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "IfsharedisTrue, then memory is shared between all processes.",
        "Y": "All changes are written to the file",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does not affect the file?",
        "Y": "IfsharedisFalse",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does Casts this storage to char type Return?",
        "Y": "a copy",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does Casts this storage to char type return?",
        "Y": "a copy",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Where is a copy of this storage stored?",
        "Y": "CUDA memory",
        "Z": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "IfTrue and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no",
        "Y": "non_blocking(bool)",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of memory is shared between all processes?",
        "Y": "IfsharedisTrue",
        "Z": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does the changes on the storage not affect the file?",
        "Y": "IfsharedisFalse",
        "Z": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is the name of the function that returns a copy of the object in CUDA memory?",
        "Y": "non_blocking(bool)",
        "Z": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the object is already in what memory and on the correct device, then no copy is performed and the original object is returned?",
        "Y": "CUDA memory",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of argument has no effect?",
        "Y": "non_blocking",
        "Z": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is the name of the argument that asynchronous with respect to the host?",
        "Y": "non_blocking(bool)",
        "Z": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens if sharedisTrue?",
        "Y": "memory is shared between all processes",
        "Z": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the changes on the storage do not affect the file, what is the name of the file that must contain at leastsize * sizeof(Type)",
        "Y": "IfsharedisFalse",
        "Z": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the file must contain at leastsize * sizeof(Type)bytes (Typeis the type of storage)?",
        "Y": "IfsharedisFalse",
        "Z": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does filename(str) mean?",
        "Y": "filename(str) \u2013 file name to map shared",
        "Z": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does it do to the storage if it's not already pinned?",
        "Y": "Moves the storage to shared memory",
        "Z": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does Casts this storage to?",
        "Y": "float",
        "Z": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does the changes on the storage do not affect the file?",
        "Y": "IfsharedisFalse",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Returns a list containing the elements of what storage?",
        "Y": "self Casts this storage to short type",
        "Z": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is the name of the file that must contain at leastsize * sizeof(Type)bytes?",
        "Y": "IfsharedisFalse",
        "Z": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What Casts this storage to short type Returns a list containing the elements of this storage?",
        "Y": "self",
        "Z": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. Ion GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "When do gradients with respect toUandV depend onS1?",
        "Y": "when the matrix has small singular values",
        "Z": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What may be multiplied by for complex-valuedinput?",
        "Y": "arbitrary phase factor",
        "Z": "Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What happens when inputhas multiple singular values?",
        "Y": "repeated singular values",
        "Z": "Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What may be multiplied byUandV?",
        "Y": "arbitrary phase factor",
        "Z": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "When is the singular value decomposition not unique?",
        "Y": "wheninputhas repeated singular values",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What setting controls whether to compute the reduced or full decomposition, and consequently, the shape of returnedUandV?",
        "Y": "Default:True",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What controls whether to computeUandV?",
        "Y": "compute_uv",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the default value for compute_uv?",
        "Y": "Default:True",
        "Z": "Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. some(bool,optional) \u2013 controls whether to compute the reduced or full decomposition, and\nconsequently, the shape of returnedUandV. Default:True. compute_uv(bool,optional) \u2013 controls whether to computeUandV. Default:True. out(tuple,optional) \u2013 the output tuple of tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What has to be the desired data type of returned tensor?",
        "Y": "one of the quantized dtypes",
        "Z": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    {
        "X": "When inputis on CUDA,torch.eig()causes what?",
        "Y": "host-device synchronization",
        "Z": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "Where is each row an eigenvalue of input?",
        "Y": "the first element is the real part and the second element is the imaginary part",
        "Z": "input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "Ifeigenvectors=False, what is it?",
        "Y": "empty tensor",
        "Z": "eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What can shape(nn)(n times n)(nn) be used to compute?",
        "Y": "normalized (unit length) eigenvectors",
        "Z": "Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "If the correspondingeigenvalues[j]is a what?",
        "Y": "real number",
        "Z": "eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "If the correspondingeigenvalues[j]andeigenvalues[j + 1]form what?",
        "Y": "a complex conjugate pair",
        "Z": "eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. (Tensor,Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What could a map-style dataset read from a folder on the disk?",
        "Y": "theidx-th image and its corresponding label",
        "Z": "At the heart of PyTorch data loading utility is thetorch.utils.data.DataLoaderclass.  It represents a Python iterable over a dataset, with support for map-style and iterable-style datasets, customizing data loading order, automatic batching, single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Who configures the options for map-style and iterable-style datasets?",
        "Y": "the constructor arguments of aDataLoader",
        "Z": "map-style and iterable-style datasets, customizing data loading order, automatic batching, single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. Foriterable-style datasets, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time).",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of a dataset that could read theidx-th image and its corresponding label from a folder on the disk?",
        "Y": "SeeDataset",
        "Z": "For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Automatic batching, single- and multi-process data loading, and what other option are configured by the constructor arguments of aDataLoa",
        "Y": "automatic memory pinning",
        "Z": "automatic batching, single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How are the options configured?",
        "Y": "the constructor arguments of aDataLoader",
        "Z": "single- and multi-process data loading, automatic memory pinning. These options are configured by the constructor arguments of aDataLoader, which has signature: The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is an instance of a subclass ofIterableDataset that implements the__iter__()protocol?",
        "Y": "iterable-style dataset",
        "Z": "The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What are iterable-style datasets particularly suitable for?",
        "Y": "cases where random reads are expensive or even improbable",
        "Z": "An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of a map-style dataset that could read theidx-th image and its corresponding label from a folder on",
        "Y": "SeeDataset",
        "Z": "The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "When using anIterableDatasetwith what?",
        "Y": "multi-process data loading",
        "Z": "For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of the dataset that implements the__iter__()protocol?",
        "Y": "SeeDataset",
        "Z": "SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is anIterableDatasetwith?",
        "Y": "multi-process data loading",
        "Z": "The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. Foriterable-style datasets, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time).",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What document describes how to avoid duplicated data when using multi-process data loading?",
        "Y": "SeeIterableDatasetdocumentations",
        "Z": "The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How is the data loading order controlled by the user-defined iterable?",
        "Y": "yielding a batched sample at each time",
        "Z": "The sections below describe in details the effects and usages of these options. The most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets: map-style datasets, iterable-style datasets. A map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples. For example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk. SeeDatasetfor more details. An iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data. For example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time. SeeIterableDatasetfor more details. Note When using anIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this. Foriterable-style datasets, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time).",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What are sampleclasses used to do?",
        "Y": "represent iterable objects over the indices to datasets",
        "Z": "The rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets.  E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD. A sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch. A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What can aSampler do in the common case with stochastic gradient decent?",
        "Y": "randomly permute a list of indices and yield each one at a time",
        "Z": "The rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets.  E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD. A sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch. A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How will a sequential or shuffled sampler be constructed?",
        "Y": "based on theshuffleargument to aDataLoader",
        "Z": "The rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets.  E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD. A sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch. A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Who supports automatically collating individual fetched data samples into batches?",
        "Y": "DataLoader",
        "Z": "A customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this. Note Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first).",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What are batched samples?",
        "Y": "containing Tensors",
        "Z": "Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first). Whenbatch_size(default1) is notNone, the data loader yields\nbatched samples instead of individual samples.batch_sizeanddrop_lastarguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecifybatch_sampler, which yields a list of keys at a time. Note",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What yields batched samples instead of individual samples whenbatch_size(default1) is notNone?",
        "Y": "data loader",
        "Z": "Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first). Whenbatch_size(default1) is notNone, the data loader yields\nbatched samples instead of individual samples.batch_sizeanddrop_lastarguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecifybatch_sampler, which yields a list of keys at a time. Note",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What type of datasets can users alternatively specifybatch_sampler?",
        "Y": "map-style datasets",
        "Z": "Neithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex. DataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last, andbatch_sampler. This is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first). Whenbatch_size(default1) is notNone, the data loader yields\nbatched samples instead of individual samples.batch_sizeanddrop_lastarguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecifybatch_sampler, which yields a list of keys at a time. Note",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "In some cases, users may want to handle batching what?",
        "Y": "manually in dataset code",
        "Z": "and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What type of data would it be cheaper to load manually?",
        "Y": "batched data",
        "Z": "and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Who returns each member of thedatasetobject?",
        "Y": "the data loader",
        "Z": "After fetching a list of samples using the indices from sampler, the function\npassed as thecollate_fnargument is used to collate lists of samples\ninto batches. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "In certain cases, users may want to handle batching what?",
        "Y": "manually",
        "Z": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is used to collate samples?",
        "Y": "automatic batching",
        "Z": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is disabled when bothbatch_sizeandbatch_samplerareNone(default value forbatch_samplerareN",
        "Y": "automatic batching",
        "Z": "In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Each sample obtained from thedatasetis processed with the function passed as what?",
        "Y": "thecollate_fnargument",
        "Z": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What could be cheaper to handle batching manually in dataset code?",
        "Y": "directly load batched data",
        "Z": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the function passed to each sample obtained from thedataset?",
        "Y": "thecollate_fnargument",
        "Z": "When fetching fromiterable-style datasetswithmulti-processing, thedrop_lastargument drops the last non-full batch of each worker\u2019s dataset replica. After fetching a list of samples using the indices from sampler, the function\npassed as thecollate_fnargument is used to collate lists of samples\ninto batches. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does the defaultcollate_fnsimply do when automatic batching is disabled?",
        "Y": "defaultcollate_fnsimply converts NumPy arrays into PyTorch Tensors",
        "Z": "When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is roughly equivalent to loading from when automatic batching is disabled?",
        "Y": "map-style dataset",
        "Z": "A customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn. In certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject. When bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument. When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "When automatic batching is disabled,collate_fnis called with what?",
        "Y": "each individual data sample",
        "Z": "When automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is expected when automatic batching is enabled?",
        "Y": "to collate the input samples into a batch",
        "Z": "In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: Seethis sectionon more aboutcollate_fn. The use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled. When automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors. When automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes behavior of the defaultcollate_fnin this case. For instance, if each data sample consists of a 3-channel image and an integral\nclass label, i.e., each element of the dataset returns a tuple(image,class_index), the defaultcollate_fncollates a list of\nsuch tuples into a single tuple of a batched image tensor and a batched class\nlabel Tensor. In particular, the defaultcollate_fnhas the following\nproperties: It always prepends a new dimension as the batch dimension.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How many subprocesses to use for data loading?",
        "Y": "how many subprocesses to use for data loading",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of the main process that will be loaded in the main process?",
        "Y": "default:0",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What can be used to define the strategy to draw samples from the dataset?",
        "Y": "anyIterablewith__len__implemented",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What must not be specified if specified?",
        "Y": "shuffle",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of how many subprocesses to use for data loading?",
        "Y": "num_workers",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the default value for data loading?",
        "Y": "default:0",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of the sampler that likesampler, but returns a batch of indices at a time?",
        "Y": "batch_sampler",
        "Z": "sampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified. batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the mutually exclusive feature of batch_sampler?",
        "Y": "withbatch_size,shuffle,sampler, anddrop_last",
        "Z": "batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does collate_fn merge a list of samples to form?",
        "Y": "a mini-batch of Tensor",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is used when using a map-style dataset?",
        "Y": "batched loading",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is num_workers?",
        "Y": "total number of workers",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. When called in a worker, this returns an object guaranteed to have the\nfollowing attributes: id: the current worker id. num_workers: the total number of workers. seed: the random seed set for the current worker. This value is\ndetermined by main process RNG and the worker id. SeeDataLoader\u2019s documentation for more details. dataset: the copy of the dataset object inthisprocess. Note\nthat this will be a different object in a different process than the one\nin the main process. When called in the main process, this returnsNone. Note When used in aworker_init_fnpassed over toDataLoader, this method can be useful to\nset up each worker process differently, for instance, usingworker_idto configure thedatasetobject to only read a specific fraction of a\nsharded dataset, or useseedto seed other libraries used in dataset\ncode. Randomly split a dataset into non-overlapping new datasets of given lengths.\nOptionally fix the generator for reproducible results, e.g.: dataset(Dataset) \u2013 Dataset to be split",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What merges a list of samples to form a mini-batch of Tensor(s)?",
        "Y": "collate_fn",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What type of dataset is collate_fn used when using batched loading?",
        "Y": "map-style dataset",
        "Z": "batch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last. num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "When using batched loading from what type of dataset?",
        "Y": "map-style dataset",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What merges a list of samples to form a mini-batch of Tensors?",
        "Y": "collate_fn",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is collate_fn used when using?",
        "Y": "batched loading",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is used when using batched loading from a map-style dataset?",
        "Y": "pin_memory",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "If your data elements are what?",
        "Y": "a custom type",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What pinned memory does the data loader copy Tensors into?",
        "Y": "CUDA",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What type of data elements does yourcollate_fn return a batch that is a custom type?",
        "Y": "a custom type",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of the data loader that will copy Tensors into CUDA pinned memory before returning them?",
        "Y": "pin_memory",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "If your data elements are a what type?",
        "Y": "custom type",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is bool,optional?",
        "Y": "pin_memory",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the size of the dataset not divisible by the batch size?",
        "Y": "IfFalseand the size of dataset is not divisible by the batch size",
        "Z": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of the name of a batch that is not divisible by the batch size?",
        "Y": "default:False",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What type of data elements does yourcollate_fnreturns a batch that is a custom type?",
        "Y": "a custom type",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is set toTrue to drop the last incomplete batch if the dataset size is not divisible by the batch size?",
        "Y": "drop_last",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "If the size of dataset is not divisible by the batch size, what happens?",
        "Y": "the last batch will be smaller",
        "Z": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the default setting for the size of a dataset?",
        "Y": "default:False",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is set toTrueto drop the last incomplete batch?",
        "Y": "drop_last",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the timeout value for collecting a batch from workers?",
        "Y": "timeout",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What should the timeout value for collecting a batch from workers always be?",
        "Y": "non-negative",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the default timeout value for collecting a batch from workers?",
        "Y": "0",
        "Z": "num_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0) collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What will be smaller if the dataset size is not divisible by the batch size?",
        "Y": "the last batch",
        "Z": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the default value for worker_init_fn?",
        "Y": "default:None",
        "Z": "collate_fn(callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset. pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What will be called on each worker subprocess with the worker id (an int in[0,num_workers-1]) as",
        "Y": "worker_init_fn",
        "Z": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "If notNone, worker_init_fn(callable,optional) will be called on each worker subprocess with what input?",
        "Y": "worker id",
        "Z": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Who will use this RNG to generate random indexes?",
        "Y": "RandomSampler",
        "Z": "timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the default value of generator(torch.Generator,optional)?",
        "Y": "default:None",
        "Z": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Who will use generator to generate random indexes and multiprocessing to generatebase_seedfor workers?",
        "Y": "RandomSampler",
        "Z": "generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is int,optional,keyword-only arg?",
        "Y": "prefetch_factor",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How many num_workers samples will be prefetched across all workers?",
        "Y": "2",
        "Z": "drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How many prefetch_factors are there?",
        "Y": "2",
        "Z": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does the data loader not shutdown the worker processes after a dataset has been consumed once?",
        "Y": "IfTrue",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does the data loader allow to keep alive after a dataset has been consumed once?",
        "Y": "Datasetinstances",
        "Z": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of the word \"False\"?",
        "Y": "Warning",
        "Z": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the term for persistent_workers?",
        "Y": "IfTrue",
        "Z": "persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is a worker_init_fncannot be an unpicklable object?",
        "Y": "lambda function",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Where is Multiprocessing best practiceson more details related to multiprocessing?",
        "Y": "PyTorch",
        "Z": "pin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below. drop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False) timeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0) worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does PyTorch have to do with multiprocessing best practices?",
        "Y": "Warning",
        "Z": "prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does persistent_workers allow to keep alive?",
        "Y": "workersDatasetinstances",
        "Z": "persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Worker_init_fncannot be an unpicklable object, e.g., a lambda function.",
        "Y": "thespawnstart method",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Where can you find more details about multiprocessing?",
        "Y": "PyTorch",
        "Z": "worker_init_fn(callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None) generator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None) prefetch_factor(int,optional,keyword-only arg) \u2013 Number of samples loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers samples prefetched across all workers. (default:2) persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does PyTorch do when it comes to multiprocessing?",
        "Y": "Warning",
        "Z": "persistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shutdown\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False) Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is an example of a function that is not unpicklable?",
        "Y": "lambda function",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Where is the Multiprocessing best practiceson more details related to multiprocessing?",
        "Y": "PyTorch",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is an example of a spawnstart method?",
        "Y": "Warning",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Worker_init_fncannot be an unpicklable object, e.g., a lambda function?",
        "Y": "thespawnstart method",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Where can you find more information about multiprocessing?",
        "Y": "PyTorch",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does worker_init_fncannot be an unpicklable object?",
        "Y": "lambda function",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of the multiprocessing best practices?",
        "Y": "PyTorch",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Worker_init_fncannot be an unpicklable object, e.g., what?",
        "Y": "a lambda function",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the warning len(dataloader)heuristic based on?",
        "Y": "the length of the sampler used",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Whendatasetis what, it instead returns an estimate based on onlen(dataset)/batch_size?",
        "Y": "anIterableDataset",
        "Z": "len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does PyTorch trust in handling multi-process loading?",
        "Y": "userdatasetcode",
        "Z": "Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is len(dataloader)heuristic based on?",
        "Y": "the length of the sampler used",
        "Z": "len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "When can more than one batch worth of samples be dropped?",
        "Y": "whendrop_lastis set",
        "Z": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Which type of dataset can not detect such cases in general?",
        "Y": "PyTorch",
        "Z": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What interacts with Multi-process data loading?",
        "Y": "howIterableDataset",
        "Z": "Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What can happen to an otherwise complete batch?",
        "Y": "broken into multiple ones",
        "Z": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What program can not detect sharding cases in general?",
        "Y": "PyTorch",
        "Z": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does howIterableDataset interact with?",
        "Y": "Multi-process data loading",
        "Z": "However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What interacts withMulti-process data loading?",
        "Y": "howIterableDataset",
        "Z": "SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What type of numbers do My data loader workers return?",
        "Y": "random",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What class represents aDataset?",
        "Y": "abstract class",
        "Z": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does My data loader workers return identical random numbers?",
        "Y": "Warning SeeReproducibility",
        "Z": "Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What do multi-process data loadingnotes return for random seed related questions?",
        "Y": "Randomness",
        "Z": "Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does an abstract class represent?",
        "Y": "aDataset",
        "Z": "SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does SeeReproducibility andMy data loader workers return?",
        "Y": "random numbers",
        "Z": "SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Randomness in multi-process data loadingnotes for what?",
        "Y": "random seed related questions",
        "Z": "Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "All datasets that represent what should subclass it?",
        "Y": "a map from keys to data samples",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What should all subclasses overwrite__getitem__() support?",
        "Y": "fetching a data sample for a given key",
        "Z": "SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What are subclasses expected to overwrite__len__()?",
        "Y": "manySamplerimplementations and the default options ofDataLoader",
        "Z": "SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What type of class represents aDataset?",
        "Y": "abstract class",
        "Z": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "All datasets that represent a map from keys to data samples should what?",
        "Y": "subclass it",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What should all subclasses do to support fetching a data sample for a given key?",
        "Y": "overwrite__getitem__()",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What could subclasses optionally do?",
        "Y": "overwrite__len__()",
        "Z": "SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What should all subclasses overwrite to overwrite__len__()?",
        "Y": "Note",
        "Z": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What do all datasets that represent from keys to data samples should subclass it?",
        "Y": "a map",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of a subclass that should overwrite__getitem__()?",
        "Y": "Note",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What should all datasets that represent a map from keys to data samples do?",
        "Y": "subclass it",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does DataLoaderby default construct?",
        "Y": "index sampler",
        "Z": "Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What must be provided to make it work with a map-style dataset with non-integral indices/keys?",
        "Y": "a custom sampler must be provided",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does a custom sampler need to be provided to make it work with a map-style dataset with non-integral indices/",
        "Y": "iterable Dataset",
        "Z": "Warning If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What should all datasets that represent an iterable of data samples should do?",
        "Y": "subclass it",
        "Z": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Where does data come from?",
        "Y": "a stream",
        "Z": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What should all subclasses do?",
        "Y": "overwrite__iter__()",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does DataLoaderby use to create a custom sampler?",
        "Y": "An iterable Dataset",
        "Z": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What default constructs a index sampler that yields integral indices?",
        "Y": "DataLoaderby",
        "Z": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is a custom sampler required to work with a map-style dataset with non-integral indices/keys?",
        "Y": "iterable Dataset",
        "Z": "DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What should all datasets that represent an iterable of data samples do?",
        "Y": "subclass it",
        "Z": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "When are iterable datasets particularly useful?",
        "Y": "when data come from a stream",
        "Z": "If thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch. Warning len(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general. SeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading. Warning SeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions. An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of the dataset that represents an iterable of data samples?",
        "Y": "An iterable Dataset",
        "Z": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is an iterable dataset particularly useful when data come from?",
        "Y": "a stream",
        "Z": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What would return an iterator of samples in an iterable dataset?",
        "Y": "overwrite__iter__()",
        "Z": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is a dataset that represents an iterable of data samples called?",
        "Y": "iterable Dataset",
        "Z": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What type of stream does a subclass of data come from?",
        "Y": "a stream",
        "Z": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "When are subclasses particularly useful?",
        "Y": "when data come from a stream",
        "Z": "All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What would overwrite__iter__() do?",
        "Y": "return an iterator of samples",
        "Z": "All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the example of splitting workload across all workers using worker_init_fn?",
        "Y": "splitting workload across all workers",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What will each sample be retrieved by?",
        "Y": "indexing tensors along the first dimension",
        "Z": "All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What does *tensors (Tensor) mean?",
        "Y": "tensors that have the same size of the first dimension",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is a concatenation of multiple datasets?",
        "Y": "Dataset",
        "Z": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is this class useful to?",
        "Y": "assemble different existing datasets",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the definition of datasets to be concatenated?",
        "Y": "List of datasets to be concatenated",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the example 1 of?",
        "Y": "splitting workload across all workers",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Each sample will be retrieved by what along the first dimension?",
        "Y": "indexing tensors",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What are tensors that have the same size of the first dimension?",
        "Y": "*tensors(Tensor)",
        "Z": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What class is useful to assemble different existing datasets?",
        "Y": "Dataset",
        "Z": "When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is this class useful for?",
        "Y": "assemble different existing dataset streams",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of the list of datasets to be concatenated?",
        "Y": "datasets(sequence) \u2013 List of datasets to be concatenated",
        "Z": "An abstract class representing aDataset. All datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Note DataLoaderby default constructs a index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided. An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is used to split workload across all workers?",
        "Y": "worker_init_fn",
        "Z": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How is each sample retrieved?",
        "Y": "indexing tensors along the first dimension",
        "Z": "Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is a concatenation of multiple datasets useful for?",
        "Y": "to assemble different existing datasets",
        "Z": "Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "What is the name of the list of datasets to be concatenated Dataset for chainning multipleIterableDatasets?",
        "Y": "List of datasets to be concatenated Dataset for chainning multipleIterableDatasets",
        "Z": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream. All subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset. When a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in__iter__(): Example 2: splitting workload across all workers usingworker_init_fn: Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension. *tensors(Tensor) \u2013 tensors that have the same size of the first dimension. Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets. datasets(sequence) \u2013 List of datasets to be concatenated Dataset for chainning multipleIterableDatasets. This class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient. datasets(iterable of IterableDataset) \u2013 datasets to be chained together Subset of a dataset at specified indices. dataset(Dataset) \u2013 The whole Dataset indices(sequence) \u2013 Indices in the whole set selected for subset Returns the information about the currentDataLoaderiterator worker process. When called in a worker, this returns an object guaranteed to have the\nfollowing attributes: id: the current worker id. num_workers: the total number of workers.",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "Ifdimis a list of dimensions, what does it do?",
        "Y": "reduce over all of them",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "If specified, the input tensor is casted what before the operation is performed?",
        "Y": "todtype",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "What is this useful for preventing?",
        "Y": "data type overflows",
        "Z": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "What is the default for the input tensor?",
        "Y": "None",
        "Z": "Returns the sum of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "What is a return of the input tensor in the given dimensiondim?",
        "Y": "the sum of each row of theinputtensor in the given dimensiondim",
        "Z": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "What is returned when a row of the inputtensor is set to a given dimensiondim?",
        "Y": "Returns the sum of each row of theinputtensor in the given dimensiondim",
        "Z": "Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "Ifdimis a list of dimensions, reduce over all of them?",
        "Y": "IfkeepdimisTrue",
        "Z": "Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "What is the name of the output tensor that has fewer dimension(s)?",
        "Y": "orlen(dim)",
        "Z": "Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "What is the sum of each row of theinputtensor in the given dimensiondim?",
        "Y": "Ifdimis a list of dimensions",
        "Z": "Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "What returns the output tensor of the same size as input except in the dimension(s)dim where it is of size 1?",
        "Y": "IfkeepdimisTrue",
        "Z": "Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "What is the name of the output tensor that has fewer dimension(s) than input?",
        "Y": "orlen(dim)",
        "Z": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None. Example: Returns the sum of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "If the output tensor is of the same size as input, what is it?",
        "Y": "IfkeepdimisTrue",
        "Z": "IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "What does the input tensor compute?",
        "Y": "bitwise NOT",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    {
        "X": "What type of type does the input tensor have?",
        "Y": "Boolean types",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    {
        "X": "For what type of tensor, it computes the logical NOT of the input tensor?",
        "Y": "bool tensors",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    {
        "X": "What is out of the input tensor?",
        "Y": "output tensor",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    {
        "X": "What does compute the bitwise NOT of the given input tensor do?",
        "Y": "Computes the bitwise NOT",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    {
        "X": "For what type of tensor does it compute the logical NOT?",
        "Y": "bool tensors",
        "Z": "Computes the bitwise NOT of the given input tensor. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical NOT. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    {
        "X": "What is a tensor with two or more dimensions?",
        "Y": "Splitsinput",
        "Z": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split().",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    {
        "X": "What is the equivalent of calling Splitsinput?",
        "Y": "torch",
        "Z": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split().",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    {
        "X": "What is tensor to split?",
        "Y": "input(Tensor)",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split().",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    {
        "X": "What is used to split a tensor with two or more dimensions into multiple tensors vertically?",
        "Y": "indices_or_sections",
        "Z": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    {
        "X": "What is the difference between splitsinput and torch.tensor_split(input, indices_or_sections",
        "Y": "ifindices_or_sectionsis an integer it must evenly divide the split dimension",
        "Z": "Splitsinput, a tensor with two or more dimensions, into multiple tensors\nvertically according toindices_or_sections. Each split is a view ofinput. This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    {
        "X": "What is the equivalent of calling input, indices_or_sections, dim=0?",
        "Y": "torch.tensor_split",
        "Z": "This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split().",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    {
        "X": "What is the split dimension of torch.tensor_split?",
        "Y": "dim=0",
        "Z": "This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0)\n(the split dimension is 0), except that ifindices_or_sectionsis an integer\nit must evenly divide the split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.vsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split().",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    {
        "X": "What is input to split?",
        "Y": "tensor",
        "Z": "Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split.",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    {
        "X": "What is a tensor with one or more dimensions?",
        "Y": "Splitsinput",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput. Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split().",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    {
        "X": "What is a split in a tensor?",
        "Y": "view ofinput",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    {
        "X": "What are splitsinput according to?",
        "Y": "indices",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors\nhorizontally according toindices_or_sections. Each split is a view ofinput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    {
        "X": "What is the split dimension of tensor_split?",
        "Y": "zero",
        "Z": "Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split.",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    {
        "X": "What is equivalent to calling torch?",
        "Y": "ifinputhas two or more dimensions",
        "Z": "Ifinputis one dimensional this is equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is\nzero), and ifinputhas two or more dimensions it\u2019s equivalent to calling\ntorch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1),\nexcept that ifindices_or_sectionsis an integer it must evenly divide\nthe split dimension or a runtime error will be thrown. This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split.",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    {
        "X": "What is input derived from NumPy'snumpy.hsplit()?",
        "Y": "tensor",
        "Z": "This function is based on NumPy\u2019snumpy.hsplit(). input(Tensor) \u2013 tensor to split. indices_or_sections(Tensor,intorlistortuple of python:ints) \u2013 See argument intorch.tensor_split().",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    {
        "X": "In what direction do the entries in each row of tensor appear in a different order than before?",
        "Y": "left/right",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr.",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    {
        "X": "What is the name of'snp.fliplr'?",
        "Y": "NumPy",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr.",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    {
        "X": "What is torch.fliplris expected to be?",
        "Y": "slower thannp.fliplr",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr.",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    {
        "X": "What does flip tensor in the left/right direction return?",
        "Y": "a new tensor",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr.",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    {
        "X": "What does flip tensor do?",
        "Y": "Flip the entries in each row in the left/right direction",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr.",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    {
        "X": "What must the tensor be at least?",
        "Y": "2-D",
        "Z": "Flip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. Note Requires the tensor to be at least 2-D. Note torch.fliplrmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.fliplr,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.fliplris expected to be slower thannp.fliplr.",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    {
        "X": "What is the name of the elements ofinput that returns a new tensor?",
        "Y": "hyperbolic cosine",
        "Z": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    {
        "X": "What library does torch.cosh use?",
        "Y": "Sleef library",
        "Z": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    {
        "X": "What does the Sleef library use?",
        "Y": "Seeherefor details",
        "Z": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    {
        "X": "Returns what with the hyperbolic cosine of the elements of input?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    {
        "X": "Wheninputis on the CPU, the implementation of torch.cosh may use what library?",
        "Y": "Sleef library",
        "Z": "Returns a new tensor with the hyperbolic cosine  of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Note Wheninputis on the CPU, the implementation of torch.cosh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. Seeherefor details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    {
        "X": "Fillsselftensor with numbers samples from what distribution parameterized by the given meanmuand standard deviationsigma?",
        "Y": "log-normal distribution",
        "Z": "Fillsselftensor with numbers samples from the log-normal distribution\nparameterized by the given mean\u03bc\\mu\u03bcand standard deviation\u03c3\\sigma\u03c3. Note thatmeanandstdare the mean and\nstandard deviation of the underlying normal distribution, and not of the\nreturned distribution:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_"
    },
    {
        "X": "What is not represented by the mean and standard deviation of the underlying normal distribution?",
        "Y": "returned distribution",
        "Z": "Fillsselftensor with numbers samples from the log-normal distribution\nparameterized by the given mean\u03bc\\mu\u03bcand standard deviation\u03c3\\sigma\u03c3. Note thatmeanandstdare the mean and\nstandard deviation of the underlying normal distribution, and not of the\nreturned distribution:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_"
    },
    {
        "X": "What is similar tobroadcast_tensors()but for?",
        "Y": "shapes",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "What is the equivalent tobroadcast_tensors()but for shapes?",
        "Y": "totorch.broadcast_tensors",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "What is this useful for?",
        "Y": "broadcasting tensors of common batch shape",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "What is used for broadcasting tensors of common batch shape but different rightmost shape?",
        "Y": "mean vectors with covariance matrices",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "What is an example of a shape compatible with all input shapes?",
        "Y": "*shapes(torch.Size) \u2013 Shapes of tensors",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "What is a shape compatible with all input shapes?",
        "Y": "RuntimeError",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "What is similar to tobroadcast_tensors() but for?",
        "Y": "shapes",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "What does shapebut avoid?",
        "Y": "create to intermediate tensors",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "What shape is used to broadcast tensors of common batch shape but different?",
        "Y": "rightmost shape",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "What is useful for broadcasting tensors of common batch shape but different rightmost shape?",
        "Y": "mean vectors with covariance matrices",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "A shape compatible with what?",
        "Y": "all input shapes",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "What happens if shapes are incompatible?",
        "Y": "RuntimeError",
        "Z": "Similar tobroadcast_tensors()but for shapes. This is equivalent totorch.broadcast_tensors(*map(torch.empty,shapes))[0].shapebut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. Example: *shapes(torch.Size) \u2013 Shapes of tensors. A shape compatible with all input shapes. shape (torch.Size) RuntimeError\u2013 If shapes are incompatible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "What is the behavior similar to python\u2019sitertools.product?",
        "Y": "Do cartesian product",
        "Z": "Do cartesian product of the given sequence of tensors. The behavior is similar to\npython\u2019sitertools.product. *tensors\u2013 any number of 1 dimensional tensors. A tensor equivalent to converting all the input tensors into lists,\ndoitertools.producton these lists, and finally convert the resulting list\ninto tensor. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod"
    },
    {
        "X": "What does the torch package have?",
        "Y": "CUDA counterpart",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the PyTorch tensor have?",
        "Y": "CUDA counterpart",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is True ifobjis?",
        "Y": "PyTorch storage object",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the data type ofinputis a complex data type?",
        "Y": "one oftorch.complex64, andtorch.complex128",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the data type ofinputis?",
        "Y": "floating point data type",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True ifobjis a PyTorch tensor.   Returns True ifobjis a PyTorch storage object.   Returns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.   Returns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the defaulttorch.Tensortype set to floating point tensor typet?",
        "Y": "current default floating pointtorch.dtype",
        "Z": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the default option for?",
        "Y": "printing",
        "Z": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the default floating pointtorch.dtype mean?",
        "Y": "Note",
        "Z": "Get the current default floating pointtorch.dtype.   Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a floating point tensor typet?",
        "Y": "defaulttorch.Tensortype",
        "Z": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the defaulttorch.Tensortype to floating point tensor typet?",
        "Y": "Note",
        "Z": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the defaulttorch.Tensortype do?",
        "Y": "Disables denormal floating numbers on CPU",
        "Z": "Sets the defaulttorch.Tensortype to floating point tensor typet.   Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the total number of elements in?",
        "Y": "theinputtensor",
        "Z": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a Disables denormal floating numbers on CPU?",
        "Y": "Set options for printing",
        "Z": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a denormal floating number on a CPU do?",
        "Y": "Disables denormal floating numbers on CPU",
        "Z": "Returns the total number of elements in theinputtensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What are listed underRandom sampling?",
        "Y": "Random sampling creation ops",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a tensor have?",
        "Y": "Splits a tensor into a specific number of chunks",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What are Disables denormal floating numbers on CPU?",
        "Y": "Set options for printing",
        "Z": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the CPU do?",
        "Y": "Disables denormal floating numbers",
        "Z": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What type of withdata does torch.empty() construct?",
        "Y": "tensor",
        "Z": "Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What happens on CPU?",
        "Y": "Disables denormal floating numbers",
        "Z": "Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What are listed underRandom samplingand include:torch.rand()torch.rand_like()torch.",
        "Y": "Random sampling creation ops",
        "Z": "Note Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What format does asparse tensor in?",
        "Y": "COO(rdinate) format",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a view of an existingtorch.Tensorinput have?",
        "Y": "specifiedsize,strideandstorage_offset",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the name of aTensor created?",
        "Y": "anumpy.ndarray",
        "Z": "Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a tensor fill with the scalar value0 have?",
        "Y": "the same size asinput",
        "Z": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor filled with the scalar value1?",
        "Y": "the shape defined by the variable argumentsize",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does an existingtorch.Tensorinput have?",
        "Y": "specifiedsize,strideandstorage_offset",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the same size asinput?",
        "Y": "a tensor filled with the scalar value0",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a tensor fill with the scalar value1 have?",
        "Y": "the same size asinput",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor withdata.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the shape of a tensor filled with the scalar value0?",
        "Y": "the shape defined by the variable argumentsize",
        "Z": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does the tensor fill with the scalar value0 have?",
        "Y": "the same size asinput",
        "Z": "Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the shape of a tensor filled with the scalar value1?",
        "Y": "the shape defined by the variable argumentsize",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What defines the shape of a tensor filled with the scalar value?",
        "Y": "variable argumentsize",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a tensor fill with the scalar value have?",
        "Y": "the same size asinput",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the tensor of sizeendstartstepleftlceil?",
        "Y": "1-D",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor of sizeendstartstepleftlceil fractextend?",
        "Y": "1-D",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor of sizeendstartstep+1leftlfloor fractextend?",
        "Y": "1-D",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "How many -D tensor of sizeendstartstep+1leftlceil fractextend",
        "Y": "1",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the tensor of sizesteps created?",
        "Y": "one-dimensional",
        "Z": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the tensor of sizeendstartstep+1leftlfloor?",
        "Y": "1-D",
        "Z": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor of sizesteps that values are evenly spaced fromstarttoend, inclusive?",
        "Y": "one-dimensional",
        "Z": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is the logarithmic scale of a one-dimensional tensor of sizesteps?",
        "Y": "basebase",
        "Z": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor on the diagonal and zeros elsewhere?",
        "Y": "2-D",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What type of tensor is created on a logarithmic scale with basebase?",
        "Y": "one-dimensional",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What is a tensor filled with uninitialized data?",
        "Y": "uninitialized",
        "Z": "Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.   Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.   Computes the Heaviside step function for each element ininput.",
        "source": "https://pytorch.org/docs/stable/torch.html#generators"
    },
    {
        "X": "What does a complex tensor return?",
        "Y": "a view ofinputas",
        "Z": "Returns a view ofinputas a complex tensor. For an input complex\ntensor ofsizem1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2m1,m2,\u2026,mi,2, this function returns a\nnew complex tensor ofsizem1,m2,\u2026,mim1, m2, \\dots, mim1,m2,\u2026,miwhere the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex"
    },
    {
        "X": "The last dimension of the input tensor is expected to represent what?",
        "Y": "real and imaginary components of complex numbers",
        "Z": "Returns a view ofinputas a complex tensor. For an input complex\ntensor ofsizem1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2m1,m2,\u2026,mi,2, this function returns a\nnew complex tensor ofsizem1,m2,\u2026,mim1, m2, \\dots, mim1,m2,\u2026,miwhere the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex"
    },
    {
        "X": "What does the function return for an input complex tensor ofsizem1,m2,...,mim1, m2, do",
        "Y": "torch.view_as_complex",
        "Z": "Returns a view ofinputas a complex tensor. For an input complex\ntensor ofsizem1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2m1,m2,\u2026,mi,2, this function returns a\nnew complex tensor ofsizem1,m2,\u2026,mim1, m2, \\dots, mim1,m2,\u2026,miwhere the last\ndimension of the input tensor is expected to represent the real and imaginary\ncomponents of complex numbers. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex"
    },
    {
        "X": "What does compute the element-wise logical XOR of the given input tensors do?",
        "Y": "Computes the element-wise logical XOR",
        "Z": "Computes the element-wise logical XOR of the given input tensors. Zeros are treated asFalseand nonzeros are\ntreated asTrue. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the tensor to compute XOR with out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor"
    },
    {
        "X": "What is the name of the function that generates a question?",
        "Y": "Alias fortorch.abs()",
        "Z": "Alias fortorch.abs()",
        "source": "https://pytorch.org/docs/stable/generated/torch.absolute.html#torch.absolute"
    },
    {
        "X": "What is an object that represents the data type of atorch.Tensor?",
        "Y": "Atorch.dtype",
        "Z": "Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How many different data types does PyTorch have?",
        "Y": "twelve",
        "Z": "Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How many -bit floating point1 torch does PyTorch have?",
        "Y": "16",
        "Z": "Eachtorch.Tensorhas atorch.dtype,torch.device, andtorch.layout. Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the 16-bit floating point1 torch?",
        "Y": "double",
        "Z": "Eachtorch.Tensorhas atorch.dtype,torch.device, andtorch.layout. Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What type of torch does atorch.Tensor have?",
        "Y": "double torch",
        "Z": "Atorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has twelve different data types: Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the 16-bit floating point2 torch?",
        "Y": "HalfTensor",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the torch that float64ortorch.double torch?",
        "Y": "64-bit floating point torch",
        "Z": "64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the unsigned torch.uint8 torch?",
        "Y": "8-bit integer",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the number of integers in the torch.int8 torch?",
        "Y": "8-bit",
        "Z": "torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the name of the torch.int16ortorch.short torch?",
        "Y": "16-bit integer",
        "Z": "torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the number of unsigned torch.uint8 torch in the BFloat16Tensor?",
        "Y": "8-bit",
        "Z": "64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the integer (signed) torch.int8 torch?",
        "Y": "8-bit",
        "Z": "64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the complex torch.complex64ortorch.cfloat 128-bit complex torch?",
        "Y": "64-bit",
        "Z": "64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the torch that is int16ortorch.short torch?",
        "Y": "16-bit integer",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the number of integers in the torch.int32ortorch.int torch?",
        "Y": "32-bit",
        "Z": "torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What type of integer is intTensor?",
        "Y": "64-bit",
        "Z": "64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the number of integers in torch.int32ortorch.int torch?",
        "Y": "32-bit",
        "Z": "torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "IntTensor is what type of integer (signed) torch.int64ortorch.long?",
        "Y": "64-bit",
        "Z": "torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How many -bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch?",
        "Y": "128",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the number of unsigned torch in the BFloat16Tensor?",
        "Y": "8-bit",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the torch that is intTensor?",
        "Y": "64-bit",
        "Z": "128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is a 8-bit integer?",
        "Y": "unsigned",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the 8-bit integer (signed) torch.int8 torch?",
        "Y": "ByteTensor",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is important when a binary16 uses 1 sign, 5 exponent, and 10 significand bits?",
        "Y": "precision",
        "Z": "Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the sign of a ByteTensor?",
        "Y": "8-bit integer",
        "Z": "torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is important when it has the same number of exponent bits asfloat32?",
        "Y": "range",
        "Z": "torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the sign for torch.int32ortorch.int torch?",
        "Y": "32-bit integer",
        "Z": "32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the 64-bit integer (signed) torch.int64ortorch.long torch?",
        "Y": "Boolean torch",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the BoolTensor sometimes referred to as?",
        "Y": "binary16",
        "Z": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of a torch.int32ortorch.long torch?",
        "Y": "64-bit integer",
        "Z": "torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is another name for a 64-bit integer?",
        "Y": "long torch",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is important when a brain floating point is used?",
        "Y": "precision",
        "Z": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is used to use 1 sign, 8 exponent and 7 significand bits?",
        "Y": "Brain Floating Point",
        "Z": "torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What can be used to find out if atorch.dtypeis a floating point data type?",
        "Y": "propertyis_floating_pointcan be used",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is BooleanTensor sometimes referred to as?",
        "Y": "binary16",
        "Z": "Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "To find out if atorch.dtypeis a complex data type, what property can be used?",
        "Y": "propertyis_complex",
        "Z": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is an object representing the device on which atorch.Tensoris or will be allocated?",
        "Y": "Atorch.device",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is a device type called?",
        "Y": "cpu'or'cuda",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device().",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is called if the device ordinal is not present?",
        "Y": "aftertorch.cuda.set_device()",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device().",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "If the device ordinal is not present, this object will always represent what for the device type?",
        "Y": "current device",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What does the torch.device contains?",
        "Y": "device type",
        "Z": "Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How can Atorch.Tensor's device be accessed?",
        "Y": "theTensor.deviceproperty",
        "Z": "Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What does Thetorch.device contain?",
        "Y": "device type",
        "Z": "Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What can be done by using a string or a device ordinal?",
        "Y": "fast prototyping of code",
        "Z": "Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What returns an ordinal for cuda tensors?",
        "Y": "matchesTensor.get_device()",
        "Z": "Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the device that is not supported for cpu tensors?",
        "Y": "Note",
        "Z": "Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What does the device ordinal allow for?",
        "Y": "fast prototyping of code",
        "Z": "Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is an object that represents the memory layout of atorch.Tensor?",
        "Y": "A torch.layoutis",
        "Z": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What do strided tensors provide?",
        "Y": "multi-dimensional,stridedview of a storage",
        "Z": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What represents the jump in the memory necessary to go from one element to the next in the k-th dimension of the Tensor?",
        "Y": "k-th stride",
        "Z": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What does the k-th stride make it possible to do?",
        "Y": "perform many tensor operations efficiently",
        "Z": "Warning Thetorch.layoutclass is in beta and subject to change. Atorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors). torch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. Example: For more information ontorch.sparse_cootensors, seetorch.sparse.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is an object representing the memory format on which atorch.Tensoris or will be allocated?",
        "Y": "A torch.memory_format is",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What are the possible values atorch.memory_format?",
        "Y": "torch.contiguous_format",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What type of order are values represented by?",
        "Y": "decreasing order",
        "Z": "Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the tensor that is or will be allocated in dense non-overlapping memory?",
        "Y": "torch.channels_last",
        "Z": "torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "Where is Tensor allocated?",
        "Y": "dense non-overlapping memory",
        "Z": "torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What order are atorch.Tensoris represented by?",
        "Y": "decreasing",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What are the possible values of torch.contiguous_format?",
        "Y": "dense non-overlapping memory",
        "Z": "Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the order in which tensor is or will be allocated in dense non-overlapping memory?",
        "Y": "NHWC",
        "Z": "Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How do we promote when the dtypes of inputs to an arithmetic operation differ?",
        "Y": "by finding the minimum dtype that satisfies the following rules",
        "Z": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What property returnsTrue if the data type is a floating point data type?",
        "Y": "propertyis_floating_pointcan be used",
        "Z": "torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What property returnsTrue if the data type is a complex data type?",
        "Y": "property is_complex can be used",
        "Z": "torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is important when Brain Floating Point has the same number of exponent bits asfloat32?",
        "Y": "range",
        "Z": "torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "When the dtypes of inputs to an operation (add,sub,div,mul) differ, we promote by finding the minimum d",
        "Y": "arithmetic",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the term used to describe the use of 1 sign, 5 exponent, and 10 significand bits?",
        "Y": "binary16",
        "Z": "torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What does get_device() do?",
        "Y": "returns an ordinal for cuda tensors",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "Where is atorch.Tensor allocated?",
        "Y": "dense non-overlapping memory",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What order are atorch.tensoris strides represented by?",
        "Y": "decreasing",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is used in functions likecloneto preserve the memory format of the input tensor?",
        "Y": "torch.preserve_format",
        "Z": "Atorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated. Possible values are: torch.contiguous_format:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values in decreasing order. torch.channels_last:\nTensor is or will be  allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order. torch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "When is 1 sign, 5 exponent, and 10 significand bits used?",
        "Y": "when precision is important",
        "Z": "Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What are some inputs to an arithmetic operation?",
        "Y": "add,sub,div,mul",
        "Z": "Data type dtype Legacy Constructors 32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How many bits does a CharTensor have?",
        "Y": "16-bit",
        "Z": "torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the size of a ShortTensor?",
        "Y": "32-bit",
        "Z": "32-bit floating point torch.float32ortorch.float torch.*.FloatTensor 64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor 64-bit complex torch.complex64ortorch.cfloat 128-bit complex torch.complex128ortorch.cdouble 16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor 16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "IntTensor is what type of integer?",
        "Y": "64-bit",
        "Z": "torch.bfloat16 torch.*.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "64-bit integer torch.int64ortorch is what?",
        "Y": "long torch",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "If there are no higher-category zero-dim operands, we promote to a type with sufficient what to hold all dimensione",
        "Y": "size and category",
        "Z": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is the name of the program that does not inspect values when determining the minimumdtypesof an operand?",
        "Y": "numpy",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What types are not yet supported?",
        "Y": "Quantized and complex types",
        "Z": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "We do not inspect values when determining the minimumdtypesof an operand. Quantized and complex types are not yet supported. Unlike what",
        "Y": "numpy",
        "Z": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What operand has a higher category than dimensioned operands?",
        "Y": "zero-dimension tensor",
        "Z": "torch.*.LongTensor Boolean torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "Promotion Examples:",
        "Y": "An integral output tensor cannot accept a floating point tensor",
        "Z": "torch.bool torch.*.BoolTensor Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important. Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 To find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type. To find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type. When the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules: If the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category. If a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category. If there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands. A floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Quantized and complex types\nare not yet supported. Promotion Examples: An integral output tensor cannot accept a floating point tensor. A boolean output tensor cannot accept a non-boolean tensor. A non-complex output tensor cannot accept a complex tensor Casting Examples: Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated.",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What is optional for the device type?",
        "Y": "device ordinal",
        "Z": "Atorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated. Thetorch.devicecontains a device type ('cpu'or'cuda') and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device(). Atorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty. Atorch.devicecan be constructed via a string or via a string and device ordinal Via a string: Via a string and device ordinal: Note Thetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. Note For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matchesTensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. Note Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent:",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "What values are specified to replaceNaN, positive infinity, and negative infinity values ininput?",
        "Y": "bynan,posinf, andneginf",
        "Z": "ReplacesNaN, positive infinity, and negative infinity values ininputwith the values specified bynan,posinf, andneginf, respectively.\nBy default,NaN`sarereplacedwithzero,positiveinfinityisreplacedwiththegreatestfinitevaluerepresentableby:attr:`input\u2019s dtype, and negative infinity\nis replaced with the least finite value representable byinput\u2019s dtype. input(Tensor) \u2013 the input tensor. nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "By default,NaN is replaced with what value?",
        "Y": "zero",
        "Z": "ReplacesNaN, positive infinity, and negative infinity values ininputwith the values specified bynan,posinf, andneginf, respectively.\nBy default,NaN`sarereplacedwithzero,positiveinfinityisreplacedwiththegreatestfinitevaluerepresentableby:attr:`input\u2019s dtype, and negative infinity\nis replaced with the least finite value representable byinput\u2019s dtype. input(Tensor) \u2013 the input tensor. nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "What is the value to replaceNaNs with?",
        "Y": "nan",
        "Z": "nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero. posinf(Number,optional) \u2013 if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable byinput\u2019s dtype.\nDefault is None. neginf(Number,optional) \u2013 if a Number, the value to replace negative infinity values with.\nIf None, negative infinity values are replaced with the lowest finite value representable byinput\u2019s dtype.\nDefault is None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "What is the default value to replaceNaNs with?",
        "Y": "zero",
        "Z": "nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero. posinf(Number,optional) \u2013 if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable byinput\u2019s dtype.\nDefault is None. neginf(Number,optional) \u2013 if a Number, the value to replace negative infinity values with.\nIf None, negative infinity values are replaced with the lowest finite value representable byinput\u2019s dtype.\nDefault is None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "What values are used to replace negative infinity values in input?",
        "Y": "bynan,posinf, andneginf",
        "Z": "ReplacesNaN, positive infinity, and negative infinity values ininputwith the values specified bynan,posinf, andneginf, respectively.\nBy default,NaN`sarereplacedwithzero,positiveinfinityisreplacedwiththegreatestfinitevaluerepresentableby:attr:`input\u2019s dtype, and negative infinity\nis replaced with the least finite value representable byinput\u2019s dtype. input(Tensor) \u2013 the input tensor. nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "What is the default value for negative infinity?",
        "Y": "the least finite value",
        "Z": "ReplacesNaN, positive infinity, and negative infinity values ininputwith the values specified bynan,posinf, andneginf, respectively.\nBy default,NaN`sarereplacedwithzero,positiveinfinityisreplacedwiththegreatestfinitevaluerepresentableby:attr:`input\u2019s dtype, and negative infinity\nis replaced with the least finite value representable byinput\u2019s dtype. input(Tensor) \u2013 the input tensor. nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "What is the value to replace positive infinity values with?",
        "Y": "posinf",
        "Z": "nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero. posinf(Number,optional) \u2013 if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable byinput\u2019s dtype.\nDefault is None. neginf(Number,optional) \u2013 if a Number, the value to replace negative infinity values with.\nIf None, negative infinity values are replaced with the lowest finite value representable byinput\u2019s dtype.\nDefault is None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "What is the greatest finite value represented by?",
        "Y": "byinput\u2019s dtype",
        "Z": "input(Tensor) \u2013 the input tensor. nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero. posinf(Number,optional) \u2013 if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable byinput\u2019s dtype.\nDefault is None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "What is the default value to replace positive infinity values with?",
        "Y": "None",
        "Z": "nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero. posinf(Number,optional) \u2013 if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable byinput\u2019s dtype.\nDefault is None. neginf(Number,optional) \u2013 if a Number, the value to replace negative infinity values with.\nIf None, negative infinity values are replaced with the lowest finite value representable byinput\u2019s dtype.\nDefault is None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "When are positive infinity values replaced with the greatest finite value representable by input's dtype?",
        "Y": "If None",
        "Z": "input(Tensor) \u2013 the input tensor. nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero. posinf(Number,optional) \u2013 if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable byinput\u2019s dtype.\nDefault is None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "When are positive infinity values replaced with the greatest finite value representable byinput's dtype?",
        "Y": "If None",
        "Z": "nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero. posinf(Number,optional) \u2013 if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable byinput\u2019s dtype.\nDefault is None. neginf(Number,optional) \u2013 if a Number, the value to replace negative infinity values with.\nIf None, negative infinity values are replaced with the lowest finite value representable byinput\u2019s dtype.\nDefault is None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "What is the value to replace negative infinity values with?",
        "Y": "neginf",
        "Z": "nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero. posinf(Number,optional) \u2013 if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable byinput\u2019s dtype.\nDefault is None. neginf(Number,optional) \u2013 if a Number, the value to replace negative infinity values with.\nIf None, negative infinity values are replaced with the lowest finite value representable byinput\u2019s dtype.\nDefault is None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "Neginf(Number,optional) \u2013 if a Number, the value to replace negative infinity values with?",
        "Y": "None",
        "Z": "nan(Number,optional) \u2013 the value to replaceNaNs with. Default is zero. posinf(Number,optional) \u2013 if a Number, the value to replace positive infinity values with.\nIf None, positive infinity values are replaced with the greatest finite value representable byinput\u2019s dtype.\nDefault is None. neginf(Number,optional) \u2013 if a Number, the value to replace negative infinity values with.\nIf None, negative infinity values are replaced with the lowest finite value representable byinput\u2019s dtype.\nDefault is None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "What does each element of the inputinput by the corresponding element ofother do?",
        "Y": "Divides",
        "Z": "Divides each element of the inputinputby the corresponding element ofother. Note By default, this performs a \u201ctrue\u201d division like Python 3.\nSee therounding_modeargument for floor division. Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result:",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "By default, this performs a \u201cwhat\u201d division like Python 3?",
        "Y": "true",
        "Z": "By default, this performs a \u201ctrue\u201d division like Python 3.\nSee therounding_modeargument for floor division. Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result:",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is another name for floor division?",
        "Y": "therounding_modeargument",
        "Z": "By default, this performs a \u201ctrue\u201d division like Python 3.\nSee therounding_modeargument for floor division. Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result:",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "Always promotes integer types to the default what?",
        "Y": "scalar type",
        "Z": "By default, this performs a \u201ctrue\u201d division like Python 3.\nSee therounding_modeargument for floor division. Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result:",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is the divisor of input(Tensor)?",
        "Y": "rounding",
        "Z": "Divides each element of the inputinputby the corresponding element ofother. Note By default, this performs a \u201ctrue\u201d division like Python 3.\nSee therounding_modeargument for floor division. Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result:",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is the name of the floor division function?",
        "Y": "therounding_modeargument",
        "Z": "Note By default, this performs a \u201ctrue\u201d division like Python 3.\nSee therounding_modeargument for floor division. Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result:",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is the divisor of the input(Tensor)?",
        "Y": "rounding",
        "Z": "Note By default, this performs a \u201ctrue\u201d division like Python 3.\nSee therounding_modeargument for floor division. Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result:",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What does input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor?",
        "Y": "rounding",
        "Z": "By default, this performs a \u201ctrue\u201d division like Python 3.\nSee therounding_modeargument for floor division. Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result:",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "Always promotes integer types to what type?",
        "Y": "default scalar type",
        "Z": "Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is the default behavior?",
        "Y": "None",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "Performs what type of rounding if both input andotherare integer types?",
        "Y": "no rounding",
        "Z": "Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is equivalent to NumPy'snp.true_divide?",
        "Y": "true division",
        "Z": "Supportsbroadcasting to a common shape,type promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What type of rounding is performed if both input and otherare integer types?",
        "Y": "no",
        "Z": "input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is the equivalent to in Python and NumPy'snp.true_divide?",
        "Y": "true division",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "\"trunc\" rounds the results of the division towards what?",
        "Y": "zero",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What type of integer division is trunc equivalent to?",
        "Y": "C-style",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is the case when the inputs are promoted to the default scalar type?",
        "Y": "if bothinputandotherare integer types",
        "Z": "input(Tensor) \u2013 the dividend other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is equivalent to the/operator in Python and NumPy'snp.true_divide?",
        "Y": "true division",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What rounds the results of the division towards zero?",
        "Y": "trunc",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is equivalent to true division in Python?",
        "Y": "C-style integer division",
        "Z": "rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What happens if both input and otherare integer types?",
        "Y": "Performs no rounding",
        "Z": "other(TensororNumber) \u2013 the divisor rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is the equivalent to in Python?",
        "Y": "true division",
        "Z": "rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is the default behavior of rounding?",
        "Y": "None",
        "Z": "rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "Does rounding_mode(str,optional) perform any rounding?",
        "Y": "no",
        "Z": "rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "Rounding_mode(str,optional) - default behavior. Performs no rounding and, if bothinputandother",
        "Y": "None",
        "Z": "rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "When does rounding_mode(str,optional) perform no rounding?",
        "Y": "if bothinputandotherare integer types",
        "Z": "rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is equivalent to the/operator in Python?",
        "Y": "true division",
        "Z": "rounding_mode(str,optional) \u2013 Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What type of rounding rounds the results of the division down?",
        "Y": "floor",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What does the//operator and NumPy'snp.floor_divide equivalent to?",
        "Y": "floor division",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What type of rounding does None perform?",
        "Y": "no rounding",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What type of division is equivalent to trunc?",
        "Y": "C-style integer division",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is the name of the rounding that rounds the results of a division down?",
        "Y": "floor",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is equivalent to floor division in Python?",
        "Y": "NumPy\u2019snp.floor_divide",
        "Z": "Type of rounding applied to the result: None - default behavior. Performs no rounding and, if bothinputandotherare integer types, promotes the inputs to the default scalar type.\nEquivalent to true division in Python (the/operator) and NumPy\u2019snp.true_divide. \"trunc\"- rounds the results of the division towards zero.\nEquivalent to C-style integer division. \"floor\"- rounds the results of the division down.\nEquivalent to floor division in Python (the//operator) and NumPy\u2019snp.floor_divide.",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "What is the name of Alias fortorch.trunc?",
        "Y": "Alias fortorch.trunc()",
        "Z": "Alias fortorch.trunc()",
        "source": "https://pytorch.org/docs/stable/generated/torch.fix.html#torch.fix"
    },
    {
        "X": "What is another name for fortorch.trunc()?",
        "Y": "Alias fortorch.trunc()",
        "Z": "Alias fortorch.trunc()",
        "source": "https://pytorch.org/docs/stable/generated/torch.fix.html#torch.fix"
    },
    {
        "X": "What does the Appendix Migrate to?",
        "Y": "PyTorch 1.2 Recursive Scripting API References",
        "Z": "Creating TorchScript Code Mixing Tracing and Scripting TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer?",
        "Y": "Python Functions and Modules",
        "Z": "TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the difference between Python and PyTorch?",
        "Y": "Python Language Reference Comparison",
        "Z": "Mixing Tracing and Scripting TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Appendix Migrating to what?",
        "Y": "PyTorch 1.2 Recursive Scripting API References",
        "Z": "TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is Debugging Disable JIT for Debugging?",
        "Y": "Python Language Reference Comparison",
        "Z": "Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What did Appendix Migrate to?",
        "Y": "PyTorch 1.2 Recursive Scripting API References",
        "Z": "Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What language can a TorchScript program be saved from?",
        "Y": "Python",
        "Z": "Creating TorchScript Code Mixing Tracing and Scripting TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What Python Language Reference Comparison Debugging Disable JIT for Debugging?",
        "Y": "Python Functions and Modules",
        "Z": "Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is TorchScript?",
        "Y": "a way to create serializable and optimizable models from PyTorch code",
        "Z": "Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What program can a TorchScript program be saved from?",
        "Y": "Python",
        "Z": "TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does TorchScript do for Debugging?",
        "Y": "Disable JIT",
        "Z": "Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the Appendix Migrating to PyTorch 1.2 Recursive Scripting API References?",
        "Y": "Frequently Asked Questions Known Issues",
        "Z": "Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of process can a TorchScript program be saved from?",
        "Y": "Python",
        "Z": "Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API Reference",
        "Y": "Frequently Asked Questions",
        "Z": "Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Any TorchScript program can be saved from what process?",
        "Y": "Python",
        "Z": "TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the Appendix Migrating to PyTorch 1.2 Recursive Scripting API References?",
        "Y": "Known Issues",
        "Z": "Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a way to create serializable and optimizable models from PyTorch code?",
        "Y": "Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript",
        "Z": "Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is TorchScript a way to create serializable and optimizable models from?",
        "Y": "PyTorch code",
        "Z": "Mixing Tracing and Scripting TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is References TorchScript?",
        "Y": "a way to create serializable and optimizable models",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a standalone TorchScript program that can be run independently from Python?",
        "Y": "C++ program",
        "Z": "TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Why are Python programs disadvantageous?",
        "Y": "performance and multi-threading reasons",
        "Z": "TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What program can be run independently from Python?",
        "Y": "TorchScript",
        "Z": "We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In what language is it possible to train models in PyTorch?",
        "Y": "Python",
        "Z": "Creating TorchScript Code Mixing Tracing and Scripting TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a gentle introduction to TorchScript?",
        "Y": "theIntroduction to TorchScripttutorial",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What language is used to run a PyTorch model?",
        "Y": "C++",
        "Z": "For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will inspect the source code, compile it as TorchScript code using the TorchScript compiler?",
        "Y": "Scripting a function ornn.Module",
        "Z": "For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the introduction to TorchScript?",
        "Y": "introduction to TorchScript",
        "Z": "TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an end-to-end example of converting a PyTorch model to TorchScript and running it in C++?",
        "Y": "Loading a PyTorch Model in C++tutorial",
        "Z": "For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleor",
        "Y": "ornn.Module",
        "Z": "Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How is a scriptFunction optimized?",
        "Y": "just-in-time compilation",
        "Z": "For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When does Compilesfn occur?",
        "Y": "Compilesfnwhen it is first called during tracing",
        "Z": "For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of converting a PyTorch model to TorchScript and running it in C++?",
        "Y": "end-to-end example",
        "Z": "For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is called when it is first called during tracing?",
        "Y": "Compilesfn",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a module that will inspect the source code and compile it as TorchScript code using the TorchScript compiler?",
        "Y": "Scripting a function ornn",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will be used to optimize a function?",
        "Y": "just-in-time compilation",
        "Z": "Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When is Compilesfn called?",
        "Y": "Compilesfnwhen it is first called during tracing",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will be used to optimize a module?",
        "Y": "just-in-time compilation",
        "Z": "Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Scripting a function ornn.Module compile it as?",
        "Y": "TorchScript code",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will be used to optimize a script module?",
        "Y": "just-in-time compilation",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does executingfuncand a reference to the value of the result of this execution do?",
        "Y": "Creates an asynchronous task",
        "Z": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of task is created?",
        "Y": "asynchronous task executingfunc",
        "Z": "Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a wrapper around?",
        "Y": "C++torch::jit::Module",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When is Compilesfn first called?",
        "Y": "tracing",
        "Z": "Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Compilesfn do?",
        "Y": "Forces completion of atorch.jit.Future[T]asynchronous task",
        "Z": "Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does this create?",
        "Y": "asynchronous task executingfunc",
        "Z": "Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does it do to complete the atorch.jit.Future[T]asynchronous task?",
        "Y": "Forces completion of atorch.jit.Future[T]asynchronous task",
        "Z": "Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a wrapper around C++torch functionally equivalent to?",
        "Y": "aScriptModule",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does atorch.jit.Future[T]asynchronous task do?",
        "Y": "Forces completion of atorch.jit.Future[T]asynchronous task",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is C++torch::jit::Module functionally equivalent to?",
        "Y": "aScriptModule",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of task does atorch.jit.Future[T]ask create?",
        "Y": "asynchronous task",
        "Z": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Forces completion of what?",
        "Y": "atorch.jit.Future[T]asynchronous task",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is jit::Module functionally equivalent to?",
        "Y": "aScriptModule",
        "Z": "Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the function that returns the result of atorch.jit.Future[T]asynchronous task?",
        "Y": "Forces completion of atorch.jit.Future[T]asynchronous task",
        "Z": "Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is atorch.jit::Module functionally equivalent to?",
        "Y": "aScriptModule",
        "Z": "Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Freezing aScriptModule do?",
        "Y": "Save an offline version of this module for use in a separate process",
        "Z": "Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does TorchScriptModule do?",
        "Y": "Forces completion of atorch.jit.Future[T]asynchronous task",
        "Z": "Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the Torch",
        "Y": "Freezing aScriptModule",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of version of atorch.jit.Future[T]asynchronous task can be saved for use in a separate process?",
        "Y": "offline",
        "Z": "Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will clone it and attempt to inline the cloned module's submodules, parameters, and attributes as",
        "Y": "Freezing aScriptModule",
        "Z": "Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can you do with the cloned module?",
        "Y": "Save an offline version of this module for use in a separate process",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What did aScriptModuleorScriptFunction previously save?",
        "Y": "withtorch.jit.save",
        "Z": "Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "A wrapper around C++torch::jit::Module is functionally equivalent to what?",
        "Y": "aScriptModule",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is C++torch::jit::Module?",
        "Y": "wrapper",
        "Z": "For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of version of aScriptModule can you save for use in a separate process?",
        "Y": "offline",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where was aScriptModuleorScriptFunction previously saved?",
        "Y": "withtorch.jit.save",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is functionally equivalent to?",
        "Y": "aScriptModule",
        "Z": "Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as",
        "Y": "Freezing aScriptModule",
        "Z": "Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is one way to save a cloned aScriptModule?",
        "Y": "Save an offline version of this module for use in a separate process",
        "Z": "Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the difference between aScriptModule and aScriptModule?",
        "Y": "represents a single function and does not have any attributes or Parameters",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is one way to save a cloned module?",
        "Y": "Save an offline version of this module for use in a separate process",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will attempt to inline the cloned module's submodules, parameters, and attributes as constants in the Torch",
        "Y": "Freezing aScriptModule",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is one way to save a module?",
        "Y": "Save an offline version of this module for use in a separate process",
        "Z": "Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where does this function provide for conatiner type refinement?",
        "Y": "TorchScript",
        "Z": "This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where can you save a version of this module for use in a separate process?",
        "Y": "offline",
        "Z": "Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does this function provide in TorchScript?",
        "Y": "conatiner type refinement",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What was previously saved withtorch.jit.save?",
        "Y": "Load aScriptModuleorScriptFunction",
        "Z": "Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What language should a function in TorchScript be left as?",
        "Y": "Python",
        "Z": "This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does this decorator indicate to the compiler that a function or method should be ignored and left as?",
        "Y": "Python function",
        "Z": "This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does this function provide for in TorchScript?",
        "Y": "conatiner type refinement",
        "Z": "Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the left-hand side expression used to indicate to the TorchScript compiler?",
        "Y": "a class instance attribute with type oftype",
        "Z": "Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the TorchScript conatiner type refinement?",
        "Y": "a pass-through function that returnsvalue",
        "Z": "This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does returnthe_value hint TorchScript compiler about?",
        "Y": "the type ofthe_value",
        "Z": "This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the returnthe_value hint TorchScript compiler?",
        "Y": "the type ofthe_value",
        "Z": "This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing or scripting can be composed to suit the specific requirements of a part of a model?",
        "Y": "TorchScript",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be composed to suit the particular requirements of a part of a model?",
        "Y": "Tracing and scripting",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can scripted functions call?",
        "Y": "traced functions",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a traced function particularly useful when you need to use around a simple feed-forward model?",
        "Y": "control-flow",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can a beam search of a sequence to sequence model call?",
        "Y": "encoder module",
        "Z": "Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an easier approach for converting a model to TorchScript?",
        "Y": "tracing or scripting",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can tracing and scripting be composed to?",
        "Y": "suit the particular requirements of a part of a model",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When are scripted functions useful?",
        "Y": "when you need to use control-flow around a simple feed-forward model",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can the beam search of a sequence to sequence model call?",
        "Y": "an encoder module generated using tracing",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a traced function useful when you need to use around a feed-forward model?",
        "Y": "control-flow",
        "Z": "Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In what language can a traced function be called?",
        "Y": "script",
        "Z": "Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can a scripted function call?",
        "Y": "an encoder module generated using tracing",
        "Z": "Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can traced functions call?",
        "Y": "script functions",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of model is most of the model?",
        "Y": "feed-forward network",
        "Z": "Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is preserved correctly inside of a script function called by a traced function?",
        "Y": "Control-flow",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a function that can call a traced function?",
        "Y": "script function in a traced function",
        "Z": "Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Traced functions can call what?",
        "Y": "script functions",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a script function called in?",
        "Y": "a traced function",
        "Z": "Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of network is most of the model?",
        "Y": "feed-forward network",
        "Z": "Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing can be called from the methods of what?",
        "Y": "script module",
        "Z": "Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When are script functions useful?",
        "Y": "when a small part of a model requires some control-flow",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be used to generate a submodule using fornn.Modules?",
        "Y": "tracing",
        "Z": "Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be called in a traced function?",
        "Y": "script function",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be used to call a submodule using tracing?",
        "Y": "script module",
        "Z": "Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be used to generate a submodule using tracing?",
        "Y": "traced module",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How can a submodule be generated using fornn.Modules?",
        "Y": "using a traced module",
        "Z": "Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "TorchScript is a statically typed subset of what programming language?",
        "Y": "Python",
        "Z": "TorchScript is a statically typed subset of Python, so many Python features apply\ndirectly to TorchScript. See the fullTorchScript Language Referencefor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the reference for TorchScript?",
        "Y": "fullTorchScript Language Reference",
        "Z": "TorchScript is a statically typed subset of Python, so many Python features apply\ndirectly to TorchScript. See the fullTorchScript Language Referencefor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What supports the use of most PyTorch functions and many Python built-ins?",
        "Y": "TorchScript",
        "Z": "TorchScript supports the use of most PyTorch functions and many Python built-ins.\nSeeTorchScript Builtinsfor a full reference of supported functions. TorchScript supports a subset of the tensor and neural network\nfunctions that PyTorch provides. Most methods on Tensor as well as functions in\nthetorchnamespace, all functions intorch.nn.functionaland\nmost modules fromtorch.nnare supported in TorchScript. SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does TorchScript Builtins provide?",
        "Y": "a full reference of supported functions",
        "Z": "TorchScript supports the use of most PyTorch functions and many Python built-ins.\nSeeTorchScript Builtinsfor a full reference of supported functions. TorchScript supports a subset of the tensor and neural network\nfunctions that PyTorch provides. Most methods on Tensor as well as functions in\nthetorchnamespace, all functions intorch.nn.functionaland\nmost modules fromtorch.nnare supported in TorchScript. SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "TorchScript supports a subset of what PyTorch provides?",
        "Y": "tensor and neural network functions",
        "Z": "TorchScript supports a subset of the tensor and neural network\nfunctions that PyTorch provides. Most methods on Tensor as well as functions in\nthetorchnamespace, all functions intorch.nn.functionaland\nmost modules fromtorch.nnare supported in TorchScript. SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where are most modules fromtorch.nn supported?",
        "Y": "TorchScript",
        "Z": "TorchScript supports a subset of the tensor and neural network\nfunctions that PyTorch provides. Most methods on Tensor as well as functions in\nthetorchnamespace, all functions intorch.nn.functionaland\nmost modules fromtorch.nnare supported in TorchScript. SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a list of unsupported PyTorch functions and modules?",
        "Y": "SeeTorchScript Unsupported Pytorch Constructs",
        "Z": "TorchScript supports the use of most PyTorch functions and many Python built-ins.\nSeeTorchScript Builtinsfor a full reference of supported functions. TorchScript supports a subset of the tensor and neural network\nfunctions that PyTorch provides. Most methods on Tensor as well as functions in\nthetorchnamespace, all functions intorch.nn.functionaland\nmost modules fromtorch.nnare supported in TorchScript. SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does TorchScript support?",
        "Y": "PyTorch functions and many Python built-ins",
        "Z": "TorchScript supports the use of most PyTorch functions and many Python built-ins.\nSeeTorchScript Builtinsfor a full reference of supported functions. TorchScript supports a subset of the tensor and neural network\nfunctions that PyTorch provides. Most methods on Tensor as well as functions in\nthetorchnamespace, all functions intorch.nn.functionaland\nmost modules fromtorch.nnare supported in TorchScript. SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where can you find a full reference of supported functions?",
        "Y": "SeeTorchScript Builtins",
        "Z": "TorchScript supports the use of most PyTorch functions and many Python built-ins.\nSeeTorchScript Builtinsfor a full reference of supported functions. TorchScript supports a subset of the tensor and neural network\nfunctions that PyTorch provides. Most methods on Tensor as well as functions in\nthetorchnamespace, all functions intorch.nn.functionaland\nmost modules fromtorch.nnare supported in TorchScript. SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "TorchScript supports a subset of what PyTorch functions?",
        "Y": "tensor and neural network functions",
        "Z": "TorchScript supports the use of most PyTorch functions and many Python built-ins.\nSeeTorchScript Builtinsfor a full reference of supported functions. TorchScript supports a subset of the tensor and neural network\nfunctions that PyTorch provides. Most methods on Tensor as well as functions in\nthetorchnamespace, all functions intorch.nn.functionaland\nmost modules fromtorch.nnare supported in TorchScript. SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is supported by TorchScript?",
        "Y": "most modules",
        "Z": "TorchScript supports the use of most PyTorch functions and many Python built-ins.\nSeeTorchScript Builtinsfor a full reference of supported functions. TorchScript supports a subset of the tensor and neural network\nfunctions that PyTorch provides. Most methods on Tensor as well as functions in\nthetorchnamespace, all functions intorch.nn.functionaland\nmost modules fromtorch.nnare supported in TorchScript. SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where can you find a list of unsupported PyTorch functions and modules?",
        "Y": "SeeTorchScript",
        "Z": "TorchScript supports a subset of the tensor and neural network\nfunctions that PyTorch provides. Most methods on Tensor as well as functions in\nthetorchnamespace, all functions intorch.nn.functionaland\nmost modules fromtorch.nnare supported in TorchScript. SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a list of unsupported PyTorch Constructs?",
        "Y": "SeeTorchScript",
        "Z": "SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules. Many of Python\u2019sbuilt-in functionsare supported in TorchScript.\nThemathmodule is also supported (seemath Modulefor details), but no other Python modules\n(built-in or third party) are supported. For a full listing of supported Python features, seePython Language Reference Coverage.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where are many of Python's built-in functions supported?",
        "Y": "TorchScript",
        "Z": "Many of Python\u2019sbuilt-in functionsare supported in TorchScript.\nThemathmodule is also supported (seemath Modulefor details), but no other Python modules\n(built-in or third party) are supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is also supported, but no other Python modules are supported?",
        "Y": "Themathmodule",
        "Z": "SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules. Many of Python\u2019sbuilt-in functionsare supported in TorchScript.\nThemathmodule is also supported (seemath Modulefor details), but no other Python modules\n(built-in or third party) are supported. For a full listing of supported Python features, seePython Language Reference Coverage.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "For a full listing of supported features, seePython Language Reference Coverage.",
        "Y": "Python",
        "Z": "SeeTorchScript Unsupported Pytorch Constructsfor a list of unsupported PyTorch functions and modules. Many of Python\u2019sbuilt-in functionsare supported in TorchScript.\nThemathmodule is also supported (seemath Modulefor details), but no other Python modules\n(built-in or third party) are supported. For a full listing of supported Python features, seePython Language Reference Coverage.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is also supported in TorchScript?",
        "Y": "Themathmodule",
        "Z": "Many of Python\u2019sbuilt-in functionsare supported in TorchScript.\nThemathmodule is also supported (seemath Modulefor details), but no other Python modules\n(built-in or third party) are supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Many of Python'sbuilt-in functions are supported in what?",
        "Y": "TorchScript",
        "Z": "Many of Python\u2019sbuilt-in functionsare supported in TorchScript.\nThemathmodule is also supported (seemath Modulefor details), but no other Python modules\n(built-in or third party) are supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What language is supported by the Python Language Reference Coverage?",
        "Y": "Python",
        "Z": "For a full listing of supported Python features, seePython Language Reference Coverage.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the full list of supported Python features?",
        "Y": "Python Language Reference Coverage",
        "Z": "For a full listing of supported Python features, seePython Language Reference Coverage.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Debugging this script use?",
        "Y": "withpdbworks",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How can we disable TorchScript?",
        "Y": "globally disable JIT",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "If the above script is calleddisable_what, we can invoke it like so?",
        "Y": "jit_example.py",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To disable what for a specific function, see@torch.jit.ignore.",
        "Y": "TorchScript compiler",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will we be able to step into the@torch.jit.scriptfunction as?",
        "Y": "normal Python function",
        "Z": "and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What compiler does TorchScript provide a code pretty-printer for allScriptModuleinstances?",
        "Y": "TorchScript",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does TorchScript provide a code pretty-printer for?",
        "Y": "allScriptModuleinstances",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the code pretty-printer do?",
        "Y": "gives an interpretation of the script method\u2019s code as valid Python syntax",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a code pretty-printer for allScriptModuleinstances?",
        "Y": "example",
        "Z": "and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "AScriptModulewith a singleforwardmethod will have what?",
        "Y": "attributecode",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What do you need to do if theScriptModulehas more than one method?",
        "Y": "access.codeon the method itself and not the module",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How can we inspect the code of a method namedfooon aScriptModule?",
        "Y": "accessing.foo.code",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the example above produce?",
        "Y": "output",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "If theScriptModulehas more than one method, you will need to do what?",
        "Y": "access.codeon the method itself and not the module",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the code for?",
        "Y": "theforwardmethod",
        "Z": "This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is another way to verify that TorchScript has captured your model code correctly?",
        "Y": "tracing or scripting",
        "Z": "This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is TorchScript's representation at a lower level than the code pretty- printer?",
        "Y": "IR graphs",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is TorchScript's compilation of the code for?",
        "Y": "theforwardmethod",
        "Z": "This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the purpose of theforwardmethod?",
        "Y": "to ensure TorchScript (tracing or scripting) has captured your model code correctly",
        "Z": "This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is another name for static single assignment?",
        "Y": "SSA",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the C++ backend of PyTorch?",
        "Y": "ATen",
        "Z": "This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of TorchScript's SSA intermediate representation?",
        "Y": "example",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does SSA stand for?",
        "Y": "static single assignment",
        "Z": "TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "The graph follows the same rules described in what section with regard toforwardmethod lookup?",
        "Y": "theInspecting Codesection",
        "Z": "TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the example script above produce?",
        "Y": "the graph",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What produces the graph?",
        "Y": "The example script above",
        "Z": "TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the example script that produces the graph?",
        "Y": "instruction%rv.1:Tensor=aten::zeros",
        "Z": "graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What do we assign the output to a (unique) value namedrv.1?",
        "Y": "%rv.1:Tensormeans",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "The graph follows the same rules described in theInspecting Codesection with regard to what?",
        "Y": "forwardmethod lookup",
        "Z": "graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the script that produces the graph?",
        "Y": "test.py",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the value that we assign the output to?",
        "Y": "ofTensortype",
        "Z": "graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the instruction that produces the graph?",
        "Y": "%rv.1:Tensor",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of value is assigned to the output?",
        "Y": "ofTensortype",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What specifies which values in scope should be passed as inputs?",
        "Y": "aten::zerosis the operator (equivalent totorch.zeros) and the input list",
        "Z": "and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where can the schema for built-in functions likeaten::zeros be found?",
        "Y": "atBuiltin Functions",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does %rv.1:Tensormeans assign the output to?",
        "Y": "a (unique) value namedrv.1",
        "Z": "The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the operator (equivalent totorch.zeros) that specifies which values in scope should be passed as inputs?",
        "Y": "aten",
        "Z": "The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be found atBuiltin Functions?",
        "Y": "built-in functions likeaten::zeros",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of an instruction that assigns the output to a (unique) value namedrv.1?",
        "Y": "instruction%rv.1:Tensor=aten::zeros",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the instruction%rv.1:Tensor=aten?",
        "Y": "test.py",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the unique value assigned to rv.1?",
        "Y": "ofTensortype",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Which operator specifies which values in scope should be passed as inputs?",
        "Y": "aten",
        "Z": "aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the unique value assigned to the output?",
        "Y": "%rv.1",
        "Z": "%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the equivalent totorch.zeros?",
        "Y": "aten",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of functions can be found atBuiltin Functions?",
        "Y": "built-in functions",
        "Z": "aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the operator?",
        "Y": "aten",
        "Z": "aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the location in the original source file that generated this instruction?",
        "Y": "#test.py:9:10",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where is the file named test.py located?",
        "Y": "line 9",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What operator specifies which values in scope should be passed as inputs?",
        "Y": "aten",
        "Z": "aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the file that generated the instruction?",
        "Y": "test.py",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Operators can also have what?",
        "Y": "associatedblocks",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Why are operators formatted to reflect their equivalent source code forms?",
        "Y": "to facilitate easy debugging",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Graphs can be inspected as shown to confirm that the computation described by what is correct?",
        "Y": "aScriptModuleis",
        "Z": "#test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be inspected to confirm that the computation described by aScriptModule is correct?",
        "Y": "Graphs",
        "Z": "Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be inspected to confirm that the computation described by aScriptModuleis correct?",
        "Y": "Graphs",
        "Z": "aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of cases exist where the trace of a given Python function/module will not be representative of the underlying code?",
        "Y": "edge cases",
        "Z": "There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is dependent on inputs?",
        "Y": "control flow",
        "Z": "There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a control flow dependent on inputs?",
        "Y": "tensor shapes",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of in-place operations of tensor views?",
        "Y": "indexing",
        "Z": "There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of control flow that is dependent on inputs (e.g. what?",
        "Y": "tensor shapes",
        "Z": "%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of in-place operations of tensor views (e.g. what on the left-hand side of an assignment) is",
        "Y": "indexing",
        "Z": "Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a control flow that is dependent on inputs?",
        "Y": "tensor shapes",
        "Z": "There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a tensor view that may not be traceable in the future?",
        "Y": "indexing",
        "Z": "There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of control flow that is dependent on what?",
        "Y": "inputs",
        "Z": "Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of control flow that is dependent on inputs (e.g. what) Tracing of in-place operations of tens",
        "Y": "tensor shapes",
        "Z": "Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of in-place operations of tensor views (e.g. what on the left-hand side of an assignment)",
        "Y": "indexing",
        "Z": "There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is one way to automatically catch many errors in traces?",
        "Y": "check_inputson",
        "Z": "There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a list of tuples of inputs that will be used to re-trace the computation and verify the",
        "Y": "example",
        "Z": "Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the term for in-place operations of tensor views?",
        "Y": "Tracing",
        "Z": "Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In-place operations of tensor views (e.g. what on the left-hand side of an assignment) may be traceable in",
        "Y": "indexing",
        "Z": "Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a list of tuples of inputs that will be used to re-trace the computation and verify the results?",
        "Y": "check_inputson",
        "Z": "%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does check_inputson thetorch.jit.trace()API.check_inputstakes give us?",
        "Y": "diagnostic information",
        "Z": "aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In the future, these cases may be what?",
        "Y": "traceable",
        "Z": "Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a check_inputson that will be used to re-trace the computation and verify the results?",
        "Y": "Gives us the following diagnostic information",
        "Z": "Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does check_inputson thetorch.jit.trace()API.check_inputstakes contain?",
        "Y": "a list of tuples of inputs",
        "Z": "There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does check_inputson do?",
        "Y": "a list of tuples of inputs that will be used to re-trace the computation and verify the results",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does check_inputson give us?",
        "Y": "diagnostic information",
        "Z": "One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does this message indicate to us that the computation differed between when we first traced it and when we traced it?",
        "Y": "thecheck_inputs",
        "Z": "Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the loop within the body ofloop_in_traced_fn depend on?",
        "Y": "the shape of the inputx",
        "Z": "One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How can a data-dependent control flow be captured?",
        "Y": "usingtorch.jit.script()",
        "Z": "One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "The loop within the body ofloop_in_traced_fndepends on what?",
        "Y": "the shape of the inputx",
        "Z": "One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How can data-dependent control flow be captured?",
        "Y": "usingtorch.jit.script()",
        "Z": "aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "A trace of a function that contains an in-place assignment on a slice of a Tensor produces several warnings and what else?",
        "Y": "a graph",
        "Z": "In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What produces warnings for several problematic patterns in traced computation?",
        "Y": "tracer",
        "Z": "The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "A trace of a function that contains an in-place assignment on a slice (a view) of a Tensor produces several warnings",
        "Y": "a graph",
        "Z": "In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an in-place assignment on a Tensor?",
        "Y": "a slice",
        "Z": "The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the tracer produce for several problematic patterns in traced computation?",
        "Y": "warnings",
        "Z": "The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be done to fix the warnings?",
        "Y": "build up the result tensor out-of-place withtorch",
        "Z": "There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can you use this to ensure TorchScript has captured correctly?",
        "Y": "model code",
        "Z": "This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How can you use the forwardmethod?",
        "Y": "to ensure TorchScript (tracing or scripting) has captured your model code correctly",
        "Z": "This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How can we fix this problem?",
        "Y": "build up the result tensor out-of-place withtorch",
        "Z": "The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the model train on GPU and do inference on?",
        "Y": "CPU",
        "Z": "Q: I would like to train a model on GPU and do inference on CPU. What are the\nbest practices? First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What do I want to train a model on?",
        "Y": "GPU",
        "Z": "Q: I would like to train a model on GPU and do inference on CPU. What are the\nbest practices? First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What are the best practices for training a model on GPU and doing inference on CPU?",
        "Y": "best practices",
        "Z": "Q: I would like to train a model on GPU and do inference on CPU. What are the\nbest practices? First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "The tracer may witness what type of creation on a specific device?",
        "Y": "tensor",
        "Z": "This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does casting the model before saving it do?",
        "Y": "ensures that the tracer has the correct device information",
        "Z": "First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What do I store attributes on?",
        "Y": "aScriptModule",
        "Z": "Q: I would like to train a model on GPU and do inference on CPU. What are the\nbest practices? First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of aScriptModule?",
        "Y": "model like:",
        "Z": "First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What may the tracer witness on a specific device?",
        "Y": "tensor creation",
        "Z": "This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does casting a model before saving ensure the tracer has?",
        "Y": "the correct device information",
        "Z": "First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where do I store attributes?",
        "Y": "aScriptModule",
        "Z": "First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does aScriptModule store attributes on?",
        "Y": "a model",
        "Z": "First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What do I store on aScriptModule?",
        "Y": "attributes",
        "Z": "Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Casting the model before saving ensures that the tracer has what?",
        "Y": "the correct device information",
        "Z": "This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is one way to inform the compiler of attributes onScriptModule?",
        "Y": "How do I store attributes on aScriptModule",
        "Z": "This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What model will result in a compilation error?",
        "Y": "IfModelis instantiated",
        "Z": "This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How many ways to inform the compiler of attributes onScriptModule?",
        "Y": "4",
        "Z": "This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will happen if a model is instantiated?",
        "Y": "a compilation error",
        "Z": "Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is one way to inform the compiler of attributes on aScriptModule?",
        "Y": "How do I store attributes on aScriptModule",
        "Z": "Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What model would result in a compilation error if the compiler didn't know aboutx?",
        "Y": "IfModelis instantiated",
        "Z": "Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of attribute does register_buffer correspond to?",
        "Y": "typeTensor",
        "Z": "Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is equivalent to an attribute of typeTensor?",
        "Y": "register_buffer",
        "Z": "IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What attribute is equivalent to register_buffer?",
        "Y": "typeTensor",
        "Z": "First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "IfModelis instantiated, what will happen to a model?",
        "Y": "a compilation error",
        "Z": "Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will result in a compilation error since the compiler doesn't know aboutx?",
        "Y": "IfModelis instantiated",
        "Z": "First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How many ways are there to inform the compiler of attributes onScriptModule?",
        "Y": "4",
        "Z": "IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is register_buffer equivalent to?",
        "Y": "attribute",
        "Z": "2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor. 3. Constants - Annotating a class member asFinal(or adding it to a list called__constants__at the class definition level) will mark the contained names\nas constants. Constants are saved directly in the code of the model. Seebuiltin-constantsfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "IfModelis is instantiated, what will happen?",
        "Y": "a compilation error",
        "Z": "IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Register_buffer is equivalent to what type of typeTensor?",
        "Y": "attribute",
        "Z": "1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor. 3. Constants - Annotating a class member asFinal(or adding it to a list called__constants__at the class definition level) will mark the contained names\nas constants. Constants are saved directly in the code of the model. Seebuiltin-constantsfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How many Constants are saved directly in the code of the model?",
        "Y": "3.",
        "Z": "2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor. 3. Constants - Annotating a class member asFinal(or adding it to a list called__constants__at the class definition level) will mark the contained names\nas constants. Constants are saved directly in the code of the model. Seebuiltin-constantsfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Annotating a class member asFinal or adding it to a list called what will mark the contained names as constants?",
        "Y": "constants",
        "Z": "2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor. 3. Constants - Annotating a class member asFinal(or adding it to a list called__constants__at the class definition level) will mark the contained names\nas constants. Constants are saved directly in the code of the model. Seebuiltin-constantsfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where are constants saved?",
        "Y": "the code of the model",
        "Z": "2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor. 3. Constants - Annotating a class member asFinal(or adding it to a list called__constants__at the class definition level) will mark the contained names\nas constants. Constants are saved directly in the code of the model. Seebuiltin-constantsfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How are constants saved in the code of the model?",
        "Y": "Seebuiltin-constantsfor details",
        "Z": "2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor. 3. Constants - Annotating a class member asFinal(or adding it to a list called__constants__at the class definition level) will mark the contained names\nas constants. Constants are saved directly in the code of the model. Seebuiltin-constantsfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Values wrapped inregister_buffer will work as they do what?",
        "Y": "onnn.Modules",
        "Z": "2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor. 3. Constants - Annotating a class member asFinal(or adding it to a list called__constants__at the class definition level) will mark the contained names\nas constants. Constants are saved directly in the code of the model. Seebuiltin-constantsfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What do I want to trace?",
        "Y": "module\u2019s method",
        "Z": "Q: I would like to trace module\u2019s method but I keep getting this error: RuntimeError:CannotinsertaTensorthatrequiresgradasaconstant.Considermakingitaparameterorinput,ordetachingthegradient This error usually means that the method you are tracing uses a module\u2019s parameters and\nyou are passing the module\u2019s method instead of the module instance (e.g.my_module_instance.forwardvsmy_module_instance). Invokingtracewith a module\u2019s method captures module parameters (which may require gradients) asconstants.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Invokingtracewith a module's method captures what asconstants?",
        "Y": "module parameters",
        "Z": "RuntimeError:CannotinsertaTensorthatrequiresgradasaconstant.Considermakingitaparameterorinput,ordetachingthegradient This error usually means that the method you are tracing uses a module\u2019s parameters and\nyou are passing the module\u2019s method instead of the module instance (e.g.my_module_instance.forwardvsmy_module_instance). Invokingtracewith a module\u2019s method captures module parameters (which may require gradients) asconstants.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What are you passing instead of the module instance?",
        "Y": "module\u2019s method",
        "Z": "RuntimeError:CannotinsertaTensorthatrequiresgradasaconstant.Considermakingitaparameterorinput,ordetachingthegradient This error usually means that the method you are tracing uses a module\u2019s parameters and\nyou are passing the module\u2019s method instead of the module instance (e.g.my_module_instance.forwardvsmy_module_instance). Invokingtracewith a module\u2019s method captures module parameters (which may require gradients) asconstants.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the method you are tracing use?",
        "Y": "module\u2019s parameters",
        "Z": "This error usually means that the method you are tracing uses a module\u2019s parameters and\nyou are passing the module\u2019s method instead of the module instance (e.g.my_module_instance.forwardvsmy_module_instance). Invokingtracewith a module\u2019s method captures module parameters (which may require gradients) asconstants. On the other hand, invokingtracewith module\u2019s instance (e.g.my_module) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Invokingtracewith a module's method captures what?",
        "Y": "module parameters",
        "Z": "Invokingtracewith a module\u2019s method captures module parameters (which may require gradients) asconstants. On the other hand, invokingtracewith module\u2019s instance (e.g.my_module) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required. To trace a specific method on a module, seetorch.jit.trace_module",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What creates a new module and copies parameters into the new module?",
        "Y": "invokingtracewith module\u2019s instance",
        "Z": "Invokingtracewith a module\u2019s method captures module parameters (which may require gradients) asconstants. On the other hand, invokingtracewith module\u2019s instance (e.g.my_module) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required. To trace a specific method on a module, seetorch.jit.trace_module",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is passed instead of the module instance?",
        "Y": "module\u2019s method",
        "Z": "This error usually means that the method you are tracing uses a module\u2019s parameters and\nyou are passing the module\u2019s method instead of the module instance (e.g.my_module_instance.forwardvsmy_module_instance). Invokingtracewith a module\u2019s method captures module parameters (which may require gradients) asconstants. On the other hand, invokingtracewith module\u2019s instance (e.g.my_module) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Invokingtracewith a module's method captures what as constants?",
        "Y": "module parameters",
        "Z": "Invokingtracewith a module\u2019s method captures module parameters (which may require gradients) asconstants. On the other hand, invokingtracewith module\u2019s instance (e.g.my_module) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required. To trace a specific method on a module, seetorch.jit.trace_module",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does invokingtrace with a module's instance do?",
        "Y": "creates a new module and correctly copies parameters into the new module",
        "Z": "Invokingtracewith a module\u2019s method captures module parameters (which may require gradients) asconstants. On the other hand, invokingtracewith module\u2019s instance (e.g.my_module) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required. To trace a specific method on a module, seetorch.jit.trace_module",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does seetorch.jit.trace_module do?",
        "Y": "trace a specific method on a module",
        "Z": "Invokingtracewith a module\u2019s method captures module parameters (which may require gradients) asconstants. On the other hand, invokingtracewith module\u2019s instance (e.g.my_module) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required. To trace a specific method on a module, seetorch.jit.trace_module",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used to trace a specific method on a module?",
        "Y": "seetorch.jit.trace_module",
        "Z": "Invokingtracewith a module\u2019s method captures module parameters (which may require gradients) asconstants. On the other hand, invokingtracewith module\u2019s instance (e.g.my_module) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required. To trace a specific method on a module, seetorch.jit.trace_module",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "If you're usingSequentialwith TorchScript, the inputs of some of theSequentialsubmodules may be",
        "Y": "falsely inferred to beTensor",
        "Z": "If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the canonical solution to this problem?",
        "Y": "subclassnn",
        "Z": "If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "The inputs of some of theSequentialsubmodules may be falsely inferred to what?",
        "Y": "beTensor",
        "Z": "If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the canonical solution?",
        "Y": "redeclareforward",
        "Z": "If you\u2019re usingSequentialwith TorchScript, the inputs of some\nof theSequentialsubmodules may be falsely inferred to beTensor, even if they\u2019re annotated otherwise. The canonical\nsolution is to subclassnn.Sequentialand redeclareforwardwith the input typed correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What version of TorchScript does this section detail?",
        "Y": "PyTorch 1.2",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can you do if you are new to TorchScript?",
        "Y": "skip this section",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How many main changes are there to the TorchScript API with PyTorch 1.2?",
        "Y": "two",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What will now attempt to recursively compile functions, methods, and classes that it encounters?",
        "Y": "1.torch.jit.script",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the command that makes compilation of TorchScript \"opt-out\" instead of \"opt-in\"?",
        "Y": "calltorch.jit.script",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How many main changes to the TorchScript API with PyTorch 1.2?",
        "Y": "two",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When you calltorch.jit.script, compilation is what?",
        "Y": "\u201copt-out\u201d, rather than \u201copt-in\u201d",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When you compile functions, methods, and classes, compilation is \"opt-out\", rather than \"opt-in\"?",
        "Y": "calltorch.jit.script",
        "Z": "1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the preferred way to createScriptModules?",
        "Y": "2.torch.jit.script(nn_module_instance)",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a simpler, easier-to-use API for?",
        "Y": "converting yournn.Modules intoScriptModules",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Torch.jit.script now try to do?",
        "Y": "recursively compile functions, methods, and classes",
        "Z": "1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In what environment are yournn.Modules ready to be optimized and executed?",
        "Y": "a non-Python environment",
        "Z": "1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "ScriptModules are ready to be optimized and executed in a non-what environment?",
        "Y": "Python",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is compiled by default?",
        "Y": "module\u2019sforward",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In what order are methods called fromforward compiled?",
        "Y": "inforward",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the module'sforward compiled by?",
        "Y": "default",
        "Z": "When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached).",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What method is lazily compiled in the order they are used inforward?",
        "Y": "fromforwardare",
        "Z": "When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a way to compile a method that is not called fromforward?",
        "Y": "add@torch.jit.export",
        "Z": "The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a way to stop the compiler from compiling a method?",
        "Y": "add@torch.jit.ignoreor@torch.jit.unused",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To what does the method need to be called?",
        "Y": "python",
        "Z": "The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the new usage look like?",
        "Y": "@ignoredcannot be exported;@unusedcan",
        "Z": "The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To compile a method that is not called fromforward that is not called fromforward, what is done?",
        "Y": "add@torch.jit.export",
        "Z": "The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does add@torch.jit.export do to stop the compiler from compiling a method?",
        "Y": "add@torch.jit.ignoreor",
        "Z": "The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @ignoreor do to a call to python?",
        "Y": "@ignoreleaves the method",
        "Z": "The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @ignoredcannot be exported?",
        "Y": "@ignoredcannot be exported;@unusedcan",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What are the two exceptions that prevent the compiler from compiling a method that is not called fromforward?",
        "Y": "@ignoredcannot be exported;@unusedcan",
        "Z": "The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does a compiler do to compile a method that is not called fromforward?",
        "Y": "add@torch.jit.export",
        "Z": "To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What cannot be exported?",
        "Y": "@ignored",
        "Z": "method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To what does the method have to be called to stop the compiler from compiling it?",
        "Y": "python",
        "Z": "To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the difference between an exported method and an unused one?",
        "Y": "@ignoredcannot be exported;@unusedcan",
        "Z": "To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Most attribute types can be what?",
        "Y": "inferred",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of type can be annotated using PEP 526-styleclass annotations?",
        "Y": "empty container types",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does add@torch.jit.ignoreor do?",
        "Y": "@ignoreleaves the method",
        "Z": "To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be inferred?",
        "Y": "attribute types",
        "Z": "Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of container types can be annotated using PEP 526-styleclass annotations?",
        "Y": "empty container types",
        "Z": "To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @ignoreleaves the method as a call to?",
        "Y": "python",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be used to mark constants instead of adding the name of the member to__constants__?",
        "Y": "aFinalclass annotation",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does add@torch.jit.ignoreor@torch.jit.unused do?",
        "Y": "stop the compiler from compiling a method",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @ignoreor do to stop the compiler from compiling a method as a call to python?",
        "Y": "@ignoreleaves",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @ignoredcannot be exported do?",
        "Y": "@ignoredcannot be exported",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What do empty container types do?",
        "Y": "annotate their types usingPEP 526-styleclass annotations",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be marked with aFinalclass annotation instead of adding the name of the member to__constants__?",
        "Y": "Constants",
        "Z": "Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the method a call to?",
        "Y": "python",
        "Z": "method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How many styleclass annotations are needed for empty container types?",
        "Y": "526",
        "Z": "method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of hints can be used in place oftorch.jit.annotate?",
        "Y": "Python 3",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What happens when a method is called to python?",
        "Y": "and@unusedreplaces it with an exception",
        "Z": "method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of containers can be annotated using PEP 526-styleclass annotations?",
        "Y": "empty container types",
        "Z": "method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be used in place oftorch.jit.annotate?",
        "Y": "Python 3 type hints",
        "Z": "The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of attributes can be annotated using PEP 526-styleclass annotations?",
        "Y": "empty container types",
        "Z": "method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecor",
        "Y": "Python 3",
        "Z": "Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of hints can be used in place oftorch.jit.annotate The@torch.jit.script_methodde",
        "Y": "Python 3",
        "Z": "Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What classes inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class?",
        "Y": "The@torch.jit.script_methoddecorator Classes",
        "Z": "The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What class inherits fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class",
        "Y": "The__constants__array",
        "Z": "The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What class does The__constants__array inherit from?",
        "Y": "Attributewrapper",
        "Z": "The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the function that changes behavior in PyTorch 1.2?",
        "Y": "Thetorch.jit.annotatefunction",
        "Z": "Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Before PyTorch 1.2, what decorator was used to make a function or method callable from code that is exported?",
        "Y": "@ignore",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used to get the @ignore decorator back?",
        "Y": "@torch.jit.unused()",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is now equivalent to @torch.jit.ignore(drop=False)?",
        "Y": "@torch.jit.ignoreis",
        "Z": "The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the decorator used to make a function or method callable from code that was exported before PyTorch 1.2?",
        "Y": "@torch.jit.ignore",
        "Z": "Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When did The@torch.jit.ignoreannotation's behavior change?",
        "Y": "PyTorch 1.2",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the decorator used to make a function or method callable from code that is exported?",
        "Y": "@torch.jit.ignore",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In what version of Python did the @torch.jit.ignoreannotation's behavior change?",
        "Y": "PyTorch 1.2",
        "Z": "The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is now equivalent to @torch.jit.ignore?",
        "Y": "@torch.jit.ignore",
        "Z": "Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is another name for the @ignore decorator in PyTorch 1.2?",
        "Y": "@torch.jit.ignore",
        "Z": "The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Which module's data is copied to aScriptModule when passed to thetorch.jit.scriptfunction?",
        "Y": "atorch.nn.Module",
        "Z": "The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "By what method is the module'sforward compiled?",
        "Y": "default",
        "Z": "When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a method on annn.Module used for?",
        "Y": "an entry point into aScriptModuleand",
        "Z": "When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Who compiles the module?",
        "Y": "TorchScript compiler",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In addition to fromforward, what is compiled in the order they are used inforward?",
        "Y": "any@torch.jit.exportmethods",
        "Z": "When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used as an entry point into aScriptModule?",
        "Y": "annn.Module",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used as an entry point into aScriptModuleand?",
        "Y": "annn.Moduleis",
        "Z": "This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is assumed to be an entry point?",
        "Y": "forwardimplicitly",
        "Z": "When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Functions and methods called what are compiled as they are seen by the compiler?",
        "Y": "fromforwardare",
        "Z": "When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a method that does not need this decorator?",
        "Y": "@torch.jit.exporton a method",
        "Z": "This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does this decorator indicate about a method on annn.Module?",
        "Y": "Warning",
        "Z": "This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is assumed to be an entry point, so it does not need this decorator?",
        "Y": "forwardimplicitly",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What are compiled as they are seen by the compiler?",
        "Y": "Functions and methods called fromforward",
        "Z": "forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Functions can be decorated with what if needed?",
        "Y": "@torch.jit.ignoreortorch.jit",
        "Z": "Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does this decorator indicate that a method on annn.Module should be compiled?",
        "Y": "Warning",
        "Z": "This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Forwardimplicitly is assumed to be what?",
        "Y": "entry point",
        "Z": "forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a method that does not need a decorator?",
        "Y": "@torch.jit.exporton a method",
        "Z": "When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does forwardimplicitly do?",
        "Y": "Warning",
        "Z": "forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the warning that can be added to Functions and methods called fromforwardimplicitly?",
        "Y": "Warning",
        "Z": "forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the main benefit of using @torch.jit.exporton a method?",
        "Y": "Functions don\u2019t change much",
        "Z": "forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is TorchScript class support?",
        "Y": "experimental",
        "Z": "Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of type is TorchScript best suited for?",
        "Y": "aNamedTuple",
        "Z": "This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is exported by default?",
        "Y": "Everything in a user definedTorchScript Classis exported by default",
        "Z": "Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Functions can be decorated with what?",
        "Y": "@torch.jit.ignoreortorch.jit.unusedif needed",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How is everything in a user definedTorchScript Class exported?",
        "Y": "by default",
        "Z": "Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API: TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Why can functions be decorated with@torch.jit.ignoreortorch.jit.unusedif needed?",
        "Y": "Functions don\u2019t change much",
        "Z": "Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a simple record-like type?",
        "Y": "aNamedTuple",
        "Z": "Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What doesn't change much?",
        "Y": "Functions",
        "Z": "Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can functions be decorated with if needed?",
        "Y": "@torch.jit.ignore",
        "Z": "Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is TorchScript best suited for?",
        "Y": "simple record-like types",
        "Z": "Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be decorated with@torch.jit.ignore if needed?",
        "Y": "functions",
        "Z": "Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Who needs to know the types ofmodule attributes?",
        "Y": "The TorchScript compiler",
        "Z": "The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API: TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API: Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can most types be inferred from?",
        "Y": "the value of the member",
        "Z": "The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API: TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API: Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What cannot have their types inferred and must have their types annotated withPEP 526-styleclass annotations?",
        "Y": "Empty lists and dicts",
        "Z": "Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API: TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "If a type cannot be inferred and is not explicitly annotated, it will not be added as what to the resultingScriptModul",
        "Y": "an attribute",
        "Z": "forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How is everything exported in a TorchScript Class?",
        "Y": "by default",
        "Z": "Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the TorchScript compiler need to know?",
        "Y": "types ofmodule attributes",
        "Z": "The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Most types can be inferred from what?",
        "Y": "value of the member",
        "Z": "The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What cannot have their types inferred and must have their types annotated with PEP 526-styleclass annotations?",
        "Y": "Empty lists and dicts",
        "Z": "The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API: TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API: Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What happens if a type is not explicitly annotated?",
        "Y": "it will not be added as an attribute",
        "Z": "Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What happens if a type cannot be inferred and is not explicitly annotated?",
        "Y": "it will not be added as an attribute",
        "Z": "The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be used to mark members as constant?",
        "Y": "TheFinaltype constructor",
        "Z": "The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API: TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API: Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What happens if members are not marked constant?",
        "Y": "If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute",
        "Z": "The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API: TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API: Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What opens opportunities for optimization if the value is known to be fixed?",
        "Y": "UsingFinal",
        "Z": "TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the new API?",
        "Y": "New API",
        "Z": "TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is assumed to have typeTensor?",
        "Y": "Containers",
        "Z": "Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What was Torch.jit.annotate used to tell?",
        "Y": "TorchScript compiler",
        "Z": "The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API: TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API: Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What are now supported?",
        "Y": "Python 3 style type hints",
        "Z": "The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API: TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API: Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Containers are assumed to have what?",
        "Y": "typeTensor",
        "Z": "Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What compiler used Torch.jit.annotate?",
        "Y": "TorchScript",
        "Z": "Old API: New API: Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is now supported by TorchScript?",
        "Y": "Python 3 style type hints",
        "Z": "Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What doesn't change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif",
        "Y": "Functions",
        "Z": "Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What compiler was Torch.jit.annotate used to tell?",
        "Y": "TorchScript",
        "Z": "Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is now supported?",
        "Y": "Python 3 style type hints",
        "Z": "Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What compiler used Torch.jit.annotate to tell what type should be?",
        "Y": "TorchScript",
        "Z": "Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Alias fortorch.le stand for?",
        "Y": "Alias fortorch.le",
        "Z": "Alias fortorch.le().",
        "source": "https://pytorch.org/docs/stable/generated/torch.less_equal.html#torch.less_equal"
    },
    {
        "X": "What is each zero or one dimensional tensortintensors first reshaped into?",
        "Y": "a(t.numel(),1)column",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors. Equivalent totorch.hstack(tensors), except each zero or one dimensional tensortintensorsis first reshaped into a(t.numel(),1)column before being stacked horizontally. tensors(sequence of Tensors) \u2013 sequence of tensors to concatenate out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.column_stack.html#torch.column_stack"
    },
    {
        "X": "What type of tensor does ifobj return True?",
        "Y": "PyTorch tensor",
        "Z": "Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor"
    },
    {
        "X": "What does ifobjis a PyTorch tensor return?",
        "Y": "True",
        "Z": "Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor"
    },
    {
        "X": "What is the function that returns true ifobjis a PyTorch tensor?",
        "Y": "doingisinstance",
        "Z": "Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor"
    },
    {
        "X": "What is better for typechecking with mypy?",
        "Y": "thatisinstancecheck",
        "Z": "Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor"
    },
    {
        "X": "What is the return value for ifobjis a PyTorch tensor?",
        "Y": "True",
        "Z": "Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor"
    },
    {
        "X": "What is the function that returns True ifobjis a PyTorch tensor?",
        "Y": "doingisinstance(obj,Tensor)",
        "Z": "Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor"
    },
    {
        "X": "What is thatisinstancecheck better for?",
        "Y": "typechecking with mypy",
        "Z": "Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor"
    },
    {
        "X": "obj(obj,Tensor) \u2013 Object to test Example:",
        "Y": "Object",
        "Z": "Returns True ifobjis a PyTorch tensor. Note that this function is simply doingisinstance(obj,Tensor).\nUsing thatisinstancecheck is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead ofis_tensor. obj(Object) \u2013 Object to test Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor"
    },
    {
        "X": "What does set_flush_denormal() do?",
        "Y": "Disables denormal floating numbers on CPU",
        "Z": "Disables denormal floating numbers on CPU. ReturnsTrueif your system supports flushing denormal numbers and it\nsuccessfully configures flush denormal mode.set_flush_denormal()is only supported on x86 architectures supporting SSE3. mode(bool) \u2013 Controls whether to enable flush denormal mode or not Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal"
    },
    {
        "X": "If your system supports flushing denormal numbers and it successfully configures what?",
        "Y": "flush denormal mode",
        "Z": "Disables denormal floating numbers on CPU. ReturnsTrueif your system supports flushing denormal numbers and it\nsuccessfully configures flush denormal mode.set_flush_denormal()is only supported on x86 architectures supporting SSE3. mode(bool) \u2013 Controls whether to enable flush denormal mode or not Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal"
    },
    {
        "X": "On what architectures is set_flush_denormal() only supported?",
        "Y": "x86 architectures",
        "Z": "Disables denormal floating numbers on CPU. ReturnsTrueif your system supports flushing denormal numbers and it\nsuccessfully configures flush denormal mode.set_flush_denormal()is only supported on x86 architectures supporting SSE3. mode(bool) \u2013 Controls whether to enable flush denormal mode or not Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal"
    },
    {
        "X": "What controls whether to enable flush denormal mode or not?",
        "Y": "mode(bool)",
        "Z": "Disables denormal floating numbers on CPU. ReturnsTrueif your system supports flushing denormal numbers and it\nsuccessfully configures flush denormal mode.set_flush_denormal()is only supported on x86 architectures supporting SSE3. mode(bool) \u2013 Controls whether to enable flush denormal mode or not Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal"
    },
    {
        "X": "What does set_flush_denormal() do on CPU?",
        "Y": "Disables denormal floating numbers",
        "Z": "Disables denormal floating numbers on CPU. ReturnsTrueif your system supports flushing denormal numbers and it\nsuccessfully configures flush denormal mode.set_flush_denormal()is only supported on x86 architectures supporting SSE3. mode(bool) \u2013 Controls whether to enable flush denormal mode or not Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal"
    },
    {
        "X": "ReturnsTrueif your system supports what?",
        "Y": "flushing denormal numbers",
        "Z": "Disables denormal floating numbers on CPU. ReturnsTrueif your system supports flushing denormal numbers and it\nsuccessfully configures flush denormal mode.set_flush_denormal()is only supported on x86 architectures supporting SSE3. mode(bool) \u2013 Controls whether to enable flush denormal mode or not Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal"
    },
    {
        "X": "Sums the product of the elements of the inputoperandsalong dimensions specified using a notation based on what convention?",
        "Y": "Einstein summation convention",
        "Z": "Sums the product of the elements of the inputoperandsalong dimensions specified using a notation\nbased on the Einstein summation convention.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "Sums the product of the elements of what dimension specified using a notation based on the Einstein summation convention?",
        "Y": "inputoperandsalong",
        "Z": "Sums the product of the elements of the inputoperandsalong dimensions specified using a notation\nbased on the Einstein summation convention.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What summation convention is used to sum the product of the elements of inputoperandsalong dimensions specified?",
        "Y": "Einstein",
        "Z": "Sums the product of the elements of the inputoperandsalong dimensions specified using a notation\nbased on the Einstein summation convention.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What is the name of the equation that is used to describe it?",
        "Y": "Equation",
        "Z": "Equation:",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What is the name of the Equation?",
        "Y": "Equation",
        "Z": "Equation:",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What can be added at the end of the equation to define the output subscripts?",
        "Y": "an arrow",
        "Z": "Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What is an example of an equation that can be explicitly defined by adding an arrow at the end of the equation followed by the subscripts for the",
        "Y": "the following equation computes the transpose of a matrix multiplication",
        "Z": "Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "How often must the output subscripts appear for some input operand?",
        "Y": "at least once",
        "Z": "Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "How can the output subscripts be explicitly defined?",
        "Y": "by adding an arrow (\u2018->\u2019) at the end of the equation",
        "Z": "Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "The following equation computes the transpose of what?",
        "Y": "matrix multiplication",
        "Z": "Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What does torch.einsumhandle ellipsis differ from?",
        "Y": "NumPy",
        "Z": "Note torch.einsumhandles ellipsis (\u2018\u2026\u2019) differently from NumPy in that it allows dimensions\ncovered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output. Note This function does not optimize the given expression, so a different formula for the same computation may\nrun faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\ncan optimize the formula for you.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "Why does torch.einsum not optimize the given expression?",
        "Y": "a different formula for the same computation may run faster or consume less memory",
        "Z": "torch.einsumhandles ellipsis (\u2018\u2026\u2019) differently from NumPy in that it allows dimensions\ncovered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output. Note This function does not optimize the given expression, so a different formula for the same computation may\nrun faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\ncan optimize the formula for you. equation(string) \u2013 The subscripts for the Einstein summation.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What project can optimize the formula for you?",
        "Y": "opt_einsum",
        "Z": "Note This function does not optimize the given expression, so a different formula for the same computation may\nrun faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\ncan optimize the formula for you. equation(string) \u2013 The subscripts for the Einstein summation. operands(Tensor) \u2013 The operands to compute the Einstein sum of. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What does torch.einsum handle differently from NumPy?",
        "Y": "ellipsis",
        "Z": "Note torch.einsumhandles ellipsis (\u2018\u2026\u2019) differently from NumPy in that it allows dimensions\ncovered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output. Note This function does not optimize the given expression, so a different formula for the same computation may\nrun faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\ncan optimize the formula for you.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What does torch.einsum handle differently from NumPy in that it allows dimensions covered by the ellipsis to be summed",
        "Y": "ellipsis",
        "Z": "torch.einsumhandles ellipsis (\u2018\u2026\u2019) differently from NumPy in that it allows dimensions\ncovered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output. Note This function does not optimize the given expression, so a different formula for the same computation may\nrun faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\ncan optimize the formula for you. equation(string) \u2013 The subscripts for the Einstein summation.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What are the subscripts for the Einstein summation?",
        "Y": "equation(string)",
        "Z": "Note This function does not optimize the given expression, so a different formula for the same computation may\nrun faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\ncan optimize the formula for you. equation(string) \u2013 The subscripts for the Einstein summation. operands(Tensor) \u2013 The operands to compute the Einstein sum of. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "Why does this function not optimize the given expression?",
        "Y": "a different formula for the same computation may run faster or consume less memory",
        "Z": "Note This function does not optimize the given expression, so a different formula for the same computation may\nrun faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\ncan optimize the formula for you. equation(string) \u2013 The subscripts for the Einstein summation. operands(Tensor) \u2013 The operands to compute the Einstein sum of. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What are the operands to compute the Einstein sum of?",
        "Y": "operands",
        "Z": "Note This function does not optimize the given expression, so a different formula for the same computation may\nrun faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\ncan optimize the formula for you. equation(string) \u2013 The subscripts for the Einstein summation. operands(Tensor) \u2013 The operands to compute the Einstein sum of. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What are operands(Tensor) \u2013 The operands to compute the Einstein sum of?",
        "Y": "Examples",
        "Z": "Note This function does not optimize the given expression, so a different formula for the same computation may\nrun faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)\ncan optimize the formula for you. equation(string) \u2013 The subscripts for the Einstein summation. operands(Tensor) \u2013 The operands to compute the Einstein sum of. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "If one of the elements being compared is what, then that element is returned.maximum()is not supported for tensors with",
        "Y": "a NaN",
        "Z": "Computes the element-wise maximum ofinputandother. Note If one of the elements being compared is a NaN, then that element is returned.maximum()is not supported for tensors with complex dtypes. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum"
    },
    {
        "X": "What is the Bartlett window function?",
        "Y": "full window size",
        "Z": "Bartlett window function. whereNNNis the full window size.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "Where is the full window size?",
        "Y": "whereNNNis the full window size",
        "Z": "whereNNNis the full window size.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is the same as totorch.bartlett_window(L+1,periodic=False)?",
        "Y": "havetorch.bartlett_window",
        "Z": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]).",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is the name of the window that contains a single value?",
        "Y": "window_length",
        "Z": "Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "If False, return a window to be used as periodic function.",
        "Y": "symmetric window",
        "Z": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What type of tensor type does ifNone use?",
        "Y": "Default",
        "Z": "Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What type of tensors are supported?",
        "Y": "floating point types",
        "Z": "Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is a window Tensor of size(window_length,)(textwindow_length,)(wind",
        "Y": "1-D tensor",
        "Z": "device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is deprecated and may be removed in a future PyTorch release?",
        "Y": "torch.norm",
        "Z": "torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord)",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is used when computing vector norms?",
        "Y": "ortorch.linalg.vector_norm()",
        "Z": "Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the signature for these functions?",
        "Y": "slightly different",
        "Z": "Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What function is used when computing vector norms?",
        "Y": "ortorch.linalg.vector_norm()",
        "Z": "Warning torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What does ortorch.linalg.matrix_norm() do?",
        "Y": "matrix norms",
        "Z": "torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "How different is the signature for these functions from the signature for torch.norm?",
        "Y": "slightly different",
        "Z": "Returns the matrix norm or vector norm of a given tensor. Warning torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. dim(int,tuple of python:ints,list of python:ints,optional) \u2013 Specifies which dimension or dimensions ofinputto\ncalculate the norm across. IfdimisNone, the norm will\nbe calculated across all dimensions ofinput. If the norm\ntype indicated bypdoes not support the specified number of\ndimensions, an error will occur. keepdim(bool,optional) \u2013 whether the output tensors havedimretained or not. Ignored ifdim=Noneandout=None. Default:False out(Tensor,optional) \u2013 the output tensor. Ignored ifdim=Noneandout=None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What function does usetorch.linalg.norm() instead of torch.norm?",
        "Y": "ortorch.linalg.vector_norm()",
        "Z": "torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What type of data type must the input tensor have?",
        "Y": "floating point or complex type",
        "Z": "input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "How is the norm calculated for complex inputs?",
        "Y": "the absolute value of each element",
        "Z": "input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will be the corresponding what?",
        "Y": "floating point type",
        "Z": "Returns the matrix norm or vector norm of a given tensor. Warning torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. dim(int,tuple of python:ints,list of python:ints,optional) \u2013 Specifies which dimension or dimensions ofinputto\ncalculate the norm across. IfdimisNone, the norm will\nbe calculated across all dimensions ofinput. If the norm\ntype indicated bypdoes not support the specified number of\ndimensions, an error will occur. keepdim(bool,optional) \u2013 whether the output tensors havedimretained or not. Ignored ifdim=Noneandout=None. Default:False out(Tensor,optional) \u2013 the output tensor. Ignored ifdim=Noneandout=None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "If input is complex and neitherdtypenoroutis specified, the result's data type will be the corresponding floating point type (e.",
        "Y": "ifinputis complexfloat",
        "Z": "input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is p(int,float,inf,-inf,'fro','nuc',optional)?",
        "Y": "the order of norm",
        "Z": "p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What are the following norms that can be calculated?",
        "Y": "ord matrix norm vector norm \u2019fro\u2019",
        "Z": "input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What does p(int,float,inf,-inf,'fro','nuc',optional',",
        "Y": "the order of norm",
        "Z": "p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the default order of norms?",
        "Y": "Default:'fro'",
        "Z": "the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "The corresponding dimensions of input are what?",
        "Y": "flattened",
        "Z": "matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the name of the norm that can be calculated?",
        "Y": "Frobenius norm",
        "Z": "p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "The corresponding dimensions of inputare what?",
        "Y": "flattened",
        "Z": "input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the name of the order in which a norm can be calculated?",
        "Y": "order of norm",
        "Z": "the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the vector norm 'fro'?",
        "Y": "matrix norm",
        "Z": "matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the order of norms?",
        "Y": "order of norm",
        "Z": "the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the ord matrix norm vector norm?",
        "Y": "fro",
        "Z": "ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "When does Frobenius norm throw an error?",
        "Y": "whendimis a list of three or more dims",
        "Z": "Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the vector norm fro?",
        "Y": "ord matrix norm",
        "Z": "ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What produces the same result asp=2 in all cases except whendimis a list of three or more dims?",
        "Y": "Frobenius norm",
        "Z": "Returns the matrix norm or vector norm of a given tensor. Warning torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. dim(int,tuple of python:ints,list of python:ints,optional) \u2013 Specifies which dimension or dimensions ofinputto\ncalculate the norm across. IfdimisNone, the norm will\nbe calculated across all dimensions ofinput. If the norm\ntype indicated bypdoes not support the specified number of\ndimensions, an error will occur. keepdim(bool,optional) \u2013 whether the output tensors havedimretained or not. Ignored ifdim=Noneandout=None. Default:False out(Tensor,optional) \u2013 the output tensor. Ignored ifdim=Noneandout=None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What can be calculated across any number of dimensions?",
        "Y": "vector norm",
        "Z": "The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What happens to the corresponding dimensions of input?",
        "Y": "flattened",
        "Z": "the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. dim(int,tuple of python:ints,list of python:ints,optional) \u2013 Specifies which dimension or dimensions ofinputto\ncalculate the norm across. IfdimisNone, the norm will\nbe calculated across all dimensions ofinput. If the norm\ntype indicated bypdoes not support the specified number of\ndimensions, an error will occur.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "Nuclear norm can only be calculated across exactly how many dimensions?",
        "Y": "two",
        "Z": "Returns the matrix norm or vector norm of a given tensor. Warning torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. dim(int,tuple of python:ints,list of python:ints,optional) \u2013 Specifies which dimension or dimensions ofinputto\ncalculate the norm across. IfdimisNone, the norm will\nbe calculated across all dimensions ofinput. If the norm\ntype indicated bypdoes not support the specified number of\ndimensions, an error will occur. keepdim(bool,optional) \u2013 whether the output tensors havedimretained or not. Ignored ifdim=Noneandout=None. Default:False out(Tensor,optional) \u2013 the output tensor. Ignored ifdim=Noneandout=None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "The vector norm can be calculated across what?",
        "Y": "any number of dimensions",
        "Z": "sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "Nuclear norm can only be calculated across what?",
        "Y": "exactly two dimensions",
        "Z": "input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the Frobenius norm?",
        "Y": "fro",
        "Z": "\u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What does nuclear norm stand for?",
        "Y": "Number",
        "Z": "\u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What case does Frobenius norm throw an error?",
        "Y": "whendimis a list of three or more dims",
        "Z": "\u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is nuclear norm?",
        "Y": "Number",
        "Z": "nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is Number \u2013 sum(abs(x)**ord)**(1./ord))?",
        "Y": "nuclear norm",
        "Z": "nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the value of the vector norm?",
        "Y": "Number",
        "Z": "\u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What can only be calculated across exactly two dimensions?",
        "Y": "Nuclear norm",
        "Z": "Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the vector norm calculated across any number of dimensions?",
        "Y": "sum(abs(x)**ord)**(1./ord)",
        "Z": "sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the sum of the vector norm?",
        "Y": "sum(abs(x)**ord)**(1./ord)",
        "Z": "\u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the output tensor ignored?",
        "Y": "ifdim=None",
        "Z": "out(Tensor,optional) \u2013 the output tensor. Ignored ifdim=Noneandout=None. dtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the input tensor casted to when performing the operation?",
        "Y": ":attr:\u2019dtype\u2019",
        "Z": "out(Tensor,optional) \u2013 the output tensor. Ignored ifdim=Noneandout=None. dtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "If specified, the input tensor is casted to what?",
        "Y": ":attr:\u2019dtype\u2019",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note Even thoughp='fro'supports any number of dimensions, the true\nmathematical definition of Frobenius norm only applies to tensors with\nexactly two dimensions.torch.linalg.norm()withord='fro'aligns\nwith the mathematical definition, since it can only be applied across\nexactly two dimensions. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What applies only to tensors with exactly two dimensions?",
        "Y": "Frobenius norm",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note Even thoughp='fro'supports any number of dimensions, the true\nmathematical definition of Frobenius norm only applies to tensors with\nexactly two dimensions.torch.linalg.norm()withord='fro'aligns\nwith the mathematical definition, since it can only be applied across\nexactly two dimensions. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is an example of a tensor that can only be applied across exactly two dimensions?",
        "Y": "Example",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of\nreturned tensor. If specified, the input tensor is casted to\n:attr:\u2019dtype\u2019 while performing the operation. Default: None. Note Even thoughp='fro'supports any number of dimensions, the true\nmathematical definition of Frobenius norm only applies to tensors with\nexactly two dimensions.torch.linalg.norm()withord='fro'aligns\nwith the mathematical definition, since it can only be applied across\nexactly two dimensions. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What does Alias fortorch.atanh do?",
        "Y": "Alias fortorch.atanh()",
        "Z": "Alias fortorch.atanh().",
        "source": "https://pytorch.org/docs/stable/generated/torch.arctanh.html#torch.arctanh"
    },
    {
        "X": "What is the name of the website that provides information about Alias fortorch.atanh?",
        "Y": "Alias fortorch.atanh",
        "Z": "Alias fortorch.atanh().",
        "source": "https://pytorch.org/docs/stable/generated/torch.arctanh.html#torch.arctanh"
    },
    {
        "X": "What happens if an element ininputevaluates toTrue?",
        "Y": "Tests",
        "Z": "Tests if any element ininputevaluates toTrue. Note This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "This function matches the behavior of what function?",
        "Y": "NumPy",
        "Z": "Tests if any element ininputevaluates toTrue. Note This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What is the dtype of output foruint8?",
        "Y": "Foruint8the dtype of output isuint8itself",
        "Z": "input(Tensor) \u2013 the input tensor. Tests if any element ininputevaluates toTrue. Note This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What is returned for each row of inputin the given dimensiondim?",
        "Y": "returnsTrueif any element in the row evaluate toTrueandFalseotherwise",
        "Z": "input(Tensor) \u2013 the input tensor. Tests if any element ininputevaluates toTrue. Note This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What does input(Tensor) do?",
        "Y": "Tests if any element ininputevaluates toTrue",
        "Z": "input(Tensor) \u2013 the input tensor. Tests if any element ininputevaluates toTrue. Note This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "This function matches the behaviour of what function in returning output of dtypebool for all supported dtypes exceptuint8?",
        "Y": "NumPy",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimensiondimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension thaninput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What is the dtype of output?",
        "Y": "isuint8itself",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimensiondimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension thaninput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What does the function return if any element in the row evaluate toTrueandFalseotherwise?",
        "Y": "returnsTrue",
        "Z": "input(Tensor) \u2013 the input tensor. Tests if any element ininputevaluates toTrue. Note This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What is the dtype of foruint8?",
        "Y": "output isuint8itself",
        "Z": "Note This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What does the function return for each row ofinputin the given dimensiondim?",
        "Y": "Trueif any element in the row evaluate toTrueandFalseotherwise",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimensiondimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension thaninput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What function returns output of dtypebool for all supported dtypes exceptuint8?",
        "Y": "NumPy",
        "Z": "Note This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What does the function return for each row of inputin the given dimensiondim?",
        "Y": "Trueif any element in the row evaluate toTrueandFalseotherwise",
        "Z": "Note This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "For each row of inputin the given dimensiondim, returns what?",
        "Y": "Trueif any element in the row evaluate toTrueandFalseotherwise",
        "Z": "Note This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "Which function returns output of dtypebool for all supported dtypes exceptuint8?",
        "Y": "NumPy",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimensiondimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension thaninput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What does this function match the behaviour of NumPy in returning output of dtypeboolfor all supported dtypes exceptuin",
        "Y": "Foruint8the dtype of output isuint8itself",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimensiondimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension thaninput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What function returns the output tensor of the same size asinput?",
        "Y": "IfkeepdimisTrue",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimensiondimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension thaninput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "If keepdimisTrue, the output tensor has what?",
        "Y": "1 fewer dimension thaninput",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimensiondimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension thaninput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "For each row ofinputin the given dimensiondim, returns what?",
        "Y": "Trueif any element in the row evaluate toTrueandFalseotherwise",
        "Z": "For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimensiondimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension thaninput. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What function ensures that the output tensor is of the same size as input except in the dimensiondim where it is of size 1?",
        "Y": "IfkeepdimisTrue",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtypeboolfor all supported dtypes exceptuint8.\nForuint8the dtype of output isuint8itself. Example: For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimensiondimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension thaninput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "Where is the output tensor of the same size asinput except in the dimensiondim?",
        "Y": "size 1",
        "Z": "For each row ofinputin the given dimensiondim,\nreturnsTrueif any element in the row evaluate toTrueandFalseotherwise. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimensiondimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension thaninput. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "What are features sometimes hidden behind?",
        "Y": "run-time flags",
        "Z": "Prototype:These features are typically not available as part of\nbinary distributions like PyPI or Conda, except sometimes behind run-time\nflags, and are at an early stage for feedback and testing. Thetorchaudiopackage consists of I/O, popular datasets and common audio transformations. Package Reference PyTorch Libraries",
        "source": "https://pytorch.org/audio/stable/index.html"
    },
    {
        "X": "Thetorchaudiopackage consists of I/O, common audio transformations, and what?",
        "Y": "popular datasets",
        "Z": "This library is part of thePyTorchproject. PyTorch is an open source\nmachine learning framework. Features described in this documentation are classified by release status: Stable:These features will be maintained long-term and there should generally\nbe no major performance limitations or gaps in documentation.\nWe also expect to maintain backwards compatibility (although\nbreaking changes can happen and notice will be given one release ahead\nof time). Beta:Features are tagged as Beta because the API may change based on\nuser feedback, because the performance needs to improve, or because\ncoverage across operators is not yet complete. For Beta features, we are\ncommitting to seeing the feature through to the Stable classification.\nWe are not, however, committing to backwards compatibility. Prototype:These features are typically not available as part of\nbinary distributions like PyPI or Conda, except sometimes behind run-time\nflags, and are at an early stage for feedback and testing. Thetorchaudiopackage consists of I/O, popular datasets and common audio transformations. Package Reference PyTorch Libraries",
        "source": "https://pytorch.org/audio/stable/index.html"
    },
    {
        "X": "What does Thetorchaudiopackage consist of?",
        "Y": "Package Reference PyTorch Libraries",
        "Z": "Prototype:These features are typically not available as part of\nbinary distributions like PyPI or Conda, except sometimes behind run-time\nflags, and are at an early stage for feedback and testing. Thetorchaudiopackage consists of I/O, popular datasets and common audio transformations. Package Reference PyTorch Libraries",
        "source": "https://pytorch.org/audio/stable/index.html"
    },
    {
        "X": "What consists of I/O, popular datasets and common audio transformations?",
        "Y": "Thetorchaudiopackage",
        "Z": "This library is part of thePyTorchproject. PyTorch is an open source\nmachine learning framework. Features described in this documentation are classified by release status: Stable:These features will be maintained long-term and there should generally\nbe no major performance limitations or gaps in documentation.\nWe also expect to maintain backwards compatibility (although\nbreaking changes can happen and notice will be given one release ahead\nof time). Beta:Features are tagged as Beta because the API may change based on\nuser feedback, because the performance needs to improve, or because\ncoverage across operators is not yet complete. For Beta features, we are\ncommitting to seeing the feature through to the Stable classification.\nWe are not, however, committing to backwards compatibility. Prototype:These features are typically not available as part of\nbinary distributions like PyPI or Conda, except sometimes behind run-time\nflags, and are at an early stage for feedback and testing. Thetorchaudiopackage consists of I/O, popular datasets and common audio transformations. Package Reference PyTorch Libraries",
        "source": "https://pytorch.org/audio/stable/index.html"
    },
    {
        "X": "What libraries are included in Thetorchaudiopackage?",
        "Y": "Package Reference PyTorch Libraries",
        "Z": "Features described in this documentation are classified by release status: Stable:These features will be maintained long-term and there should generally\nbe no major performance limitations or gaps in documentation.\nWe also expect to maintain backwards compatibility (although\nbreaking changes can happen and notice will be given one release ahead\nof time). Beta:Features are tagged as Beta because the API may change based on\nuser feedback, because the performance needs to improve, or because\ncoverage across operators is not yet complete. For Beta features, we are\ncommitting to seeing the feature through to the Stable classification.\nWe are not, however, committing to backwards compatibility. Prototype:These features are typically not available as part of\nbinary distributions like PyPI or Conda, except sometimes behind run-time\nflags, and are at an early stage for feedback and testing. Thetorchaudiopackage consists of I/O, popular datasets and common audio transformations. Package Reference PyTorch Libraries",
        "source": "https://pytorch.org/audio/stable/index.html"
    },
    {
        "X": "What types of types does this function support?",
        "Y": "float,double,cfloatandcdoubledtypes forinput",
        "Z": "Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted\nLU factorization of A fromtorch.lu(). This function supportsfloat,double,cfloatandcdoubledtypes forinput. b(Tensor) \u2013 the RHS tensor of size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. LU_data(Tensor) \u2013 the pivoted LU factorization of A fromtorch.lu()of size(\u2217,m,m)(*, m, m)(\u2217,m,m),\nwhere\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "What is the RHS tensor of size(,m,k)(*, m, k)(,m",
        "Y": "b(Tensor)",
        "Z": "b(Tensor) \u2013 the RHS tensor of size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. LU_data(Tensor) \u2013 the pivoted LU factorization of A fromtorch.lu()of size(\u2217,m,m)(*, m, m)(\u2217,m,m),\nwhere\u2217*\u2217is zero or more batch dimensions. LU_pivots(IntTensor) \u2013 the pivots of the LU factorization fromtorch.lu()of size(\u2217,m)(*, m)(\u2217,m),\nwhere\u2217*\u2217is zero or more batch dimensions.\nThe batch dimensions ofLU_pivotsmust be equal to the batch dimensions ofLU_data. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "What is the pivoted LU factorization of A fromtorch.lu()?",
        "Y": "LU_data(Tensor)",
        "Z": "Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted\nLU factorization of A fromtorch.lu(). This function supportsfloat,double,cfloatandcdoubledtypes forinput. b(Tensor) \u2013 the RHS tensor of size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. LU_data(Tensor) \u2013 the pivoted LU factorization of A fromtorch.lu()of size(\u2217,m,m)(*, m, m)(\u2217,m,m),\nwhere\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "This function supports float,double, and what other type of input?",
        "Y": "cfloat",
        "Z": "Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted\nLU factorization of A fromtorch.lu(). This function supportsfloat,double,cfloatandcdoubledtypes forinput. b(Tensor) \u2013 the RHS tensor of size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. LU_data(Tensor) \u2013 the pivoted LU factorization of A fromtorch.lu()of size(\u2217,m,m)(*, m, m)(\u2217,m,m),\nwhere\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "What types of input does this function support?",
        "Y": "float,double,cfloatandcdoubledtypes",
        "Z": "This function supportsfloat,double,cfloatandcdoubledtypes forinput. b(Tensor) \u2013 the RHS tensor of size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. LU_data(Tensor) \u2013 the pivoted LU factorization of A fromtorch.lu()of size(\u2217,m,m)(*, m, m)(\u2217,m,m),\nwhere\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "What does LU_data(Tensor) do?",
        "Y": "LU factorization",
        "Z": "This function supportsfloat,double,cfloatandcdoubledtypes forinput. b(Tensor) \u2013 the RHS tensor of size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. LU_data(Tensor) \u2013 the pivoted LU factorization of A fromtorch.lu()of size(\u2217,m,m)(*, m, m)(\u2217,m,m),\nwhere\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "What type of input does this function support?",
        "Y": "forinput",
        "Z": "This function supportsfloat,double,cfloatandcdoubledtypes forinput. b(Tensor) \u2013 the RHS tensor of size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. LU_data(Tensor) \u2013 the pivoted LU factorization of A fromtorch.lu()of size(\u2217,m,m)(*, m, m)(\u2217,m,m),\nwhere\u2217*\u2217is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "What is the pivoted LU factorization of A fromtorch.lu()of size(,m,m)(*,",
        "Y": "LU_data",
        "Z": "b(Tensor) \u2013 the RHS tensor of size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. LU_data(Tensor) \u2013 the pivoted LU factorization of A fromtorch.lu()of size(\u2217,m,m)(*, m, m)(\u2217,m,m),\nwhere\u2217*\u2217is zero or more batch dimensions. LU_pivots(IntTensor) \u2013 the pivots of the LU factorization fromtorch.lu()of size(\u2217,m)(*, m)(\u2217,m),\nwhere\u2217*\u2217is zero or more batch dimensions.\nThe batch dimensions ofLU_pivotsmust be equal to the batch dimensions ofLU_data. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "What are the pivots of the LU factorization fromtorch.lu()of size(,m)(*, m",
        "Y": "LU_pivots",
        "Z": "b(Tensor) \u2013 the RHS tensor of size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. LU_data(Tensor) \u2013 the pivoted LU factorization of A fromtorch.lu()of size(\u2217,m,m)(*, m, m)(\u2217,m,m),\nwhere\u2217*\u2217is zero or more batch dimensions. LU_pivots(IntTensor) \u2013 the pivots of the LU factorization fromtorch.lu()of size(\u2217,m)(*, m)(\u2217,m),\nwhere\u2217*\u2217is zero or more batch dimensions.\nThe batch dimensions ofLU_pivotsmust be equal to the batch dimensions ofLU_data. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "What must the batch dimensions ofLU_pivots be equal to?",
        "Y": "the batch dimensions ofLU_data",
        "Z": "b(Tensor) \u2013 the RHS tensor of size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. LU_data(Tensor) \u2013 the pivoted LU factorization of A fromtorch.lu()of size(\u2217,m,m)(*, m, m)(\u2217,m,m),\nwhere\u2217*\u2217is zero or more batch dimensions. LU_pivots(IntTensor) \u2013 the pivots of the LU factorization fromtorch.lu()of size(\u2217,m)(*, m)(\u2217,m),\nwhere\u2217*\u2217is zero or more batch dimensions.\nThe batch dimensions ofLU_pivotsmust be equal to the batch dimensions ofLU_data. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "What must be the batch dimensions of LU_pivots be to the batch dimensions of LU_data?",
        "Y": "equal",
        "Z": "b(Tensor) \u2013 the RHS tensor of size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. LU_data(Tensor) \u2013 the pivoted LU factorization of A fromtorch.lu()of size(\u2217,m,m)(*, m, m)(\u2217,m,m),\nwhere\u2217*\u2217is zero or more batch dimensions. LU_pivots(IntTensor) \u2013 the pivots of the LU factorization fromtorch.lu()of size(\u2217,m)(*, m)(\u2217,m),\nwhere\u2217*\u2217is zero or more batch dimensions.\nThe batch dimensions ofLU_pivotsmust be equal to the batch dimensions ofLU_data. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "What does the documentation of bytorch.sort() provide?",
        "Y": "exact semantics",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending\norder by value. This is the second value returned bytorch.sort().  See its documentation\nfor the exact semantics of this method. input(Tensor) \u2013 the input tensor. dim(int,optional) \u2013 the dimension to sort along descending(bool,optional) \u2013 controls the sorting order (ascending or descending) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort"
    },
    {
        "X": "What is the dimension to sort along descending?",
        "Y": "dim(int,optional)",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending\norder by value. This is the second value returned bytorch.sort().  See its documentation\nfor the exact semantics of this method. input(Tensor) \u2013 the input tensor. dim(int,optional) \u2013 the dimension to sort along descending(bool,optional) \u2013 controls the sorting order (ascending or descending) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort"
    },
    {
        "X": "What is returned bytorch.sort()?",
        "Y": "second value",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending\norder by value. This is the second value returned bytorch.sort().  See its documentation\nfor the exact semantics of this method. input(Tensor) \u2013 the input tensor. dim(int,optional) \u2013 the dimension to sort along descending(bool,optional) \u2013 controls the sorting order (ascending or descending) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort"
    },
    {
        "X": "What do you need to know about the second value returned bytorch.sort()?",
        "Y": "semantics",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending\norder by value. This is the second value returned bytorch.sort().  See its documentation\nfor the exact semantics of this method. input(Tensor) \u2013 the input tensor. dim(int,optional) \u2013 the dimension to sort along descending(bool,optional) \u2013 controls the sorting order (ascending or descending) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort"
    },
    {
        "X": "What is the dimension to sort along descending(bool,optional)?",
        "Y": "dim(int,optional)",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending\norder by value. This is the second value returned bytorch.sort().  See its documentation\nfor the exact semantics of this method. input(Tensor) \u2013 the input tensor. dim(int,optional) \u2013 the dimension to sort along descending(bool,optional) \u2013 controls the sorting order (ascending or descending) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort"
    },
    {
        "X": "What returns the cumulative maximum of elements of input in the dimensiondim?",
        "Y": "a namedtuple",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of\nelements ofinputin the dimensiondim. Andindicesis the index\nlocation of each maximum value found in the dimensiondim. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax"
    },
    {
        "X": "What is the index location of each maximum value found in the dimensiondim?",
        "Y": "Andindices",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of\nelements ofinputin the dimensiondim. Andindicesis the index\nlocation of each maximum value found in the dimensiondim. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin"
    },
    {
        "X": "What is the dimension to do the operation over out(tuple,optional)?",
        "Y": "dim(int)",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of\nelements ofinputin the dimensiondim. Andindicesis the index\nlocation of each maximum value found in the dimensiondim. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin"
    },
    {
        "X": "What is the output specified by for a 3-D tensor?",
        "Y": "inputandindexmust have the same number of dimensions",
        "Z": "Gathers values along an axis specified bydim. For a 3-D tensor the output is specified by: inputandindexmust have the same number of dimensions.\nIt is also required thatindex.size(d)<=input.size(d)for all\ndimensionsd!=dim.outwill have the same shape asindex.\nNote thatinputandindexdo not broadcast against each other. input(Tensor) \u2013 the source tensor dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to gather sparse_grad(bool,optional) \u2013 IfTrue, gradient w.r.t.inputwill be a sparse tensor. out(Tensor,optional) \u2013 the destination tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather"
    },
    {
        "X": "Do inputandindexdo broadcast against each other?",
        "Y": "not broadcast against each other",
        "Z": "Gathers values along an axis specified bydim. For a 3-D tensor the output is specified by: inputandindexmust have the same number of dimensions.\nIt is also required thatindex.size(d)<=input.size(d)for all\ndimensionsd!=dim.outwill have the same shape asindex.\nNote thatinputandindexdo not broadcast against each other. input(Tensor) \u2013 the source tensor dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to gather sparse_grad(bool,optional) \u2013 IfTrue, gradient w.r.t.inputwill be a sparse tensor. out(Tensor,optional) \u2013 the destination tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather"
    },
    {
        "X": "What does index(LongTensor) contain?",
        "Y": "indices of elements to gather sparse_grad(bool,optional)",
        "Z": "Gathers values along an axis specified bydim. For a 3-D tensor the output is specified by: inputandindexmust have the same number of dimensions.\nIt is also required thatindex.size(d)<=input.size(d)for all\ndimensionsd!=dim.outwill have the same shape asindex.\nNote thatinputandindexdo not broadcast against each other. input(Tensor) \u2013 the source tensor dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to gather sparse_grad(bool,optional) \u2013 IfTrue, gradient w.r.t.inputwill be a sparse tensor. out(Tensor,optional) \u2013 the destination tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather"
    },
    {
        "X": "What must have the same number of dimensions?",
        "Y": "inputandindex",
        "Z": "inputandindexmust have the same number of dimensions.\nIt is also required thatindex.size(d)<=input.size(d)for all\ndimensionsd!=dim.outwill have the same shape asindex.\nNote thatinputandindexdo not broadcast against each other. input(Tensor) \u2013 the source tensor dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to gather sparse_grad(bool,optional) \u2013 IfTrue, gradient w.r.t.inputwill be a sparse tensor. out(Tensor,optional) \u2013 the destination tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather"
    },
    {
        "X": "What is the destination tensor?",
        "Y": "out",
        "Z": "Gathers values along an axis specified bydim. For a 3-D tensor the output is specified by: inputandindexmust have the same number of dimensions.\nIt is also required thatindex.size(d)<=input.size(d)for all\ndimensionsd!=dim.outwill have the same shape asindex.\nNote thatinputandindexdo not broadcast against each other. input(Tensor) \u2013 the source tensor dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to gather sparse_grad(bool,optional) \u2013 IfTrue, gradient w.r.t.inputwill be a sparse tensor. out(Tensor,optional) \u2013 the destination tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather"
    },
    {
        "X": "What type of elements represent if each element of input is real-valued or not?",
        "Y": "boolean elements",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.\nAll real-valued types are considered real. Complex values are considered real when their imaginary part is 0. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis real and False elsewhere Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal"
    },
    {
        "X": "What is the default value of a boolean tensor?",
        "Y": "True",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.\nAll real-valued types are considered real. Complex values are considered real when their imaginary part is 0. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis real and False elsewhere Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal"
    },
    {
        "X": "What type of values are considered real?",
        "Y": "real",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.\nAll real-valued types are considered real. Complex values are considered real when their imaginary part is 0. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis real and False elsewhere Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal"
    },
    {
        "X": "When are complex values considered real?",
        "Y": "when their imaginary part is 0. input(Tensor) \u2013 the input tensor",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.\nAll real-valued types are considered real. Complex values are considered real when their imaginary part is 0. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis real and False elsewhere Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal"
    },
    {
        "X": "What is a boolean tensor that is whereinputis real and False elsewhere?",
        "Y": "True",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.\nAll real-valued types are considered real. Complex values are considered real when their imaginary part is 0. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis real and False elsewhere Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal"
    },
    {
        "X": "What is nonlinearity gain?",
        "Y": "nonlinearity gain Linear / Identity",
        "Z": "nonlinearity gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What gain Linear / Identity 111 Conv1,2,3D 111 Tanh 53frac5335 Re",
        "Y": "nonlinearity",
        "Z": "nonlinearity gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the name of the 111 Sigmoid?",
        "Y": "111 Tanh",
        "Z": "gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the definition of Identity 111 Conv1,2,3D 111 Tanh 53frac5335 ReLU",
        "Y": "Linear",
        "Z": "Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the name of the 111 Tanh?",
        "Y": "111 Sigmoid",
        "Z": "111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is 111 Tanh?",
        "Y": "111 Tanh",
        "Z": "111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the number of the Leaky Relu 2sqrt22 Leaky Relu?",
        "Y": "111 Tanh",
        "Z": "111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How many Tanhs are there?",
        "Y": "111",
        "Z": "111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What should you use in order to implementSelf-Normalizing Neural Networks?",
        "Y": "'linear'",
        "Z": "34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does this give the initial weights?",
        "Y": "variance of1/N",
        "Z": "Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does the default gain forSELUsacrifice?",
        "Y": "normalisation effect",
        "Z": "Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the name of the 2sqrt22?",
        "Y": "Leaky Relu",
        "Z": "2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does the default gain forSELU sacrifice for more stable gradient flow in rectangular layers?",
        "Y": "normalisation effect",
        "Z": "21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does the default gain forSELUsacrifice for more stable gradient flow in rectangular layers?",
        "Y": "the default gain forSELUsacrifices the normalisation effect",
        "Z": "Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2).",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is 21+negative_slope2sqrtfrac21 + textnegative_slope",
        "Y": "Leaky Relu",
        "Z": "Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the name of the relu that is 21+negative_slope2sqrtfrac21 +",
        "Y": "Leaky Relu",
        "Z": "Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the default gain forSELUsacrifices the normalisation effect for more stable gradient flow in rectangular layers?",
        "Y": "linear",
        "Z": "Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the initial weights' variance?",
        "Y": "variance of1/N",
        "Z": "In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the optional parameter for the non-linear function?",
        "Y": "param",
        "Z": "param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the optional parameter for the non-linear function Examples?",
        "Y": "nonlinearity",
        "Z": "In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the default gain for forSELUsacrifices the normalisation effect for more stable gradient flow in rectangular layers?",
        "Y": "selu",
        "Z": "Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the variance of the initial weights?",
        "Y": "variance of1/N",
        "Z": "Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the non-linear function param?",
        "Y": "optional parameter",
        "Z": "Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "In order to implementSelf-Normalizing Neural Networks, you should use what?",
        "Y": "linear",
        "Z": "In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the non-linear function?",
        "Y": "nonlinearity",
        "Z": "nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is an n-dimensionaltorch?",
        "Y": "tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor",
        "Z": "val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is param for the non-linear function?",
        "Y": "optional parameter",
        "Z": "param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Param is what type of parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform distribution?",
        "Y": "optional",
        "Z": "param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What type of torch is a tensor?",
        "Y": "2-dimensional",
        "Z": "Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the input Tensor filled with values drawn from?",
        "Y": "the uniform distribution",
        "Z": "Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the uniform distributionU?",
        "Y": "a,b",
        "Z": "Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the mean of the normal distribution std?",
        "Y": "standard deviation of the normal distribution",
        "Z": "param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Fills the input Tensor with values drawn from what?",
        "Y": "the uniform distribution",
        "Z": "Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does a tensor represent?",
        "Y": "a\u2013 the lower bound of the uniform distribution",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does std stand for?",
        "Y": "the standard deviation of the normal distribution",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the upper bound of the uniform distribution?",
        "Y": "b",
        "Z": "b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the value to fill the tensor with?",
        "Y": "val",
        "Z": "val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the value to fill the tensor with Examples?",
        "Y": "an n-dimensionaltorch.Tensor val",
        "Z": "b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "b\u2013 what is the upper bound of the uniform distribution?",
        "Y": "the upper bound",
        "Z": "b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What do examples fill the input Tensor with values drawn from?",
        "Y": "the normal distribution",
        "Z": "Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the standard deviation of the normal distribution?",
        "Y": "std",
        "Z": "std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Fills the input Tensor with values drawn from what distribution?",
        "Y": "normal distribution",
        "Z": "Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Fills the input Tensor with what value1?",
        "Y": "scalar",
        "Z": "Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What value does a tensor fill the input Tensor with?",
        "Y": "scalar value0",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the mean of the normal distribution?",
        "Y": "mean",
        "Z": "mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What value does the input Tensor fill with?",
        "Y": "scalar",
        "Z": "std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does a tensor do when as many inputs are preserved as possible?",
        "Y": "Preserves the identity of the inputs inLinearlayers",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What value does Tensor val fill the input Tensor with?",
        "Y": "scalar value",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Fills the 2-dimensional inputTensorwith what?",
        "Y": "identity matrix",
        "Z": "Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does a tensor do inLinearlayers?",
        "Y": "Preserves the identity of the inputs",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the function that preserves the identity of the inputs inLinearlayers?",
        "Y": "Preserves the identity of the inputs inLinearlayers",
        "Z": "Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does Linearlayers do?",
        "Y": "Preserves the identity of the inputs",
        "Z": "val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What is the input Tensor filled with?",
        "Y": "scalar value0",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does a tensor do that allows as many inputs to be preserved as possible?",
        "Y": "Preserves the identity of the inputs inLinearlayers",
        "Z": "Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does Examples Fill the input Tensor with?",
        "Y": "scalar value0",
        "Z": "Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does filling the 2-dimensional inputTensor do?",
        "Y": "Preserves the identity of the inputs",
        "Z": "Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "Fills the input Tensor with what value?",
        "Y": "scalar value1",
        "Z": "Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does filling the 2-dimensional inputTensor with the identity matrix do?",
        "Y": "Preserves the identity of the inputs",
        "Z": "Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does tensor fill the 2-dimensional inputTensor with?",
        "Y": "the identity matrix",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does tensor do?",
        "Y": "Fills the input Tensor with the scalar value0",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a normal\ndistribution. The resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where Also known as Glorot initialization. tensor\u2013 an n-dimensionaltorch.Tensor gain\u2013 an optional scaling factor Examples Fills the inputTensorwith values according to the method\ndescribed inDelving deep into rectifiers: Surpassing human-level\nperformance on ImageNet classification- He, K. et al. (2015), using a\nuniform distribution. The resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where Also known as He initialization. tensor\u2013 an n-dimensionaltorch.Tensor",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does tensor do inLinearlayers?",
        "Y": "Preserves the identity of the inputs",
        "Z": "tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does Examples fill the input Tensor with?",
        "Y": "scalar value0",
        "Z": "Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What does Filling the 2-dimensional inputTensor with the identity matrix do?",
        "Y": "Preserves the identity of the inputs",
        "Z": "Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What dimension is a tensor?",
        "Y": "n-dimensional",
        "Z": "Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples Fills the {3, 4, 5}-dimensional inputTensorwith the Dirac\ndelta function. Preserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity tensor\u2013 a {3, 4, 5}-dimensionaltorch.Tensor groups(optional) \u2013 number of groups in the conv layer (default: 1) Examples Fills the inputTensorwith values according to the method\ndescribed inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010), using a uniform\ndistribution. The resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where Also known as Glorot initialization.",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "InLinearlayers, where as many inputs are preserved, what does Fills the 2-dimensional inputTensorwith the identity matrix?",
        "Y": "Preserves the identity of the inputs",
        "Z": "Fills the input Tensor with values drawn from the normal\ndistributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2). tensor\u2013 an n-dimensionaltorch.Tensor mean\u2013 the mean of the normal distribution std\u2013 the standard deviation of the normal distribution Examples Fills the input Tensor with the valueval\\text{val}val. tensor\u2013 an n-dimensionaltorch.Tensor val\u2013 the value to fill the tensor with Examples Fills the input Tensor with the scalar value1. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the input Tensor with the scalar value0. tensor\u2013 an n-dimensionaltorch.Tensor Examples Fills the 2-dimensional inputTensorwith the identity\nmatrix. Preserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible. tensor\u2013 a 2-dimensionaltorch.Tensor Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "What reduces the amount of matrix multiplications in a batch matrix-matrix product?",
        "Y": "add step",
        "Z": "Performs a batch matrix-matrix product of matrices stored\ninbatch1andbatch2,\nwith a reduced add step (all matrix multiplications get accumulated\nalong the first dimension).inputis added to the final result. batch1andbatch2must be 3-D tensors each containing the\nsame number of matrices. Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "What must batch1andbatch2 be?",
        "Y": "3-D tensors",
        "Z": "Performs a batch matrix-matrix product of matrices stored\ninbatch1andbatch2,\nwith a reduced add step (all matrix multiplications get accumulated\nalong the first dimension).inputis added to the final result. batch1andbatch2must be 3-D tensors each containing the\nsame number of matrices. Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin\nit will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. batch1(Tensor) \u2013 the first batch of matrices to be multiplied batch2(Tensor) \u2013 the second batch of matrices to be multiplied beta(Number,optional) \u2013 multiplier forinput(\u03b2\\beta\u03b2) input(Tensor) \u2013 matrix to be added alpha(Number,optional) \u2013 multiplier forbatch1 @ batch2(\u03b1\\alpha\u03b1)",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "What happens in a reduced add step?",
        "Y": "all matrix multiplications get accumulated along the first dimension",
        "Z": "Performs a batch matrix-matrix product of matrices stored\ninbatch1andbatch2,\nwith a reduced add step (all matrix multiplications get accumulated\nalong the first dimension).inputis added to the final result. batch1andbatch2must be 3-D tensors each containing the\nsame number of matrices. Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin\nit will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. batch1(Tensor) \u2013 the first batch of matrices to be multiplied batch2(Tensor) \u2013 the second batch of matrices to be multiplied beta(Number,optional) \u2013 multiplier forinput(\u03b2\\beta\u03b2) input(Tensor) \u2013 matrix to be added alpha(Number,optional) \u2013 multiplier forbatch1 @ batch2(\u03b1\\alpha\u03b1) out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "Batch1andbatch2must be what?",
        "Y": "3-D tensors",
        "Z": "Performs a batch matrix-matrix product of matrices stored\ninbatch1andbatch2,\nwith a reduced add step (all matrix multiplications get accumulated\nalong the first dimension).inputis added to the final result. batch1andbatch2must be 3-D tensors each containing the\nsame number of matrices. Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "What is a(bnm)(b times n times m)(bn",
        "Y": "Ifbatch1",
        "Z": "Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin\nit will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32.",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "If input is ignored, andnanandinfin it will not be propagated, what is it?",
        "Y": "Ifbetais 0,",
        "Z": "batch1andbatch2must be 3-D tensors each containing the\nsame number of matrices. Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin\nit will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers.",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "Argumentsbetaandalphamust be real numbers, otherwise they should be what?",
        "Y": "integers",
        "Z": "batch1andbatch2must be 3-D tensors each containing the\nsame number of matrices. Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin\nit will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers.",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "Which inputs will not be propagated ifbetais 0?",
        "Y": "andnanandinfin",
        "Z": "Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin\nit will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32.",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "For inputs of typeFloatTensororDoubleTensor, what must be?",
        "Y": "argumentsbetaandalphamust be real numbers",
        "Z": "Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin\nit will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32.",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "If input is ignored, andnanandinfin it will not be propagated?",
        "Y": "Ifbetais 0,",
        "Z": "Performs a batch matrix-matrix product of matrices stored\ninbatch1andbatch2,\nwith a reduced add step (all matrix multiplications get accumulated\nalong the first dimension).inputis added to the final result. batch1andbatch2must be 3-D tensors each containing the\nsame number of matrices. Ifbatch1is a(b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)tensor,batch2is a(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)tensor,inputmust bebroadcastablewith a(n\u00d7p)(n \\times p)(n\u00d7p)tensor\nandoutwill be a(n\u00d7p)(n \\times p)(n\u00d7p)tensor. Ifbetais 0, theninputwill be ignored, andnanandinfin\nit will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. batch1(Tensor) \u2013 the first batch of matrices to be multiplied batch2(Tensor) \u2013 the second batch of matrices to be multiplied beta(Number,optional) \u2013 multiplier forinput(\u03b2\\beta\u03b2) input(Tensor) \u2013 matrix to be added alpha(Number,optional) \u2013 multiplier forbatch1 @ batch2(\u03b1\\alpha\u03b1)",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "What must argumentsbeta andalpha be for inputs of typeFloatTensororDoubleTensor?",
        "Y": "argumentsbetaandalphamust be real numbers",
        "Z": "Ifbetais 0, theninputwill be ignored, andnanandinfin\nit will not be propagated. For inputs of typeFloatTensororDoubleTensor, argumentsbetaandalphamust be real numbers, otherwise they should be integers. This operator supportsTensorFloat32. batch1(Tensor) \u2013 the first batch of matrices to be multiplied batch2(Tensor) \u2013 the second batch of matrices to be multiplied beta(Number,optional) \u2013 multiplier forinput(\u03b2\\beta\u03b2) input(Tensor) \u2013 matrix to be added",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "What is the name of the Moore-Penrose inverse?",
        "Y": "pseudoinverse",
        "Z": "Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is the most computationally convenient way to understand the pseudoinverse?",
        "Y": "SVD",
        "Z": "Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does the SVD support?",
        "Y": "batches of matrices",
        "Z": "The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does the Moon-Penrose inverse stand for?",
        "Y": "the pseudoinverse",
        "Z": "Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "The pseudoinverse is more computationally convenient to understand through what?",
        "Y": "SVD",
        "Z": "Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What supports input of float, double, cfloat, and cdouble dtypes?",
        "Y": "SVD",
        "Z": "The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "Ifhermitian= what is assumed to be Hermitian if complex or symmetric if real?",
        "Y": "True",
        "Z": "The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What part of the matrix is used in computations?",
        "Y": "lower triangular part of the matrix",
        "Z": "Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is assumed to be Hermitian if complex or symmetric if real?",
        "Y": "Ifhermitian= True",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "Supports input of what types of input?",
        "Y": "float, double, cfloat and cdouble dtypes",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What type of matrices does Ais support?",
        "Y": "batches of matrices",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "Ifhermitian= what, is Ais assumed to be Hermitian if complex or symmetric if real?",
        "Y": "True",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What part of the matrix is used instead of Hermitian?",
        "Y": "lower triangular part of the matrix",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "The singular values below the specifiedrcondthreshold are treated as what?",
        "Y": "zero",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What should be done to the singular values that are below the specifiedrcondthreshold?",
        "Y": "Note",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is assumed to be if complex or symmetric if real?",
        "Y": "Hermitian",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What values that are below the specifiedrcondthreshold are treated as zero and discarded in the computation?",
        "Y": "The singular values",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is the difference between the singular values and the norm of the eigenvalues whenhermitian= True?",
        "Y": "Note",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What are the singular values that are below the specifiedrcondthreshold treated as?",
        "Y": "zero",
        "Z": "The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does the function usestorch.linalg.svd() if?",
        "Y": "True",
        "Z": "The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does this function synchronize with the CPU for?",
        "Y": "CUDA inputs",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is a matrix on the left multiplied by?",
        "Y": "the pseudoinverse",
        "Z": "The pseudoinverse may bedefined algebraicallybut it is more computationally convenient to understand itthrough the SVD Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What function synchronizes a CUDA input with the CPU?",
        "Y": "usestorch.linalg.svd()ifhermitian= False",
        "Z": "The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does this function synchronize with the CPU?",
        "Y": "CUDA inputs",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices then\nthe output has the same batch dimensions. Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What function is used for multiplying a matrix on the left by the pseudoinverse?",
        "Y": "usingtorch.linalg.lstsq()if possible",
        "Z": "The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does the function usestorch.linalg.svd() ifhermitian= False?",
        "Y": "True",
        "Z": "Ifhermitian= True,Ais assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations. The singular values (or the norm of the eigenvalues whenhermitian= True)\nthat are below the specifiedrcondthreshold are treated as zero and discarded in the computation. Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "Why is it always preferable to uselstsq()?",
        "Y": "faster and more numerically stable",
        "Z": "This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is a warning about usinglstsq()?",
        "Y": "Warning",
        "Z": "Note This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is the name of the function used to synchronize a CUDA input with the CPU?",
        "Y": "Warning",
        "Z": "This function usestorch.linalg.svd()ifhermitian= Falseandtorch.linalg.eigh()ifhermitian= True.\nFor CUDA inputs, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.lstsq()if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: It is always prefered to uselstsq()when possible, as it is faster and more\nnumerically stable than computing the pseudoinverse explicitly. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does torch.linalg.inv() compute?",
        "Y": "the inverse of a square matrix",
        "Z": "torch.linalg.inv()computes the inverse of a square matrix. torch.linalg.lstsq()computesA.pinv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) \u2013 the tolerance value to determine when is a singular value zero\nIf it is atorch.Tensor, its shape must be\nbroadcastable to that of the singular values ofAas returned bytorch.svd().\nDefault:1e-15.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does torch.linalg.lstsq()computesA.pinv() @Bwith?",
        "Y": "numerically stable algorithm",
        "Z": "torch.linalg.lstsq()computesA.pinv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) \u2013 the tolerance value to determine when is a singular value zero\nIf it is atorch.Tensor, its shape must be\nbroadcastable to that of the singular values ofAas returned bytorch.svd().\nDefault:1e-15. hermitian(bool,optional) \u2013 indicates whetherAis Hermitian if complex\nor symmetric if real. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is the tolerance value to determine when is a singular value zero?",
        "Y": "rcond",
        "Z": "torch.linalg.lstsq()computesA.pinv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) \u2013 the tolerance value to determine when is a singular value zero\nIf it is atorch.Tensor, its shape must be\nbroadcastable to that of the singular values ofAas returned bytorch.svd().\nDefault:1e-15. hermitian(bool,optional) \u2013 indicates whetherAis Hermitian if complex\nor symmetric if real. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is the default value of atorch.Tensor?",
        "Y": "1e-15",
        "Z": "See also torch.linalg.inv()computes the inverse of a square matrix. torch.linalg.lstsq()computesA.pinv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) \u2013 the tolerance value to determine when is a singular value zero\nIf it is atorch.Tensor, its shape must be\nbroadcastable to that of the singular values ofAas returned bytorch.svd().\nDefault:1e-15.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What computes the inverse of a square matrix?",
        "Y": "torch.linalg.inv()",
        "Z": "See also torch.linalg.inv()computes the inverse of a square matrix. torch.linalg.lstsq()computesA.pinv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) \u2013 the tolerance value to determine when is a singular value zero\nIf it is atorch.Tensor, its shape must be\nbroadcastable to that of the singular values ofAas returned bytorch.svd().\nDefault:1e-15.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is the tensor of shape(*, m, n)where*is zero or more batch dimensions?",
        "Y": "A(Tensor)",
        "Z": "torch.linalg.lstsq()computesA.pinv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) \u2013 the tolerance value to determine when is a singular value zero\nIf it is atorch.Tensor, its shape must be\nbroadcastable to that of the singular values ofAas returned bytorch.svd().\nDefault:1e-15. hermitian(bool,optional) \u2013 indicates whetherAis Hermitian if complex\nor symmetric if real. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is the default value for atorch.Tensor?",
        "Y": "Default:1e-15",
        "Z": "torch.linalg.lstsq()computesA.pinv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) \u2013 the tolerance value to determine when is a singular value zero\nIf it is atorch.Tensor, its shape must be\nbroadcastable to that of the singular values ofAas returned bytorch.svd().\nDefault:1e-15. hermitian(bool,optional) \u2013 indicates whetherAis Hermitian if complex\nor symmetric if real. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does torch.linalg.inv() compute of a square matrix?",
        "Y": "inverse",
        "Z": "torch.linalg.inv()computes the inverse of a square matrix. torch.linalg.lstsq()computesA.pinv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) \u2013 the tolerance value to determine when is a singular value zero\nIf it is atorch.Tensor, its shape must be\nbroadcastable to that of the singular values ofAas returned bytorch.svd().\nDefault:1e-15.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is the default value of the atorch.Tensor?",
        "Y": "1e-15",
        "Z": "torch.linalg.inv()computes the inverse of a square matrix. torch.linalg.lstsq()computesA.pinv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) \u2013 the tolerance value to determine when is a singular value zero\nIf it is atorch.Tensor, its shape must be\nbroadcastable to that of the singular values ofAas returned bytorch.svd().\nDefault:1e-15.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What does hermitian indicate if complex or symmetric if real?",
        "Y": "Hermitian",
        "Z": "torch.linalg.lstsq()computesA.pinv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) \u2013 the tolerance value to determine when is a singular value zero\nIf it is atorch.Tensor, its shape must be\nbroadcastable to that of the singular values ofAas returned bytorch.svd().\nDefault:1e-15. hermitian(bool,optional) \u2013 indicates whetherAis Hermitian if complex\nor symmetric if real. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What is the default value of Hermitian?",
        "Y": "False",
        "Z": "torch.linalg.lstsq()computesA.pinv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, n)where*is zero or more batch dimensions. rcond(floatorTensor,optional) \u2013 the tolerance value to determine when is a singular value zero\nIf it is atorch.Tensor, its shape must be\nbroadcastable to that of the singular values ofAas returned bytorch.svd().\nDefault:1e-15. hermitian(bool,optional) \u2013 indicates whetherAis Hermitian if complex\nor symmetric if real. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "What returns the cumulative minimum of elements of input in the dimensiondim?",
        "Y": "a namedtuple",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of\nelements ofinputin the dimensiondim. Andindicesis the index\nlocation of each maximum value found in the dimensiondim. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin"
    },
    {
        "X": "Indicesis what of each mode value found?",
        "Y": "index location",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the mode\nvalue of each row of theinputtensor in the given dimensiondim, i.e. a value which appears most often\nin that row, andindicesis the index location of each mode value found. By default,dimis the last dimension of theinputtensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "By default,dimis is what?",
        "Y": "the last dimension of theinputtensor",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the mode\nvalue of each row of theinputtensor in the given dimensiondim, i.e. a value which appears most often\nin that row, andindicesis the index location of each mode value found. By default,dimis the last dimension of theinputtensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "How often does a value appear in a given row of theinputtensor?",
        "Y": "most often",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the mode\nvalue of each row of theinputtensor in the given dimensiondim, i.e. a value which appears most often\nin that row, andindicesis the index location of each mode value found. By default,dimis the last dimension of theinputtensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "By default,dimis the what dimension of theinputtensor?",
        "Y": "last dimension",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the mode\nvalue of each row of theinputtensor in the given dimensiondim, i.e. a value which appears most often\nin that row, andindicesis the index location of each mode value found. By default,dimis the last dimension of theinputtensor. IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "By default,dimis the last dimension of what?",
        "Y": "theinputtensor",
        "Z": "By default,dimis the last dimension of theinputtensor. IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "When are the output tensors of the same size asinput except in the dimensiondimwhere they are of size 1?",
        "Y": "IfkeepdimisTrue",
        "Z": "By default,dimis the last dimension of theinputtensor. IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "What is the result of the output tensors being squeezed?",
        "Y": "1 fewer dimension thaninput",
        "Z": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "Where is this function not defined?",
        "Y": "fortorch.cuda.Tensoryet",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the mode\nvalue of each row of theinputtensor in the given dimensiondim, i.e. a value which appears most often\nin that row, andindicesis the index location of each mode value found. By default,dimis the last dimension of theinputtensor. IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "By default,dimis what dimension of the inputtensor?",
        "Y": "last dimension",
        "Z": "By default,dimis the last dimension of theinputtensor. IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "In what case are output tensors of the same size as input except in the dimensiondimwhere they are of size 1?",
        "Y": "IfkeepdimisTrue",
        "Z": "By default,dimis the last dimension of theinputtensor. IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "What is another name for squeezed output tensors?",
        "Y": "seetorch.squeeze()",
        "Z": "By default,dimis the last dimension of theinputtensor. IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "Note This function is not defined what?",
        "Y": "fortorch.cuda.Tensoryet",
        "Z": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "What makes the output tensors of the same size as input except in the dimensiondimwhere they are of size 1?",
        "Y": "IfkeepdimisTrue",
        "Z": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "What is the name of the function that results in the output tensors having 1 fewer dimension than input?",
        "Y": "seetorch.squeeze()",
        "Z": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices)",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "This function is not defined what?",
        "Y": "fortorch.cuda.Tensoryet",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the mode\nvalue of each row of theinputtensor in the given dimensiondim, i.e. a value which appears most often\nin that row, andindicesis the index location of each mode value found. By default,dimis the last dimension of theinputtensor. IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "What is this function not defined?",
        "Y": "fortorch.cuda.Tensoryet",
        "Z": "Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "Out(tuple,optional) is the result tuple of two what?",
        "Y": "output tensors",
        "Z": "Note This function is not defined fortorch.cuda.Tensoryet. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the result tuple of two output tensors (values, indices) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "What website does Alias fortorch.ne belong to?",
        "Y": "Alias fortorch.ne",
        "Z": "Alias fortorch.ne().",
        "source": "https://pytorch.org/docs/stable/generated/torch.not_equal.html#torch.not_equal"
    },
    {
        "X": "What does optimizer_class(torch.nn.Optimizer) contain?",
        "Y": "the class of the local optimizer",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the name of the local optimizer?",
        "Y": "group",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is a bool for when parameters are packed into larger buckets?",
        "Y": "parameters_as_bucket_views",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What will remain intact when disabled?",
        "Y": "butparams.data",
        "Z": "parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Param.datafields will point to what at different offsets?",
        "Y": "bucket views",
        "Z": "parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What will be forwarded to the given optimizer?",
        "Y": "all trailing arguments",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is an example of how to add a param group to theOptimizersparam_groups?",
        "Y": "Add a param group to theOptimizersparam_groups",
        "Z": "parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What can be useful when fine tuning a pre-trained network?",
        "Y": "Add a param group to theOptimizersparam_groups",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What can be made trainable and added to theOptimizeras training progresses?",
        "Y": "frozen layers",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does param_group(dict) specify?",
        "Y": "Tensors",
        "Z": "param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "How many consolidated state_dicts are there per rank?",
        "Y": "one",
        "Z": "param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is to(int)?",
        "Y": "the rank that receives the global states",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the default value for the rank that receives the global states?",
        "Y": "0",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What can be made trainable by adding a param group to theOptimizersparam_groups?",
        "Y": "frozen layers",
        "Z": "**default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0)",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What Specifies what Tensors should be optimized along with group specific optimization options?",
        "Y": "param_group(dict)",
        "Z": "params(Iterable) \u2013 anIterableoftorch.Tensors optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "How many consolidated state_dicts are updated per rank?",
        "Y": "one per rank",
        "Z": "This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the rank that receives the global states?",
        "Y": "to(int)",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "How many states does the consolidated state_dict list update?",
        "Y": "one per rank",
        "Z": "Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "To(int) \u2013 the rank that receives the global states. (default: what) Restore the global parameter groups as well as the",
        "Y": "0",
        "Z": "to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update).",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "How many consolidated state_dicts does theOptimizersparam_groups update?",
        "Y": "one per rank",
        "Z": "Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does param_group(dict) do?",
        "Y": "Specifies what Tensors should be optimized along with group specific optimization options",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0)",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the default value for global parameter groups?",
        "Y": "0",
        "Z": "This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the default value for the rank that receives global states?",
        "Y": "0",
        "Z": "param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does state_dict(dict) contain?",
        "Y": "optimizer state",
        "Z": "param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the name of the call that returns the state of the optimizer?",
        "Y": "tostate_dict()",
        "Z": "param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "How many entries does the state of the optimizer as adict contain?",
        "Y": "two",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is a dict containing all parameter groups?",
        "Y": "param_groups",
        "Z": "Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is a dict containing all parameter groups Partitions parameters across distributed data parallel ranks?",
        "Y": "param_groups",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is state_dict(dict)?",
        "Y": "optimizer state",
        "Z": "to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Should be an object returned from a call from what?",
        "Y": "tostate_dict()",
        "Z": "to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update).",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "How many entries does the state of the optimizer contain?",
        "Y": "two",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Restore the global parameter groups as well as what else?",
        "Y": "shard",
        "Z": "Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What dict contains all parameter groups Partitions parameters across distributed data parallel ranks?",
        "Y": "param_groups",
        "Z": "Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Restore what as well as the shard?",
        "Y": "global parameter groups",
        "Z": "Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the name of the optimizer state?",
        "Y": "state_dict(dict)",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does state_dict(dict) represent?",
        "Y": "optimizer state",
        "Z": "parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Should be an object returned from a call to what?",
        "Y": "state_dict()",
        "Z": "state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the state of the optimizer as adict?",
        "Y": "Gets this rank\u2019sstate_dict",
        "Z": "Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is a list of dict?",
        "Y": "a list ofparam_groups",
        "Z": "param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Which element corresponds to rank 0, etc.?",
        "Y": "Element 0",
        "Z": "param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "We need all the ranks for the broadcast what?",
        "Y": "insidestep()",
        "Z": "param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does insidestep return for a given rank?",
        "Y": "local_state_dict",
        "Z": "Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is a list of dicts?",
        "Y": "a list ofparam_groups",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Element 0 corresponds to what rank?",
        "Y": "rank 0,",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the name of the broadcast we need all the ranks for?",
        "Y": "insidestep()",
        "Z": "param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does insidestep() return for a given rank?",
        "Y": "local_state_dict",
        "Z": "param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "We need all the ranks for the broadcast for what?",
        "Y": "insidestep()",
        "Z": "The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does param_groups contain across distributed data parallel ranks?",
        "Y": "Partitions parameters",
        "Z": "param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does rank(int) return?",
        "Y": "getlocal_state_dictfor",
        "Z": "The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the name of the function that returns the local_state_dict for a given rank?",
        "Y": "insidestep()",
        "Z": "param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the name of the rank to getlocal_state_dictfor?",
        "Y": "rank",
        "Z": "The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What differs between?",
        "Y": "optimizer classes",
        "Z": "differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "A list ofparam_groups is a list of what?",
        "Y": "dict",
        "Z": "Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What corresponds to rank 0, etc.?",
        "Y": "Element 0",
        "Z": "This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep().",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does state_dict(dict) return?",
        "Y": "globalstate_dict",
        "Z": "param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What class differs between param_groups and param_groups?",
        "Y": "optimizer classes",
        "Z": "differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does getlocal_state_dict for state_dict(dict) return?",
        "Y": "globalstate_dict",
        "Z": "param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Param_groups contains all parameter groups across distributed data parallel ranks.",
        "Y": "Partitions parameters",
        "Z": "param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Which element of the list corresponds to rank 0, etc.?",
        "Y": "Element 0",
        "Z": "param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does getlocal_state_dictfor state_dict(dict) return?",
        "Y": "globalstate_dict",
        "Z": "param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is a part of distributed data parallel ranks?",
        "Y": "Partitions parameters",
        "Z": "Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Globalstate_dict consist of a list of what?",
        "Y": "shards",
        "Z": "Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "Partitions parameters across distributed data what?",
        "Y": "parallel ranks",
        "Z": "Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards.",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What function returns the local_state_dict for a given rank?",
        "Y": "insidestep()",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the globalstate_dict?",
        "Y": "last known global optimizer state",
        "Z": "Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is a single optimization step called?",
        "Y": "parameter update",
        "Z": "Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is returned for a given rank?",
        "Y": "local_state_dict",
        "Z": "Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the name of the rank to getlocal_state_dict for state_dict(dict)?",
        "Y": "rank",
        "Z": "Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is the last known global optimizer state?",
        "Y": "globalstate_dict",
        "Z": "optimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer. group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:group.WORLDinitialized bytorch.distributed.init_process_group()). parameters_as_bucket_views(bool) \u2013 when enabled, parameters will\nbe packed into larger buckets to speed up communication andparam.datafields will point to bucket views at different\noffsets. When disabled, each individual parameter will be\ncommunicated separately, butparams.datawill stay intact. **default\u2013 all trailing arguments will be forwarded to the given optimizer. Example: Add a param group to theOptimizersparam_groups. This can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses. param_group(dict) \u2013 Specifies what Tensors should be optimized\nalong with group specific optimization options. Update the consolidated state_dict list, one per rank. to(int) \u2013 the rank that receives the global states. (default: 0) Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is a closure that reevaluates the model and returns the loss?",
        "Y": "closure",
        "Z": "Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What type of loss depends on the underlying optimizer?",
        "Y": "Optional",
        "Z": "Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What does optional loss depend on?",
        "Y": "underlying optimizer",
        "Z": "Restore the global parameter groups as well as the shard. state_dict(dict) \u2013 optimizer state. Should be an object returned\nfrom a call tostate_dict() Gets this rank\u2019sstate_dict.  The state of the optimizer as adict.\nIt contains two entries: differs between optimizer classes. param_groups - a dict containing all parameter groups Partitions parameters across distributed data parallel ranks. a list ofparam_groups(which is a list of dict) where each\nelement of the list contains the param_groups for a rank. Element 0\ncorresponds to rank 0, etc. We need all the ranks for the broadcast\ninsidestep(). Returns the local_state_dict for a given rank. rank(int) \u2013 rank to getlocal_state_dictfor state_dict(dict) \u2013 globalstate_dict the last known global optimizer state, which consist of a list of\nthe shards. Performs a single optimization step (parameter update). closure(callable) \u2013 A closure that reevaluates the model and\nreturns the loss. Optional for most optimizers. optional loss, depends on the underlying optimizer",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "What is expected to be the inverse ofstft()?",
        "Y": "Inverse short time Fourier Transform",
        "Z": "Inverse short time Fourier Transform. This is expected to be the inverse ofstft().\nIt has the same parameters (+ additional optional parameter oflength) and it should return the\nleast squares estimation of the original signal. The algorithm will check using the NOLA condition (\nnonzero overlap). Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information.",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Inverse short time Fourier Transform is expected to be the inverse of what?",
        "Y": "ofstft()",
        "Z": "Inverse short time Fourier Transform. This is expected to be the inverse ofstft().\nIt has the same parameters (+ additional optional parameter oflength) and it should return the\nleast squares estimation of the original signal. The algorithm will check using the NOLA condition (\nnonzero overlap).",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What should the Inverse short time Fourier Transform return?",
        "Y": "least squares estimation",
        "Z": "Inverse short time Fourier Transform. This is expected to be the inverse ofstft().\nIt has the same parameters (+ additional optional parameter oflength) and it should return the\nleast squares estimation of the original signal. The algorithm will check using the NOLA condition (\nnonzero overlap). Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information.",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What condition will the algorithm check using?",
        "Y": "NOLA condition",
        "Z": "Inverse short time Fourier Transform. This is expected to be the inverse ofstft().\nIt has the same parameters (+ additional optional parameter oflength) and it should return the\nleast squares estimation of the original signal. The algorithm will check using the NOLA condition (\nnonzero overlap). Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information.",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the envelop created by?",
        "Y": "the summation of all the windows",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded).",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the envelop created by the summation of all the windows at a certain point in time?",
        "Y": "0",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded).",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Why does stft return a shorter signal than the original signal?",
        "Y": "Sincestft()discards elements at the end of the signal if they do not fit in a frame",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded).",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the envelop created by the summation of all windows at certain point in time?",
        "Y": "never zero",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the envelop created by the summation of all windows never zero at certain point in time?",
        "Y": "0",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded).",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "If the signal isn\u2019t padded, what can result in a shorter signal than the original signal?",
        "Y": "ifcenteris False",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded).",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What discards elements at the end of the signal if they do not fit in a frame?",
        "Y": "Sincestft()",
        "Z": "Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "If the signal isn't padded, what happens?",
        "Y": "IfcenterisTrue",
        "Z": "Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "IfcenterisTrue, there will be padding e.g. what?",
        "Y": "constant",
        "Z": "Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What can be trimmed off exactly because they can be calculated?",
        "Y": "Left padding",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "If the signal isn't padded, what can result in a shorter signal than the original signal?",
        "Y": "ifcenteris False",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "If centeris False, then there will be padding e.g. 'constant','reflect', etc.",
        "Y": "IfcenterisTrue",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "IfcenterisTrue, then there will be padding e.g.'reflect','reflect', etc.",
        "Y": "constant",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Why can left padding be trimmed off exactly?",
        "Y": "because they can be calculated",
        "Z": "IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the last window of a signal?",
        "Y": "last window",
        "Z": "Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "If there is padding, what is it called?",
        "Y": "IfcenterisTrue",
        "Z": "IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "If centerisTrue, then there will be padding e.g.'reflect','reflect', etc.",
        "Y": "constant",
        "Z": "IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the default setting for padding?",
        "Y": "IfcenterisTrue",
        "Z": "IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the last window in a window?",
        "Y": "last",
        "Z": "Important consideration in the parameterswindowandcenterso that the envelop\ncreated by the summation of all the windows is never zero at certain point in time. Specifically,\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0\u2211t=\u2212\u221e\u221e\u200b\u2223w\u22232[n\u2212t\u00d7hop_length]=\u200b0. Sincestft()discards elements at the end of the signal if they do not fit in a frame,istftmay return a shorter signal than the original signal (can occur ifcenteris False\nsince the signal isn\u2019t padded). IfcenterisTrue, then there will be padding e.g.'constant','reflect', etc.\nLeft padding can be trimmed off exactly because they can be calculated but right padding cannot be\ncalculated without additional information. Example: Suppose the last window is:[17,18,0,0,0]vs[18,0,0,0,0]",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Who wrote \"Signal estimation from modified short-time Fourier transform\"?",
        "Y": "D. W. Griffin and J. S. Lim",
        "Z": "[1] D. W. Griffin and J. S. Lim, \u201cSignal estimation from modified short-time Fourier transform,\u201d\nIEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984. input(Tensor) \u2013 The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead.",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What was the title of the IEEE Trans. ASSP?",
        "Y": "vol.32, no.2, pp.236-243",
        "Z": "[1] D. W. Griffin and J. S. Lim, \u201cSignal estimation from modified short-time Fourier transform,\u201d\nIEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984. input(Tensor) \u2013 The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead.",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the input tensor expected to be?",
        "Y": "output ofstft()",
        "Z": "[1] D. W. Griffin and J. S. Lim, \u201cSignal estimation from modified short-time Fourier transform,\u201d\nIEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984. input(Tensor) \u2013 The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead.",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "When was real input deprecated?",
        "Y": "1.8.0",
        "Z": "[1] D. W. Griffin and J. S. Lim, \u201cSignal estimation from modified short-time Fourier transform,\u201d\nIEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984. input(Tensor) \u2013 The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead.",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the channeldimension?",
        "Y": "optional",
        "Z": "The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Since what version is real input deprecated?",
        "Y": "1.8.0",
        "Z": "Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the default value of hop_length(Optional[int])?",
        "Y": "Default:n_fft//4)",
        "Z": "input(Tensor) \u2013 The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is expected to be output ofstft()?",
        "Y": "input tensor",
        "Z": "The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Since what version is the input tensor deprecated?",
        "Y": "1.8.0",
        "Z": "The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is window(Optional[torch.Tensor])?",
        "Y": "optional window function",
        "Z": "win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is Torch.ones(Optional[torch.Tensor]) called?",
        "Y": "win_length",
        "Z": "Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length))",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is center(bool)?",
        "Y": "Whetherinputwas padded on both sides",
        "Z": "win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the default value for center(bool)?",
        "Y": "True",
        "Z": "hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the distance between neighboring window frames?",
        "Y": "hop_length",
        "Z": "hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the default value of center(bool)?",
        "Y": "True",
        "Z": "hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is normalized(bool)?",
        "Y": "Whether the STFT was normalized",
        "Z": "window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Whether the STFT was onesided or onesided?",
        "Y": "onesided",
        "Z": "Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What does trueifn_fft! mean in the input size?",
        "Y": "fft_size",
        "Z": "input(Tensor) \u2013 The input tensor. Expected to be output ofstft(),\ncan either be complex (channel,fft_size,n_frame), or real\n(channel,fft_size,n_frame, 2) where thechanneldimension is optional. Deprecated since version 1.8.0:Real input is deprecated, use complex inputs as returned bystft(...,return_complex=True)instead. n_fft(int) \u2013 Size of Fourier transform hop_length(Optional[int]) \u2013 The distance between neighboring sliding window frames.\n(Default:n_fft//4) win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Whether the STFT was normalized. (Default:False) onesided(Optional) \u2013 Whether the STFT was one",
        "Y": "normalized",
        "Z": "center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the amount to trim the signal by (i.e. the original signal length)?",
        "Y": "length",
        "Z": "center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the amount to trim the signal by?",
        "Y": "the original signal length",
        "Z": "win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the default signal length?",
        "Y": "whole signal",
        "Z": "center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is onesided(Optional[bool])?",
        "Y": "Whether the STFT was onesided",
        "Z": "onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the default value for the output of a STFT?",
        "Y": "return_complex",
        "Z": "onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length)",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "Is return_complex compatible or incompatible with onesided=True?",
        "Y": "incompatible",
        "Z": "win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the value of size (..., signal_length)?",
        "Y": "Least squares estimation of the original signal",
        "Z": "win_length(Optional[int]) \u2013 The size of window frame and STFT filter. (Default:n_fft) window(Optional[torch.Tensor]) \u2013 The optional window function.\n(Default:torch.ones(win_length)) center(bool) \u2013 Whetherinputwas padded on both sides so that thettt-th frame is\ncentered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\n(Default:True) normalized(bool) \u2013 Whether the STFT was normalized. (Default:False) onesided(Optional[bool]) \u2013 Whether the STFT was onesided.\n(Default:Trueifn_fft!=fft_sizein the input size) length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the value to trim the signal by?",
        "Y": "length",
        "Z": "length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is incompatible withonesided=True?",
        "Y": "return_complex",
        "Z": "length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is the Least squares estimation of the original signal of size?",
        "Y": "Tensor",
        "Z": "length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is Optional[bool]) \u2013 Whether the output should be complex or if the input should be assumed to derive from",
        "Y": "return_complex",
        "Z": "length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is incompatible with return_complex?",
        "Y": "withonesided=True",
        "Z": "length(Optional[int]) \u2013 The amount to trim the signal by (i.e. the\noriginal signal length). (Default: whole signal) return_complex(Optional[bool]) \u2013 Whether the output should be complex, or if the input should be\nassumed to derive from a real signal and window.\nNote that this is incompatible withonesided=True.\n(Default:False) Least squares estimation of the original signal of size (\u2026, signal_length) Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"
    },
    {
        "X": "What is non-integerstep subject to when comparing againstend?",
        "Y": "floating point rounding errors",
        "Z": "Note that non-integerstepis subject to floating point rounding errors when\ncomparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) \u2013 the starting value for the set of points. Default:0. end(Number) \u2013 the ending value for the set of points step(Number) \u2013 the gap between each pair of adjacent points. Default:1. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Ifdtypeis not given, infer the data type from the other input\narguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to\nbetorch.int64. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"
    },
    {
        "X": "What is the default value for the starting value for the set of points?",
        "Y": "Default:0",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart. Note that non-integerstepis subject to floating point rounding errors when\ncomparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) \u2013 the starting value for the set of points. Default:0.",
        "source": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"
    },
    {
        "X": "What is returned by sizeendstartstepleftlceil fractextend?",
        "Y": "1-D tensor",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart. Note that non-integerstepis subject to floating point rounding errors when\ncomparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) \u2013 the starting value for the set of points. Default:0.",
        "source": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"
    },
    {
        "X": "Non-integerstepis subject to what when comparing againstend?",
        "Y": "floating point rounding errors",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart. Note that non-integerstepis subject to floating point rounding errors when\ncomparing againstend; to avoid inconsistency, we advise adding a small epsilon toendin such cases. start(Number) \u2013 the starting value for the set of points. Default:0. end(Number) \u2013 the ending value for the set of points step(Number) \u2013 the gap between each pair of adjacent points. Default:1. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Ifdtypeis not given, infer the data type from the other input\narguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to\nbetorch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"
    },
    {
        "X": "What is step(Number)?",
        "Y": "the gap between each pair of adjacent points",
        "Z": "end(Number) \u2013 the ending value for the set of points step(Number) \u2013 the gap between each pair of adjacent points. Default:1. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Ifdtypeis not given, infer the data type from the other input\narguments. If any ofstart,end, orstopare floating-point, thedtypeis inferred to be the default dtype, seeget_default_dtype(). Otherwise, thedtypeis inferred to\nbetorch.int64. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"
    },
    {
        "X": "What is the default value for the ending value for the set of points step(Number)?",
        "Y": "Default:1",
        "Z": "end(Number) \u2013 the ending value for the set of points step(Number) \u2013 the gap between each pair of adjacent points. Default:1. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"
    },
    {
        "X": "Warning More than one element of a created tensor may refer to what?",
        "Y": "a single memory location",
        "Z": "Warning More than one element of a created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to\nthe tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "What can in-place operations result in?",
        "Y": "incorrect behavior",
        "Z": "More than one element of a created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to\nthe tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "What should you do if you need to write to the tensors?",
        "Y": "clone them first",
        "Z": "More than one element of a created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to\nthe tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "What returns a view of a tensor?",
        "Y": "PyTorch functions",
        "Z": "Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor. size(tupleorints) \u2013 the shape of the output tensor stride(tupleorints) \u2013 the stride of the output tensor storage_offset(int,optional) \u2013 the offset in the underlying storage of the output tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "Why are the functions liketorch.Tensor.expand() more advisable to use?",
        "Y": "easier to read",
        "Z": "Warning More than one element of a created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to\nthe tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "More than one element of a created tensor may refer to what?",
        "Y": "single memory location",
        "Z": "More than one element of a created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to\nthe tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "What may result in incorrect behavior?",
        "Y": "in-place operations",
        "Z": "More than one element of a created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to\nthe tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "What should you do if you need to write to the tensors first?",
        "Y": "clone",
        "Z": "More than one element of a created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to\nthe tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "What are PyTorch functions that return a view of a tensor called?",
        "Y": "liketorch.Tensor.expand()",
        "Z": "Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor. size(tupleorints) \u2013 the shape of the output tensor stride(tupleorints) \u2013 the stride of the output tensor storage_offset(int,optional) \u2013 the offset in the underlying storage of the output tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "Why is liketorch.Tensor.expand() more advisable to use?",
        "Y": "easier to read",
        "Z": "More than one element of a created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to\nthe tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "More than one element of a created tensor may refer to a single what?",
        "Y": "memory location",
        "Z": "More than one element of a created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to\nthe tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "Why are PyTorch functions liketorch.Tensor.expand() more advisable to use?",
        "Y": "easier to read",
        "Z": "Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor. size(tupleorints) \u2013 the shape of the output tensor stride(tupleorints) \u2013 the stride of the output tensor storage_offset(int,optional) \u2013 the offset in the underlying storage of the output tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "What is the shape of the output tensor stride?",
        "Y": "size(tupleorints)",
        "Z": "Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor. size(tupleorints) \u2013 the shape of the output tensor stride(tupleorints) \u2013 the stride of the output tensor storage_offset(int,optional) \u2013 the offset in the underlying storage of the output tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "What is size(tupleorints)?",
        "Y": "the shape of the output tensor stride(tupleorints)",
        "Z": "Many PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, liketorch.Tensor.expand(), are easier to read and are therefore more\nadvisable to use. input(Tensor) \u2013 the input tensor. size(tupleorints) \u2013 the shape of the output tensor stride(tupleorints) \u2013 the stride of the output tensor storage_offset(int,optional) \u2013 the offset in the underlying storage of the output tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "What is a transposed version of input?",
        "Y": "tensor",
        "Z": "Returns a tensor that is a transposed version ofinput.\nThe given dimensionsdim0anddim1are swapped. The resultingouttensor shares its underlying storage with theinputtensor, so changing the content of one would change the content\nof the other. input(Tensor) \u2013 the input tensor. dim0(int) \u2013 the first dimension to be transposed dim1(int) \u2013 the second dimension to be transposed Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose"
    },
    {
        "X": "The resultingouttensor shares its underlying storage with what?",
        "Y": "theinputtensor",
        "Z": "Returns a tensor that is a transposed version ofinput.\nThe given dimensionsdim0anddim1are swapped. The resultingouttensor shares its underlying storage with theinputtensor, so changing the content of one would change the content\nof the other. input(Tensor) \u2013 the input tensor. dim0(int) \u2013 the first dimension to be transposed dim1(int) \u2013 the second dimension to be transposed Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose"
    },
    {
        "X": "What is the second dimension to be transposed?",
        "Y": "dim1(int)",
        "Z": "Returns a tensor that is a transposed version ofinput.\nThe given dimensionsdim0anddim1are swapped. The resultingouttensor shares its underlying storage with theinputtensor, so changing the content of one would change the content\nof the other. input(Tensor) \u2013 the input tensor. dim0(int) \u2013 the first dimension to be transposed dim1(int) \u2013 the second dimension to be transposed Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose"
    },
    {
        "X": "Returns the cross product of vectors in what?",
        "Y": "dimensiondimofinputandother",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother. inputandothermust have the same size, and the size of theirdimdimension should be 3. Ifdimis not given, it defaults to the first dimension found with the\nsize 3. Note that this might be unexpected. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor dim(int,optional) \u2013 the dimension to take the cross-product in. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross"
    },
    {
        "X": "The size of theirdimdimension should be what?",
        "Y": "3",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother. inputandothermust have the same size, and the size of theirdimdimension should be 3. Ifdimis not given, it defaults to the first dimension found with the\nsize 3. Note that this might be unexpected. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor dim(int,optional) \u2013 the dimension to take the cross-product in. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross"
    },
    {
        "X": "Ifdimis is not given, what happens?",
        "Y": "defaults to the first dimension found with the size 3",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother. inputandothermust have the same size, and the size of theirdimdimension should be 3. Ifdimis not given, it defaults to the first dimension found with the\nsize 3. Note that this might be unexpected. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor dim(int,optional) \u2013 the dimension to take the cross-product in. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross"
    },
    {
        "X": "What might happen if the size of inputandother is not given?",
        "Y": "unexpected",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother. inputandothermust have the same size, and the size of theirdimdimension should be 3. Ifdimis not given, it defaults to the first dimension found with the\nsize 3. Note that this might be unexpected. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor dim(int,optional) \u2013 the dimension to take the cross-product in. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross"
    },
    {
        "X": "What is the second input tensor dim(int,optional)?",
        "Y": "other(Tensor)",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother. inputandothermust have the same size, and the size of theirdimdimension should be 3. Ifdimis not given, it defaults to the first dimension found with the\nsize 3. Note that this might be unexpected. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor dim(int,optional) \u2013 the dimension to take the cross-product in. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross"
    },
    {
        "X": "How far can a n-D tensor be rotated?",
        "Y": "90 degrees",
        "Z": "Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.\nRotation direction is from the first towards the second axis if k > 0, and from the second towards the first for k < 0. input(Tensor) \u2013 the input tensor. k(int) \u2013 number of times to rotate dims(a listortuple) \u2013 axis to rotate Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90"
    },
    {
        "X": "What is the number of times to rotate dims?",
        "Y": "k(int)",
        "Z": "Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.\nRotation direction is from the first towards the second axis if k > 0, and from the second towards the first for k < 0. input(Tensor) \u2013 the input tensor. k(int) \u2013 number of times to rotate dims(a listortuple) \u2013 axis to rotate Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90"
    },
    {
        "X": "What does a listortuple represent?",
        "Y": "axis",
        "Z": "Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.\nRotation direction is from the first towards the second axis if k > 0, and from the second towards the first for k < 0. input(Tensor) \u2013 the input tensor. k(int) \u2013 number of times to rotate dims(a listortuple) \u2013 axis to rotate Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90"
    },
    {
        "X": "What does asscatter_() do?",
        "Y": "Adds all values from the tensorotherintoself",
        "Z": "Adds all values from the tensorotherintoselfat the indices\nspecified in theindextensor in a similar fashion asscatter_(). For each value insrc, it is added to\nan index inselfwhich is specified by its index insrcfordimension!=dimand by the corresponding value inindexfordimension=dim. For a 3-D tensor,selfis updated as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "For each value insrc, it is added to what?",
        "Y": "index inself",
        "Z": "Adds all values from the tensorotherintoselfat the indices\nspecified in theindextensor in a similar fashion asscatter_(). For each value insrc, it is added to\nan index inselfwhich is specified by its index insrcfordimension!=dimand by the corresponding value inindexfordimension=dim. For a 3-D tensor,selfis updated as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "What happens to a 3-D tensor for a 3-D tensor?",
        "Y": "selfis updated as:",
        "Z": "Adds all values from the tensorotherintoselfat the indices\nspecified in theindextensor in a similar fashion asscatter_(). For each value insrc, it is added to\nan index inselfwhich is specified by its index insrcfordimension!=dimand by the corresponding value inindexfordimension=dim. For a 3-D tensor,selfis updated as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "Where can this operation behave nondeterministically when given tensors?",
        "Y": "CUDA device",
        "Z": "Note This operation may behave nondeterministically when given tensors on a CUDA device. SeeReproducibilityfor more information. Note The backward pass is implemented only forsrc.shape==index.shape. dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to scatter and add, can be\neither empty or of the same dimensionality assrc. When empty, the\noperation returnsselfunchanged. src(Tensor) \u2013 the source elements to scatter and add Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "What does the backward pass implement only forsrc.shape==index.shape?",
        "Y": "SeeReproducibility",
        "Z": "Note This operation may behave nondeterministically when given tensors on a CUDA device. SeeReproducibilityfor more information. Note The backward pass is implemented only forsrc.shape==index.shape. dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to scatter and add, can be\neither empty or of the same dimensionality assrc. When empty, the\noperation returnsselfunchanged. src(Tensor) \u2013 the source elements to scatter and add Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "What is the axis along which to index index(LongTensor)?",
        "Y": "dim(int)",
        "Z": "Adds all values from the tensorotherintoselfat the indices\nspecified in theindextensor in a similar fashion asscatter_(). For each value insrc, it is added to\nan index inselfwhich is specified by its index insrcfordimension!=dimand by the corresponding value inindexfordimension=dim. For a 3-D tensor,selfis updated as: self,indexandsrcshould have same number of\ndimensions. It is also required thatindex.size(d)<=src.size(d)for all\ndimensionsd, and thatindex.size(d)<=self.size(d)for all dimensionsd!=dim. Note thatindexandsrcdo not broadcast. Note This operation may behave nondeterministically when given tensors on a CUDA device. SeeReproducibilityfor more information. Note The backward pass is implemented only forsrc.shape==index.shape. dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to scatter and add, can be\neither empty or of the same dimensionality assrc. When empty, the\noperation returnsselfunchanged. src(Tensor) \u2013 the source elements to scatter and add Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "When empty, what does the operation return?",
        "Y": "returnsselfunchanged",
        "Z": "Note This operation may behave nondeterministically when given tensors on a CUDA device. SeeReproducibilityfor more information. Note The backward pass is implemented only forsrc.shape==index.shape. dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to scatter and add, can be\neither empty or of the same dimensionality assrc. When empty, the\noperation returnsselfunchanged. src(Tensor) \u2013 the source elements to scatter and add Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "What is the source element to scatter and add?",
        "Y": "src(Tensor)",
        "Z": "Adds all values from the tensorotherintoselfat the indices\nspecified in theindextensor in a similar fashion asscatter_(). For each value insrc, it is added to\nan index inselfwhich is specified by its index insrcfordimension!=dimand by the corresponding value inindexfordimension=dim. For a 3-D tensor,selfis updated as: self,indexandsrcshould have same number of\ndimensions. It is also required thatindex.size(d)<=src.size(d)for all\ndimensionsd, and thatindex.size(d)<=self.size(d)for all dimensionsd!=dim. Note thatindexandsrcdo not broadcast. Note This operation may behave nondeterministically when given tensors on a CUDA device. SeeReproducibilityfor more information. Note The backward pass is implemented only forsrc.shape==index.shape. dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to scatter and add, can be\neither empty or of the same dimensionality assrc. When empty, the\noperation returnsselfunchanged. src(Tensor) \u2013 the source elements to scatter and add Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "When given tensors on what device may this operation behave nondeterministically?",
        "Y": "CUDA",
        "Z": "Note This operation may behave nondeterministically when given tensors on a CUDA device. SeeReproducibilityfor more information. Note The backward pass is implemented only forsrc.shape==index.shape. dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to scatter and add, can be\neither empty or of the same dimensionality assrc. When empty, the\noperation returnsselfunchanged. src(Tensor) \u2013 the source elements to scatter and add Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "What is the name of the operation that may behave nondeterministically when given tensors on a CUDA device?",
        "Y": "Reproducibility",
        "Z": "Note This operation may behave nondeterministically when given tensors on a CUDA device. SeeReproducibilityfor more information. Note The backward pass is implemented only forsrc.shape==index.shape. dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to scatter and add, can be\neither empty or of the same dimensionality assrc. When empty, the\noperation returnsselfunchanged. src(Tensor) \u2013 the source elements to scatter and add Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "When empty, the operation returns what?",
        "Y": "selfunchanged",
        "Z": "Note This operation may behave nondeterministically when given tensors on a CUDA device. SeeReproducibilityfor more information. Note The backward pass is implemented only forsrc.shape==index.shape. dim(int) \u2013 the axis along which to index index(LongTensor) \u2013 the indices of elements to scatter and add, can be\neither empty or of the same dimensionality assrc. When empty, the\noperation returnsselfunchanged. src(Tensor) \u2013 the source elements to scatter and add Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "The upper triangular part of a matrix returns how many tensors?",
        "Y": "2",
        "Z": "Returns the upper triangular part of a matrix (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu"
    },
    {
        "X": "What part of a matrix is defined as the elements on and above the diagonal?",
        "Y": "the upper triangular part",
        "Z": "Returns the upper triangular part of a matrix (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu"
    },
    {
        "X": "What is n0n geq 0n0 called?",
        "Y": "the order of the polygamma function",
        "Z": "Computes thenthn^{th}nthderivative of the digamma function oninput.n\u22650n \\geq 0n\u22650is called the order of the polygamma function. Note This function is implemented only for nonnegative integersn\u22650n \\geq 0n\u22650. n(int) \u2013 the order of the polygamma function input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma"
    },
    {
        "X": "The order of the polygamma function is implemented only for what?",
        "Y": "nonnegative integers",
        "Z": "Computes thenthn^{th}nthderivative of the digamma function oninput.n\u22650n \\geq 0n\u22650is called the order of the polygamma function. Note This function is implemented only for nonnegative integersn\u22650n \\geq 0n\u22650. n(int) \u2013 the order of the polygamma function input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma"
    },
    {
        "X": "What is the input tensor of the polygamma function?",
        "Y": "input(Tensor)",
        "Z": "Computes thenthn^{th}nthderivative of the digamma function oninput.n\u22650n \\geq 0n\u22650is called the order of the polygamma function. Note This function is implemented only for nonnegative integersn\u22650n \\geq 0n\u22650. n(int) \u2013 the order of the polygamma function input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma"
    },
    {
        "X": "What is an example of the order of the polygamma function?",
        "Y": "Example",
        "Z": "Computes thenthn^{th}nthderivative of the digamma function oninput.n\u22650n \\geq 0n\u22650is called the order of the polygamma function. Note This function is implemented only for nonnegative integersn\u22650n \\geq 0n\u22650. n(int) \u2013 the order of the polygamma function input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma"
    },
    {
        "X": "Calculates the variance of all elements in what?",
        "Y": "theinputtensor",
        "Z": "IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample variance is calculated, without any correction. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. Calculates the variance of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.var.html#torch.var"
    },
    {
        "X": "What is calculated if Bessel's correction is used?",
        "Y": "the sample variance",
        "Z": "IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample variance is calculated, without any correction. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. Calculates the variance of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.var.html#torch.var"
    },
    {
        "X": "What does out(Tensor,optional) calculate?",
        "Y": "the variance of all elements in theinputtensor",
        "Z": "unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. Calculates the variance of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.var.html#torch.var"
    },
    {
        "X": "What is calculated in the inputtensor?",
        "Y": "the variance of all elements",
        "Z": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. Calculates the variance of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.var.html#torch.var"
    },
    {
        "X": "What is the default value of Bessel's correction?",
        "Y": "IfunbiasedisTrue",
        "Z": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. Calculates the variance of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.var.html#torch.var"
    },
    {
        "X": "Whether the output tensor hasdimretained or not?",
        "Y": "keepdim",
        "Z": "unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. Calculates the variance of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.var.html#torch.var"
    },
    {
        "X": "How does this function check if allinputandothersatisfy the condition?",
        "Y": "elementwise",
        "Z": "This function checks if allinputandothersatisfy the condition: elementwise, for all elements ofinputandother. The behaviour of this function is analogous tonumpy.allclose input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08 rtol(float,optional) \u2013 relative tolerance. Default: 1e-05 equal_nan(bool,optional) \u2013 ifTrue, then twoNaNs will be considered equal. Default:False Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose"
    },
    {
        "X": "Atol(float,optional) is used to compare atol(float,optional) to what?",
        "Y": "absolute tolerance",
        "Z": "This function checks if allinputandothersatisfy the condition: elementwise, for all elements ofinputandother. The behaviour of this function is analogous tonumpy.allclose input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08 rtol(float,optional) \u2013 relative tolerance. Default: 1e-05 equal_nan(bool,optional) \u2013 ifTrue, then twoNaNs will be considered equal. Default:False Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose"
    },
    {
        "X": "What is the default value of equal_nan(bool,optional)?",
        "Y": "False Example",
        "Z": "This function checks if allinputandothersatisfy the condition: elementwise, for all elements ofinputandother. The behaviour of this function is analogous tonumpy.allclose input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08 rtol(float,optional) \u2013 relative tolerance. Default: 1e-05 equal_nan(bool,optional) \u2013 ifTrue, then twoNaNs will be considered equal. Default:False Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose"
    },
    {
        "X": "How can a TorchScript program be saved from a Python process and loaded in a process where there is no Python dependency?",
        "Y": "Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency",
        "Z": "Mixing Tracing and Scripting TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In what language can a TorchScript program be run independently from Python?",
        "Y": "C++",
        "Z": "Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions",
        "Y": "Python Functions and Modules",
        "Z": "PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer?",
        "Y": "Python Language Reference Comparison Debugging",
        "Z": "PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the introduction to TorchScripttutorial?",
        "Y": "introduction to TorchScript",
        "Z": "Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What Python Functions and Modules?",
        "Y": "Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer",
        "Z": "Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Why is it possible to export a model via TorchScript to a production environment?",
        "Y": "Python programs may be disadvantageous for performance and multi-threading reasons",
        "Z": "Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API",
        "Y": "Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer",
        "Z": "Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In what language is it possible to train models in PyTorch using familiar tools?",
        "Y": "Python",
        "Z": "TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is one way to disable JIT for Debugging?",
        "Y": "Disable JIT for Debugging",
        "Z": "Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive",
        "Y": "Inspecting Code Interpreting Graphs Tracer",
        "Z": "Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What do we provide to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python",
        "Y": "tools",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is TorchScript a way to do from PyTorch code?",
        "Y": "create serializable and optimizable models",
        "Z": "Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References?",
        "Y": "Frequently Asked Questions",
        "Z": "Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "References TorchScript is a way to do what from PyTorch code?",
        "Y": "create serializable and optimizable models",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does TorchScript provide?",
        "Y": "tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python",
        "Z": "Creating TorchScript Code Mixing Tracing and Scripting TorchScript Language Built-in Functions and Modules PyTorch Functions and Modules Python Functions and Modules Python Language Reference Comparison Debugging Disable JIT for Debugging Inspecting Code Interpreting Graphs Tracer Frequently Asked Questions Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is TorchScript a way to do?",
        "Y": "create serializable and optimizable models from PyTorch code",
        "Z": "Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a benefit of using TorchScript?",
        "Y": "Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency",
        "Z": "TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does TorchScript do?",
        "Y": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution",
        "Z": "Known Issues Appendix Migrating to PyTorch 1.2 Recursive Scripting API References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the end-to-end example of converting a PyTorch model to TorchScript and running it in C++?",
        "Y": "theLoading a PyTorch Model in C++tutorial",
        "Z": "For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "A wrapper around C++torch::jit::Module. Functionally equivalent to what?",
        "Y": "aScriptModule",
        "Z": "For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Scripting a function ornn.Module compile as?",
        "Y": "TorchScript code",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Trace a module and return an executableScriptModulethat will be optimized using what?",
        "Y": "just-in-time compilation",
        "Z": "References TorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons. For a gentle introduction to TorchScript, see theIntroduction to TorchScripttutorial. For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see theLoading a PyTorch Model in C++tutorial.   Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of version of a script module can be saved for use in a separate process?",
        "Y": "offline",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Load aScriptModuleorScriptFunctionpreviously saved what?",
        "Y": "withtorch.jit.save",
        "Z": "Scripting a function ornn.Modulewill inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return aScriptModuleorScriptFunction.   Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.   Compilesfnwhen it is first called during tracing.   Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is used to optimize aScriptModule?",
        "Y": "just-in-time compilation",
        "Z": "Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.   Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of version of aScriptModule can be saved for use in a separate process?",
        "Y": "offline",
        "Z": "Creates an asynchronous task executingfuncand a reference to the value of the result of this execution.   Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Torch.jit.Future[T]asynchronous task do?",
        "Y": "Forces completion of atorch.jit.Future[T]asynchronous task",
        "Z": "Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Where can you save a version of aScriptModule for use in a separate process?",
        "Y": "offline",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the TorchScript function that provides for conatiner type refinement?",
        "Y": "a pass-through function that returnsvalue",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the TorchScript method that provides for conatiner type refinement?",
        "Y": "a pass-through function that returnsvalue",
        "Z": "Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What version of aScriptModule can you save for use in a separate process?",
        "Y": "offline",
        "Z": "Forces completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.   A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the left-hand expression used to indicate to the TorchScript compiler?",
        "Y": "a class instance attribute with type oftype",
        "Z": "Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does returnsthe_value hint TorchScript compiler?",
        "Y": "the type ofthe_value",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can call traced functions?",
        "Y": "Scripted functions",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What inside of a script function called by a traced function is preserved correctly?",
        "Y": "Control-flow",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a script function called by a traced function?",
        "Y": "a traced function",
        "Z": "A wrapper around C++torch::jit::Module.   Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.   Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph.   Save an offline version of this module for use in a separate process.   Load aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save   This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.   This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.   This function provides for conatiner type refinement in TorchScript.   This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.   This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value. In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a traced function particularly useful for?",
        "Y": "when you need to use control-flow around a simple feed-forward model",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can a scripted function call an encoder module generated using?",
        "Y": "tracing",
        "Z": "Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can be used to generate a submodule using tracing that can be called from the methods of a script module?",
        "Y": "fornn.Modules",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How can a submodule be called from the methods of a script module?",
        "Y": "using a traced module",
        "Z": "In many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model. Scripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing. Example (calling a traced function in script): Traced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly. Example (calling a script function in a traced function): This composition also works fornn.Modules as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module. Example (using a traced module):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Debugging what script works except for when we invoke the@torch.jit.scriptfunction?",
        "Y": "withpdb",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can we do to make the @torch.jit.scriptfunction a normal Python function and not compile it?",
        "Y": "globally disable JIT",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the above script?",
        "Y": "disable_jit_example.py",
        "Z": "Debugging this script withpdbworks except for when we invoke the@torch.jit.scriptfunction. We can globally disable\nJIT, so that we can call the@torch.jit.scriptfunction as a normal Python function and not compile it. If the above script\nis calleddisable_jit_example.py, we can invoke it like so: and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can we step into the@torch.jit.scriptfunction as?",
        "Y": "normal Python function",
        "Z": "and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To disable what for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-print",
        "Y": "TorchScript compiler",
        "Z": "and we will be able to step into the@torch.jit.scriptfunction as a normal Python function. To disable the\nTorchScript compiler for a specific function, see@torch.jit.ignore. TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does TorchScript provide for allScriptModuleinstances?",
        "Y": "a code pretty-printer",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can you use TorchScript's compilation of the code for theforwardmethod?",
        "Y": "to ensure TorchScript (tracing or scripting) has captured your model code correctly",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What produces the output of TorchScript's compilation of the code for theforwardmethod?",
        "Y": "The example above",
        "Z": "AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can you use this output to do?",
        "Y": "to ensure TorchScript (tracing or scripting) has captured your model code correctly",
        "Z": "AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of a static single assignment (SSA) intermediate representation?",
        "Y": "example",
        "Z": "AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is TorchScript's compilation of?",
        "Y": "the code for theforwardmethod",
        "Z": "This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly. TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What instruction produces the graph?",
        "Y": "test.py:9:10",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does TorchScript assign the output to?",
        "Y": "%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,",
        "Y": "test.py:9:10",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does %rv.1:Tensormeans we assign the output to?",
        "Y": "%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1",
        "Z": "%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What operator is equivalent to totorch.zeros?",
        "Y": "aten::zerosis the operator",
        "Z": "TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does graph follow the same rules described in theInspecting Codesection with regard to?",
        "Y": "forwardmethod lookup",
        "Z": "graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the script assign the output to a (unique) value namedrv.1?",
        "Y": "%rv.1:Tensormeans",
        "Z": "graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the operator (equivalent totorch.zeros)?",
        "Y": "aten::zeros",
        "Z": "TorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs. TorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example: graphfollows the same rules described in theInspecting Codesection\nwith regard toforwardmethod lookup. The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the unique value that we assign the output to?",
        "Y": "ofTensortype",
        "Z": "The example script above produces the graph: Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the file that generated the instruction%rv.1:Tensor=aten?",
        "Y": "test.py",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does assign the output to a (unique) value namedrv.1?",
        "Y": "%rv.1:Tensormeans",
        "Z": "%rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code?",
        "Y": "Tracing of control flow that is dependent on inputs",
        "Z": "Take the instruction%rv.1:Tensor=aten::zeros(%4,%6,%6,%10,%12)#test.py:9:10for\nexample. %rv.1:Tensormeans we assign the output to a (unique) value namedrv.1, that value is ofTensortype and that we do not know its concrete shape. aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of in-place operations of tensor views (e.g. what is on the left-hand side of an assignment)",
        "Y": "indexing",
        "Z": "#test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of in-place operations of tensor views (e.g. in-place operations) on the left-hand side of",
        "Y": "indexing",
        "Z": "aten::zerosis the operator (equivalent totorch.zeros) and the input list(%4,%6,%6,%10,%12)specifies which values in scope should be passed as inputs. The schema for built-in functions likeaten::zeroscan be found atBuiltin Functions. #test.py:9:10is the location in the original source file that generated this instruction. In this case, it is a file namedtest.py, on line 9, and at character 10. Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is thetorch.jit.trace()API.check_inputstakes?",
        "Y": "check_inputson",
        "Z": "Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is an example of an edge case where the trace of a given Python function/module will not be representative of the underlying code?",
        "Y": "example",
        "Z": "Notice that operators can also have associatedblocks, namely theprim::Loopandprim::Ifoperators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described\nby aScriptModuleis correct, in both automated and manual fashion, as\ndescribed below. There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the message indicate that the computation differed between when we first traced it and when we traced it?",
        "Y": "thecheck_inputs",
        "Z": "One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of in-place operations of tensor views (e.g. what on the left-hand side of an assignment) Note",
        "Y": "indexing",
        "Z": "Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the loop within the body ofloop_in_traced_fndepend on?",
        "Y": "the shape of the inputx",
        "Z": "There are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include: Tracing of control flow that is dependent on inputs (e.g. tensor shapes) Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Tracing of in-place operations of what?",
        "Y": "tensor views",
        "Z": "Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a list of inputs that will be used to re-trace the computation and verify the results?",
        "Y": "tuples",
        "Z": "Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In what time frame may these cases be traceable?",
        "Y": "future",
        "Z": "Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is a list of inputs that will be used to retrace the computation and verify the results?",
        "Y": "tuples",
        "Z": "One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does the list of tuples of inputs that will be used to re-trace the computation give us?",
        "Y": "diagnostic information",
        "Z": "One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor produce",
        "Y": "a graph",
        "Z": "One way to automatically catch many errors in traces is by usingcheck_inputson thetorch.jit.trace()API.check_inputstakes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "The message indicates that the computation differed between when we first traced it and when we traced it with what?",
        "Y": "check_inputs",
        "Z": "Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "The loop within the body ofloop_in_traced_fndepends on what of the inputx?",
        "Y": "shape",
        "Z": "Gives us the following diagnostic information: This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with thecheck_inputs. Indeed,\nthe loop within the body ofloop_in_traced_fndepends on the shape\nof the inputx, and thus when we try anotherxwith a different\nshape, the trace differs. In this case, data-dependent control flow like this can be captured usingtorch.jit.script()instead: Which produces: The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place withtorch.cat:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "The example above produces this output: This is TorchScript's compilation of the code for what?",
        "Y": "theforwardmethod",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What can you use the code pretty-printer for allScriptModuleinstances?",
        "Y": "to ensure TorchScript (tracing or scripting) has captured your model code correctly",
        "Z": "TorchScript provides a code pretty-printer for allScriptModuleinstances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example: AScriptModulewith a singleforwardmethod will have an attributecode, which you can use to inspect theScriptModule\u2019s code.\nIf theScriptModulehas more than one method, you will need to access.codeon the method itself and not the module. We can inspect the\ncode of a method namedfooon aScriptModuleby accessing.foo.code.\nThe example above produces this output: This is TorchScript\u2019s compilation of the code for theforwardmethod.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the first step in converting a model from GPU to CPU?",
        "Y": "convert your model from GPU to CPU and then save it",
        "Z": "First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What do you store on aScriptModule?",
        "Y": "attributes",
        "Z": "First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is equivalent to an attribute (see 4) of typeTensor?",
        "Y": "register_buffer",
        "Z": "First convert your model from GPU to CPU and then save it, like so: This is recommended because the tracer may witness tensor creation on a\nspecific device, so casting an already-loaded model may have unexpected\neffects. Casting the modelbeforesaving it ensures that the tracer has\nthe correct device information. Q: How do I store attributes on aScriptModule? Say we have a model like: IfModelis instantiated it will result in a compilation error\nsince the compiler doesn\u2019t know aboutx. There are 4 ways to inform the\ncompiler of attributes onScriptModule: 1.nn.Parameter- Values wrapped innn.Parameterwill work as they\ndo onnn.Modules 2.register_buffer- Values wrapped inregister_bufferwill work as\nthey do onnn.Modules. This is equivalent to an attribute (see 4) of typeTensor.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To compile a method other thanforward that is not called fromforward, what is done?",
        "Y": "add@torch.jit.export",
        "Z": "This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2. 1.torch.jit.scriptwill now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you calltorch.jit.script,\ncompilation is \u201copt-out\u201d, rather than \u201copt-in\u201d. 2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What do these changes combine to provide for converting yournn.Modules intoScriptModules?",
        "Y": "a simpler, easier-to-use API",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What module is compiled by default?",
        "Y": "module\u2019sforward",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How are methods called fromforward lazily compiled?",
        "Y": "in the order they are used inforward",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "To stop the compiler from compiling a method, what is done?",
        "Y": "add@torch.jit.ignoreor@torch.jit.unused",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @ignoreor do to a method?",
        "Y": "@ignoreleaves the method as a call to python",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What styleclass annotations are used for empty container types?",
        "Y": "PEP 526",
        "Z": "2.torch.jit.script(nn_module_instance)is now the preferred way to createScriptModules, instead of inheriting fromtorch.jit.ScriptModule.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyournn.Modules intoScriptModules, ready to be optimized and executed in a\nnon-Python environment. The new usage looks like this: The module\u2019sforwardis compiled by default. Methods called fromforwardare lazily compiled in the order they are used inforward. To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @ignoreor do to a method as a call to python?",
        "Y": "@ignoreleaves",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What type of types can be annotated using PEP 526-styleclass annotations?",
        "Y": "empty container types",
        "Z": "To compile a method other thanforwardthat is not called fromforward, add@torch.jit.export. To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How can empty container types be marked with aFinalclass annotation instead of adding the name of the member to__constants__?",
        "Y": "annotate their types usingPEP 526-styleclass annotations",
        "Z": "To stop the compiler from compiling a method, add@torch.jit.ignoreor@torch.jit.unused.@ignoreleaves the method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @unusedreplace a method with?",
        "Y": "an exception",
        "Z": "method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @torch.jit.ignore and @torch.jit.unused provide?",
        "Y": "details",
        "Z": "method as a call to python, and@unusedreplaces it with an exception.@ignoredcannot be exported;@unusedcan. Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is required for empty container types?",
        "Y": "annotate their types usingPEP 526-styleclass annotations",
        "Z": "Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does @torch.jit.ignore and @torch.jit.unused contain?",
        "Y": "details",
        "Z": "Most attribute types can be inferred, sotorch.jit.Attributeis not necessary. For empty container types, annotate their types usingPEP 526-styleclass annotations. Constants can be marked with aFinalclass annotation instead of adding the name of the member to__constants__. Python 3 type hints can be used in place oftorch.jit.annotate The@torch.jit.script_methoddecorator Classes that inherit fromtorch.jit.ScriptModule Thetorch.jit.Attributewrapper class The__constants__array Thetorch.jit.annotatefunction Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "In what version of Torch does the @torch.jit.ignoreannotation's behavior change?",
        "Y": "PyTorch 1.2",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What module's data is copied to aScriptModule when passed to thetorch.jit.scriptfunction?",
        "Y": "atorch.nn.Module",
        "Z": "When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How are methods called fromforward compiled?",
        "Y": "in the order they are used inforward",
        "Z": "The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How are functions and methods called fromforward compiled?",
        "Y": "as they are seen by the compiler",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method):",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "When does the @torch.jit.ignoreannotation's behavior change?",
        "Y": "PyTorch 1.2",
        "Z": "The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What is the name of the @ignore decorator used to make a function or method callable from code that is exported?",
        "Y": "@torch.jit.ignore",
        "Z": "The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "Everything in a user definedTorchScript Classis exported what?",
        "Y": "by default",
        "Z": "Warning The@torch.jit.ignoreannotation\u2019s behavior changes in\nPyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\nor method callable from code that is exported. To get this functionality back,\nuse@torch.jit.unused().@torch.jit.ignoreis now equivalent\nto@torch.jit.ignore(drop=False). See@torch.jit.ignoreand@torch.jit.unusedfor details. When passed to thetorch.jit.scriptfunction, atorch.nn.Module\u2019s data is\ncopied to aScriptModuleand the TorchScript compiler compiles the module.\nThe module\u2019sforwardis compiled by default. Methods called fromforwardare\nlazily compiled in the order they are used inforward, as well as any@torch.jit.exportmethods. This decorator indicates that a method on annn.Moduleis used as an entry point into aScriptModuleand should be compiled. forwardimplicitly is assumed to be an entry point, so it does not need this decorator.\nFunctions and methods called fromforwardare compiled as they are seen\nby the compiler, so they do not need this decorator either. Example (using@torch.jit.exporton a method): Functions don\u2019t change much, they can be decorated with@torch.jit.ignoreortorch.jit.unusedif needed. Warning TorchScript class support is experimental. Currently it is best suited\nfor simple record-like types (think aNamedTuplewith methods\nattached). Everything in a user definedTorchScript Classis\nexported by default, functions can be decorated with@torch.jit.ignoreif needed. The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API:",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What was Torch.jit.annotate used for?",
        "Y": "to tell the TorchScript compiler what the type should be",
        "Z": "The TorchScript compiler needs to know the types ofmodule attributes. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated withPEP 526-styleclass annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resultingScriptModule Old API: New API: TheFinaltype constructor can be used to mark members asconstant. If members are not marked constant, they will be copied to the resultingScriptModuleas an attribute. UsingFinalopens opportunities for optimization if the value is known to be fixed and gives additional type safety. Old API: New API: Containers are assumed to have typeTensorand be non-optional (seeDefault Typesfor more information). Previously,torch.jit.annotatewas used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "What does Einsum stand for?",
        "Y": "Sums the product of the elements of the inputoperandsalong dimensions",
        "Z": "Sums the product of the elements of the inputoperandsalong dimensions specified using a notation\nbased on the Einstein summation convention. Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given byequation. The details of\nthis format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of theoperandsalong the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation:",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summ",
        "Y": "Einsum",
        "Z": "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given byequation. The details of\nthis format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of theoperandsalong the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What is the general idea of Einsum?",
        "Y": "label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output",
        "Z": "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given byequation. The details of\nthis format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of theoperandsalong the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "How is the output computed?",
        "Y": "by summing the product of the elements of theoperandsalong the dimensions whose subscripts are not part of the output",
        "Z": "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given byequation. The details of\nthis format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of theoperandsalong the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B)?",
        "Y": "matrix multiplication",
        "Z": "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given byequation. The details of\nthis format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of theoperandsalong the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What is the summation subscript?",
        "Y": "j",
        "Z": "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given byequation. The details of\nthis format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of theoperandsalong the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What is the name of the Equation that allows multi-dimensional linear algebraic array operations?",
        "Y": "Equation",
        "Z": "Sums the product of the elements of the inputoperandsalong dimensions specified using a notation\nbased on the Einstein summation convention. Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them\nin a short-hand format based on the Einstein summation convention, given byequation. The details of\nthis format are described below, but the general idea is to label every dimension of the inputoperandswith some subscript and define which subscripts are part of the output. The output is then computed by summing\nthe product of the elements of theoperandsalong the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum astorch.einsum(\u201cij,jk->ik\u201d, A, B).\nHere, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation:",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What does Theequationstring separate subcripts for each operand by?",
        "Y": "a comma",
        "Z": "Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "The dimensions labeled with the same subscript must be what?",
        "Y": "broadcastable",
        "Z": "Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What is the exception?",
        "Y": "if a subscript is repeated for the same input operand",
        "Z": "Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What will be part of the output?",
        "Y": "The subscripts that appear exactly once in theequation",
        "Z": "Equation: Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What specifies the subscripts for each dimension of the inputoperands in the same order as the dimensions?",
        "Y": "Theequationstring",
        "Z": "Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What will be part of the output, sorted in increasing alphabetical order?",
        "Y": "The subscripts that appear exactly once in theequation",
        "Z": "Theequationstring specifies the subscripts (lower case letters[\u2018a\u2019, \u2018z\u2019]) for each dimension of\nthe inputoperandsin the same order as the dimensions, separating subcripts for each operand by a\ncomma (\u2018,\u2019), e.g.\u2018ij,jk\u2019specify subscripts for two 2D operands. The dimensions labeled with the same subscript\nmust be broadcastable, that is, their size must either match or be1. The exception is if a subscript is\nrepeated for the same input operand, in which case the dimensions labeled with this subscript for this operand\nmust match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that\nappear exactly once in theequationwill be part of the output, sorted in increasing alphabetical order.\nThe output is computed by multiplying the inputoperandselement-wise, with their dimensions aligned based\non the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation\nfollowed by the subscripts for the output. For instance, the following equation computes the transpose of a\nmatrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and\nat most once for the output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "What function returns the full window size?",
        "Y": "Bartlett window function",
        "Z": "Bartlett window function. whereNNNis the full window size. The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is the value of the Bartlett window function?",
        "Y": "full window size",
        "Z": "Bartlett window function. whereNNNis the full window size. The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is inputwindow_length?",
        "Y": "a positive integer",
        "Z": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is true when the returned window is used as a periodic window?",
        "Y": "ifperiodic",
        "Z": "Bartlett window function. whereNNNis the full window size. The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is the size of the returned window periodic(bool,optional)?",
        "Y": "window_length(int)",
        "Z": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What type of tensor types are supported?",
        "Y": "floating point types",
        "Z": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is the inputwindow_length a positive integer controlling the returned window size?",
        "Y": "whereNNNis the full window size",
        "Z": "whereNNNis the full window size. The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is true when the returned window is used as a periodic window with functions liketorch.stft()?",
        "Y": "ifperiodic",
        "Z": "whereNNNis the full window size. The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is the size of returned window periodic(bool,optional)?",
        "Y": "window_length(int)",
        "Z": "whereNNNis the full window size. The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "What is true when the returned window is ready to be used as a periodic window with functions liketorch.stft()?",
        "Y": "ifperiodic",
        "Z": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.bartlett_window(L,periodic=True)equal totorch.bartlett_window(L+1,periodic=False)[:-1]). Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window"
    },
    {
        "X": "The input tensor's data type must be what?",
        "Y": "floating point or complex type",
        "Z": "Returns the matrix norm or vector norm of a given tensor. Warning torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord)",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "For complex inputs, the norm is calculated using what?",
        "Y": "absolute value of each element",
        "Z": "Returns the matrix norm or vector norm of a given tensor. Warning torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. dim(int,tuple of python:ints,list of python:ints,optional) \u2013 Specifies which dimension or dimensions ofinputto\ncalculate the norm across. IfdimisNone, the norm will\nbe calculated across all dimensions ofinput. If the norm\ntype indicated bypdoes not support the specified number of\ndimensions, an error will occur. keepdim(bool,optional) \u2013 whether the output tensors havedimretained or not. Ignored ifdim=Noneandout=None. Default:False out(Tensor,optional) \u2013 the output tensor. Ignored ifdim=Noneandout=None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "If the input is complex and neitherdtypenoroutis specified, what will be the corresponding floating point type?",
        "Y": "the result\u2019s data type",
        "Z": "Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What does p(int,float,inf,-inf,'fro','nuc',optional) return",
        "Y": "the order of norm",
        "Z": "Returns the matrix norm or vector norm of a given tensor. Warning torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions. dim(int,tuple of python:ints,list of python:ints,optional) \u2013 Specifies which dimension or dimensions ofinputto\ncalculate the norm across. IfdimisNone, the norm will\nbe calculated across all dimensions ofinput. If the norm\ntype indicated bypdoes not support the specified number of\ndimensions, an error will occur. keepdim(bool,optional) \u2013 whether the output tensors havedimretained or not. Ignored ifdim=Noneandout=None. Default:False out(Tensor,optional) \u2013 the output tensor. Ignored ifdim=Noneandout=None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the default value for the order of norm?",
        "Y": "Default:'fro",
        "Z": "Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "The input tensor's data type must be either a what?",
        "Y": "floating point or complex type",
        "Z": "Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "p(int,float,inf,-inf,'fro','nuc',optional) \u2013 what",
        "Y": "the order of norm",
        "Z": "input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the default name for the order of norm?",
        "Y": "Default:'fro",
        "Z": "torch.norm is deprecated and may be removed in a future PyTorch release. Usetorch.linalg.norm(), instead, ortorch.linalg.vector_norm()when computing vector norms andtorch.linalg.matrix_norm()when\ncomputing matrix norms. Note, however, the signature for these functions\nis slightly different than the signature for torch.norm. input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord)",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "If input is complex and neitherdtypenoroutis specified, the result's data type will be what?",
        "Y": "corresponding floating point type",
        "Z": "input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is the name of the nuclear norm?",
        "Y": "Frobenius norm",
        "Z": "input(Tensor) \u2013 The input tensor. Its data type must be either a floating\npoint or complex type. For complex inputs, the norm is calculated using the\nabsolute value of each element. If the input is complex and neitherdtypenoroutis specified, the result\u2019s data type will\nbe the corresponding floating point type (e.g. float ifinputis\ncomplexfloat). p(int,float,inf,-inf,'fro','nuc',optional) \u2013 the order of norm. Default:'fro'The following norms can be calculated: ord matrix norm vector norm \u2019fro\u2019 Frobenius norm \u2013 \u2018nuc\u2019 nuclear norm \u2013 Number \u2013 sum(abs(x)**ord)**(1./ord) The vector norm can be calculated across any number of dimensions.\nThe corresponding dimensions ofinputare flattened into\none dimension, and the norm is calculated on the flattened\ndimension. Frobenius norm produces the same result asp=2in all cases\nexcept whendimis a list of three or more dims, in which\ncase Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "What is 111 Conv1,2,3D 111 Sigmoid 111 Tanh 53frac5335",
        "Y": "gain Linear / Identity",
        "Z": "gain Linear / Identity 111 Conv{1,2,3}D 111 Sigmoid 111 Tanh 53\\frac{5}{3}35\u200b ReLU 2\\sqrt{2}2\u200b Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b SELU 34\\frac{3}{4}43\u200b Warning In order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalisation\neffect for more stable gradient flow in rectangular layers. nonlinearity\u2013 the non-linear function (nn.functionalname) param\u2013 optional parameter for the non-linear function Examples Fills the input Tensor with values drawn from the uniform\ndistributionU(a,b)\\mathcal{U}(a, b)U(a,b). tensor\u2013 an n-dimensionaltorch.Tensor a\u2013 the lower bound of the uniform distribution b\u2013 the upper bound of the uniform distribution Examples",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    }
]