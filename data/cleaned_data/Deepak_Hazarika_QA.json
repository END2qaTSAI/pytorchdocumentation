[
    {
        "X": "In what platform do the key nn modules run?",
        "Y": "FP32",
        "Z": "torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What module implements versions of the key nn modules Conv2d() and Linear() which run in FP32 but with rounding",
        "Y": "torch.nn.quantized",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is needed for quantization aware training?",
        "Y": "fused operations",
        "Z": "This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Conv2d and Linear() use rounding to simulate the effect of what?",
        "Y": "INT8 quantization",
        "Z": "This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What module implements versions of the key nn modules Conv2d() and Linear()?",
        "Y": "torch.nn.quantized",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In what platform do the modules Conv2d() and Linear() run?",
        "Y": "FP32",
        "Z": "This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In what platform do Conv2d() and Linear() run?",
        "Y": "FP32",
        "Z": "This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does torch.nn.quantized implement the quantized versions of?",
        "Y": "nn layers",
        "Z": "torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the module that implements the quantized versions of the nn layers?",
        "Y": "quantized",
        "Z": "torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are some fused operations implemented by torch.nn.intrinsic.quantized?",
        "Y": "conv + relu",
        "Z": "torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What quantization does the rounding of the nn modules simulate?",
        "Y": "INT8",
        "Z": "torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is one of the quantized versions of the nn layers?",
        "Y": "torch.nn.Conv2d",
        "Z": "torch.quantization This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What module implements quantized versions of the nn layers?",
        "Y": "torch.nn.quantized.dynamic",
        "Z": "torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are some of the fused operations implemented by this module?",
        "Y": "conv + relu",
        "Z": "This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does torch.nn.quantized implement?",
        "Y": "nn layers",
        "Z": "This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does torch.nn.quantized.dynamic implement?",
        "Y": "Dynamically quantized Linear, LSTM, LSTMCell, GRUCell, and RNNCell",
        "Z": "This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are the quantized implementations of fused operations?",
        "Y": "conv + relu",
        "Z": "This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What quantization does the FP32 version of Conv2d and Linear() simulate?",
        "Y": "INT8",
        "Z": "This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is one of the quantized versions of nn layers?",
        "Y": "RNNCell",
        "Z": "torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Returns what if grad mode is currently enabled?",
        "Y": "True",
        "Z": "Returns True if grad mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_grad_enabled.html#torch.is_grad_enabled"
    },
    {
        "X": "What mode is currently enabled?",
        "Y": "grad mode",
        "Z": "Returns True if grad mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_grad_enabled.html#torch.is_grad_enabled"
    },
    {
        "X": "Returns True if what mode is currently enabled?",
        "Y": "grad mode",
        "Z": "Returns True if grad mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_grad_enabled.html#torch.is_grad_enabled"
    },
    {
        "X": "What is used to fake quantize a tensor?",
        "Y": "scale, zero_point, quant_min and quant_max",
        "Z": "Returns a new tensor with the data in input fake quantized using scale,\nzero_point, quant_min and quant_max. input (Tensor) \u2013 the input value(s), in torch.float32. scale (double) \u2013 quantization scale zero_point (int64) \u2013 quantization zero_point quant_min (int64) \u2013 lower bound of the quantized domain quant_max (int64) \u2013 upper bound of the quantized domain A newly fake_quantized tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine"
    },
    {
        "X": "What is the input value in?",
        "Y": "torch.float32",
        "Z": "Returns a new tensor with the data in input fake quantized using scale,\nzero_point, quant_min and quant_max. input (Tensor) \u2013 the input value(s), in torch.float32. scale (double) \u2013 quantization scale zero_point (int64) \u2013 quantization zero_point quant_min (int64) \u2013 lower bound of the quantized domain quant_max (int64) \u2013 upper bound of the quantized domain A newly fake_quantized tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine"
    },
    {
        "X": "What is the lower bound of?",
        "Y": "quantized domain quant_max",
        "Z": "Returns a new tensor with the data in input fake quantized using scale,\nzero_point, quant_min and quant_max. input (Tensor) \u2013 the input value(s), in torch.float32. scale (double) \u2013 quantization scale zero_point (int64) \u2013 quantization zero_point quant_min (int64) \u2013 lower bound of the quantized domain quant_max (int64) \u2013 upper bound of the quantized domain A newly fake_quantized tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine"
    },
    {
        "X": "What is the data in input?",
        "Y": "fake quantized",
        "Z": "Returns a new tensor with the data in input fake quantized using scale,\nzero_point, quant_min and quant_max. input (Tensor) \u2013 the input value(s), in torch.float32. scale (double) \u2013 quantization scale zero_point (int64) \u2013 quantization zero_point quant_min (int64) \u2013 lower bound of the quantized domain quant_max (int64) \u2013 upper bound of the quantized domain A newly fake_quantized tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine"
    },
    {
        "X": "What is the quantization scale zero_point?",
        "Y": "int64",
        "Z": "Returns a new tensor with the data in input fake quantized using scale,\nzero_point, quant_min and quant_max. input (Tensor) \u2013 the input value(s), in torch.float32. scale (double) \u2013 quantization scale zero_point (int64) \u2013 quantization zero_point quant_min (int64) \u2013 lower bound of the quantized domain quant_max (int64) \u2013 upper bound of the quantized domain A newly fake_quantized tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine"
    },
    {
        "X": "What does Bernoulli(p)textBernoulli(textttp)Bernoull",
        "Y": "an independent sample",
        "Z": "Fills each location of self with an independent sample from\nBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). self can have integral\ndtype. p should either be a scalar or tensor containing probabilities to be\nused for drawing the binary random number. If it is a tensor, the ith\\text{i}^{th}ith element of self tensor\nwill be set to a value sampled from\nBernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]})Bernoulli(p_tensor[i]). In this case p must have\nfloating point dtype. See also bernoulli() and torch.bernoulli()",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"
    },
    {
        "X": "What type of dtype can self have?",
        "Y": "integral dtype",
        "Z": "Fills each location of self with an independent sample from\nBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). self can have integral\ndtype. p should either be a scalar or tensor containing probabilities to be\nused for drawing the binary random number. If it is a tensor, the ith\\text{i}^{th}ith element of self tensor\nwill be set to a value sampled from\nBernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]})Bernoulli(p_tensor[i]). In this case p must have\nfloating point dtype. See also bernoulli() and torch.bernoulli()",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"
    },
    {
        "X": "What should either be a scalar or tensor containing probabilities to be used for drawing the binary random number?",
        "Y": "p",
        "Z": "p should either be a scalar or tensor containing probabilities to be\nused for drawing the binary random number. If it is a tensor, the ith\\text{i}^{th}ith element of self tensor\nwill be set to a value sampled from\nBernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]})Bernoulli(p_tensor[i]). In this case p must have\nfloating point dtype. See also bernoulli() and torch.bernoulli()",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"
    },
    {
        "X": "If p is what, the ithtextithith element of self tensor will be set to a",
        "Y": "a tensor",
        "Z": "Fills each location of self with an independent sample from\nBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). self can have integral\ndtype. p should either be a scalar or tensor containing probabilities to be\nused for drawing the binary random number. If it is a tensor, the ith\\text{i}^{th}ith element of self tensor\nwill be set to a value sampled from\nBernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]})Bernoulli(p_tensor[i]). In this case p must have\nfloating point dtype. See also bernoulli() and torch.bernoulli()",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"
    },
    {
        "X": "What type of dtype must p have?",
        "Y": "floating point dtype",
        "Z": "Fills each location of self with an independent sample from\nBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). self can have integral\ndtype. p should either be a scalar or tensor containing probabilities to be\nused for drawing the binary random number. If it is a tensor, the ith\\text{i}^{th}ith element of self tensor\nwill be set to a value sampled from\nBernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]})Bernoulli(p_tensor[i]). In this case p must have\nfloating point dtype. See also bernoulli() and torch.bernoulli()",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"
    },
    {
        "X": "What must p have in order for it to be a tensor?",
        "Y": "floating point dtype",
        "Z": "p should either be a scalar or tensor containing probabilities to be\nused for drawing the binary random number. If it is a tensor, the ith\\text{i}^{th}ith element of self tensor\nwill be set to a value sampled from\nBernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]})Bernoulli(p_tensor[i]). In this case p must have\nfloating point dtype. See also bernoulli() and torch.bernoulli()",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"
    },
    {
        "X": "What is another name for bernoulli()?",
        "Y": "torch.bernoulli()",
        "Z": "Fills each location of self with an independent sample from\nBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). self can have integral\ndtype. p should either be a scalar or tensor containing probabilities to be\nused for drawing the binary random number. If it is a tensor, the ith\\text{i}^{th}ith element of self tensor\nwill be set to a value sampled from\nBernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]})Bernoulli(p_tensor[i]). In this case p must have\nfloating point dtype. See also bernoulli() and torch.bernoulli()",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"
    },
    {
        "X": "Which matrices must be 3-D tensors each containing the same number of matrices?",
        "Y": "batch1 and batch2",
        "Z": "Performs a batch matrix-matrix product of matrices in batch1\nand batch2.\ninput is added to the final result. batch1 and batch2 must be 3-D tensors each containing the same\nnumber of matrices.",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "What is added to the final result?",
        "Y": "input",
        "Z": "Performs a batch matrix-matrix product of matrices in batch1\nand batch2.\ninput is added to the final result. batch1 and batch2 must be 3-D tensors each containing the same\nnumber of matrices.",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "What is used in batch1 and batch2 to perform a batch matrix-matrix product?",
        "Y": "matrices",
        "Z": "Performs a batch matrix-matrix product of matrices in batch1\nand batch2.\ninput is added to the final result. batch1 and batch2 must be 3-D tensors each containing the same\nnumber of matrices.",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "What is a (bnm)(b times n times m)(bn",
        "Y": "batch1",
        "Z": "If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m) tensor, batch2 is a\n(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p) tensor, then input must be\nbroadcastable with a\n(b\u00d7n\u00d7p)(b \\times n \\times p)(b\u00d7n\u00d7p) tensor and out will be a\n(b\u00d7n\u00d7p)(b \\times n \\times p)(b\u00d7n\u00d7p) tensor. Both alpha and beta mean the\nsame as the scaling factors used in torch.addbmm(). If beta is 0, then input will be ignored, and nan and inf in\nit will not be propagated.",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "What mean the same as the scaling factors used in torch.addbmm()?",
        "Y": "alpha and beta",
        "Z": "If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m) tensor, batch2 is a\n(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p) tensor, then input must be\nbroadcastable with a\n(b\u00d7n\u00d7p)(b \\times n \\times p)(b\u00d7n\u00d7p) tensor and out will be a\n(b\u00d7n\u00d7p)(b \\times n \\times p)(b\u00d7n\u00d7p) tensor. Both alpha and beta mean the\nsame as the scaling factors used in torch.addbmm(). If beta is 0, then input will be ignored, and nan and inf in\nit will not be propagated.",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "What will not be propagated if beta is zero?",
        "Y": "nan and inf",
        "Z": "If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m) tensor, batch2 is a\n(b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p) tensor, then input must be\nbroadcastable with a\n(b\u00d7n\u00d7p)(b \\times n \\times p)(b\u00d7n\u00d7p) tensor and out will be a\n(b\u00d7n\u00d7p)(b \\times n \\times p)(b\u00d7n\u00d7p) tensor. Both alpha and beta mean the\nsame as the scaling factors used in torch.addbmm(). If beta is 0, then input will be ignored, and nan and inf in\nit will not be propagated.",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "If input is ignored, input will be ignored and nan and inf in it will not be propagated?",
        "Y": "If beta is 0,",
        "Z": "If beta is 0, then input will be ignored, and nan and inf in\nit will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and\nalpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32. input (Tensor) \u2013 the tensor to be added batch1 (Tensor) \u2013 the first batch of matrices to be multiplied batch2 (Tensor) \u2013 the second batch of matrices to be multiplied beta (Number, optional) \u2013 multiplier for input (\u03b2\\beta\u03b2)",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be what?",
        "Y": "real numbers",
        "Z": "For inputs of type FloatTensor or DoubleTensor, arguments beta and\nalpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32. input (Tensor) \u2013 the tensor to be added batch1 (Tensor) \u2013 the first batch of matrices to be multiplied batch2 (Tensor) \u2013 the second batch of matrices to be multiplied beta (Number, optional) \u2013 multiplier for input (\u03b2\\beta\u03b2) alpha (Number, optional) \u2013 multiplier for batch1@batch2\\text{batch1} \\mathbin{@} \\text{batch2}batch1@batch2 (\u03b1\\alpha\u03b1)",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "Which operator supports inputs of type FloatTensor or DoubleTensor?",
        "Y": "TensorFloat32",
        "Z": "If beta is 0, then input will be ignored, and nan and inf in\nit will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and\nalpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32. input (Tensor) \u2013 the tensor to be added batch1 (Tensor) \u2013 the first batch of matrices to be multiplied batch2 (Tensor) \u2013 the second batch of matrices to be multiplied beta (Number, optional) \u2013 multiplier for input (\u03b2\\beta\u03b2)",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "What will be ignored if beta is 0?",
        "Y": "input",
        "Z": "If beta is 0, then input will be ignored, and nan and inf in\nit will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and\nalpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32. input (Tensor) \u2013 the tensor to be added batch1 (Tensor) \u2013 the first batch of matrices to be multiplied batch2 (Tensor) \u2013 the second batch of matrices to be multiplied beta (Number, optional) \u2013 multiplier for input (\u03b2\\beta\u03b2)",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "What does the operator support for inputs of type FloatTensor or DoubleTensor?",
        "Y": "TensorFloat32",
        "Z": "If beta is 0, then input will be ignored, and nan and inf in\nit will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and\nalpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32. input (Tensor) \u2013 the tensor to be added batch1 (Tensor) \u2013 the first batch of matrices to be multiplied batch2 (Tensor) \u2013 the second batch of matrices to be multiplied beta (Number, optional) \u2013 multiplier for input (\u03b2\\beta\u03b2)",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "What is the first batch of to be multiplied?",
        "Y": "matrices",
        "Z": "For inputs of type FloatTensor or DoubleTensor, arguments beta and\nalpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32. input (Tensor) \u2013 the tensor to be added batch1 (Tensor) \u2013 the first batch of matrices to be multiplied batch2 (Tensor) \u2013 the second batch of matrices to be multiplied beta (Number, optional) \u2013 multiplier for input (\u03b2\\beta\u03b2) alpha (Number, optional) \u2013 multiplier for batch1@batch2\\text{batch1} \\mathbin{@} \\text{batch2}batch1@batch2 (\u03b1\\alpha\u03b1)",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "What happens if all elements in input evaluate to True?",
        "Y": "Tests",
        "Z": "Tests if all elements in input evaluate to True. Note This function matches the behaviour of NumPy in returning\noutput of dtype bool for all supported dtypes except uint8.\nFor uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "This function matches the behavior of what function in returning output of dtype bool?",
        "Y": "NumPy",
        "Z": "Note This function matches the behaviour of NumPy in returning\noutput of dtype bool for all supported dtypes except uint8.\nFor uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "For what dtype is the dtype of output uint8 itself?",
        "Y": "uint8",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtype bool for all supported dtypes except uint8.\nFor uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "For each row of input in the given dimension dim, what returns if all elements in the row evaluate to True?",
        "Y": "True",
        "Z": "Tests if all elements in input evaluate to True. Note This function matches the behaviour of NumPy in returning\noutput of dtype bool for all supported dtypes except uint8.\nFor uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "This function matches the behaviour of what function in returning output of dtype bool for all supported dtypes except uint8",
        "Y": "NumPy",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtype bool for all supported dtypes except uint8.\nFor uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "For each row of input in the given dimension dim, returns what if all elements in the row evaluate to True?",
        "Y": "True",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtype bool for all supported dtypes except uint8.\nFor uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "For which dtype is the dtype of output uint8 itself?",
        "Y": "uint8",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtype bool for all supported dtypes except uint8.\nFor uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "For each row of input in a dimension dim, what returns if all elements in the row evaluate to True and False otherwise?",
        "Y": "True",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtype bool for all supported dtypes except uint8.\nFor uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "Which function returns output of dtype bool for all supported dtypes except uint8?",
        "Y": "NumPy",
        "Z": "This function matches the behaviour of NumPy in returning\noutput of dtype bool for all supported dtypes except uint8.\nFor uint8 the dtype of output is uint8 itself. Example: For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "If all elements in the row evaluate to True, what does keepdim return?",
        "Y": "True",
        "Z": "Example: For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "If keepdim is True, the output tensor is of what size?",
        "Y": "size 1",
        "Z": "For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "If dim is squeezed, the output tensor has how many dimensions less than input?",
        "Y": "1",
        "Z": "For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "For each row of input in the given dimension dim, returns what if all elements in the row evaluate to True and False otherwise?",
        "Y": "True",
        "Z": "For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "If keepdim is True, what is of the same size as input?",
        "Y": "the output tensor",
        "Z": "If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "What does dim (int) represent?",
        "Y": "the dimension to reduce",
        "Z": "For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "For each row of input in a given dimension dim, what returns if all elements in the row evaluate to True?",
        "Y": "True",
        "Z": "For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "If keepdim is True, what is of the same size as input except in the dimension dim where it is of size 1?",
        "Y": "the output tensor",
        "Z": "For each row of input in the given dimension dim,\nreturns True if all elements in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "If keepdim is True, the output tensor has how much less dimension than input?",
        "Y": "1",
        "Z": "If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "What is the output tensor called?",
        "Y": "out",
        "Z": "If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "Out (Tensor, optional) \u2013 the output tensor. What is the output tensor?",
        "Y": "Example",
        "Z": "If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "Where can you find more information on TensorBoard?",
        "Y": "https://www.tensorflow.org/tensorboard/",
        "Z": "Before going further, more details on TensorBoard can be found at\nhttps://www.tensorflow.org/tensorboard/ Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What are all supported for PyTorch models and tensors?",
        "Y": "Scalars, images, histograms, graphs, and embedding visualizations",
        "Z": "Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs. The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What do these utilities let you log into a directory for visualization within the TensorBoard UI?",
        "Y": "PyTorch models and metrics",
        "Z": "Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs. The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What are all supported for PyTorch models and tensors as well as Caffe2 nets and blobs?",
        "Y": "Scalars, images, histograms, graphs, and embedding visualizations",
        "Z": "Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs. The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What class is your main entry to log data for consumption and visualization by TensorBoard?",
        "Y": "SummaryWriter",
        "Z": "The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the SummaryWriter class used for?",
        "Y": "example",
        "Z": "Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs. The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What should a TensorBoard be?",
        "Y": "installable and runnable",
        "Z": "The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "The SummaryWriter class can be visualized with what?",
        "Y": "TensorBoard",
        "Z": "The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What can be visualized with?",
        "Y": "TensorBoard",
        "Z": "This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What can we do to avoid cluttering the UI and have better result clustering?",
        "Y": "group plots",
        "Z": "Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs. The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What will be grouped together in the TensorBoard interface?",
        "Y": "Loss/train",
        "Z": "Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does TensorBoard group plots according to?",
        "Y": "Expected result",
        "Z": "This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What can be logged for one experiment?",
        "Y": "Lots of information",
        "Z": "Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How can we avoid cluttering the UI and have better result clustering?",
        "Y": "group plots",
        "Z": "Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "In what interface will Accuracy/train and Accuracy/test be grouped separately?",
        "Y": "TensorBoard",
        "Z": "Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the expected result?",
        "Y": "Add image data to summary",
        "Z": "global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the expected result of the log_dir to be consumed by TensorBoard?",
        "Y": "Writes entries directly to event files",
        "Z": "Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Writes entries directly to event files in the log_dir to be consumed by what?",
        "Y": "TensorBoard",
        "Z": "Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does the SummaryWriter class provide?",
        "Y": "a high-level API",
        "Z": "The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How does the SummaryWriter update the file contents?",
        "Y": "asynchronously",
        "Z": "Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does the asynchronous update allow?",
        "Y": "a training program to call methods to add data to the file directly from the training loop",
        "Z": "Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does the SummaryWriter write to the event file?",
        "Y": "events and summaries",
        "Z": "The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does the SummaryWriter class do?",
        "Y": "updates the file contents asynchronously",
        "Z": "Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What class provides a high-level API to create an event file in a given directory and add summaries and events to it?",
        "Y": "SummaryWriter",
        "Z": "Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How does the SummaryWriter class allow a training program to call methods to add data to the file directly from the training loop?",
        "Y": "without slowing down training",
        "Z": "The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How does the SummaryWriter class help a training program?",
        "Y": "without slowing down training",
        "Z": "Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How does the SummaryWriter class update the file contents?",
        "Y": "asynchronously",
        "Z": "The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What suffix is appended to the default log_dir?",
        "Y": "log_dir",
        "Z": "comment (string) \u2013 Comment log_dir suffix appended to the default\nlog_dir. If log_dir is assigned, this argument has no effect. purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "If log_dir is assigned, what effect does this argument have?",
        "Y": "no effect",
        "Z": "comment (string) \u2013 Comment log_dir suffix appended to the default\nlog_dir. If log_dir is assigned, this argument has no effect. purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Where will any events whose global_step is larger or equal to TTT be purged and hidden?",
        "Y": "TensorBoard",
        "Z": "comment (string) \u2013 Comment log_dir suffix appended to the default\nlog_dir. If log_dir is assigned, this argument has no effect. purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What should crashed and resumed experiments have the same?",
        "Y": "log_dir",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "When does this argument have no effect?",
        "Y": "If log_dir is assigned",
        "Z": "comment (string) \u2013 Comment log_dir suffix appended to the default\nlog_dir. If log_dir is assigned, this argument has no effect. purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "When logging crashes at step T+XT+XT+X and restarts at step TTT, events whose global_step is larger",
        "Y": "purged and hidden",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What should have the same log_dir?",
        "Y": "crashed and resumed experiments",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "When logging crashes at T+XT+XT+XT+X and restarts at TTT, events whose global_step is",
        "Y": "TensorBoard",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the size of the queue for pending events and summaries before one of the \u2018add\u2019 calls forces a flush to disk?",
        "Y": "max_queue",
        "Z": "max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the default size of the queue for pending events and summaries before one of the \u2018add\u2019 calls forces a flush to disk?",
        "Y": "ten items",
        "Z": "max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "When logging crashes at TTT and restarts at TTT, events whose global_step is larger or equal to TTT will be what?",
        "Y": "purged",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How often do you flush the pending events and summaries to disk?",
        "Y": "flush_secs",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the default time to flush pending events and summaries to disk?",
        "Y": "every two minutes",
        "Z": "flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "In what directory is a filename suffix added to all event filenames?",
        "Y": "log_dir",
        "Z": "max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Where can you find more details on filename construction?",
        "Y": "tensorboard.summary.writer.event_file_writer.EventFileWriter",
        "Z": "filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What type of data can be added to a summary?",
        "Y": "scalar data",
        "Z": "max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How often to flush the pending events and summaries to disk?",
        "Y": "flush_secs",
        "Z": "flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is added to all event filenames in the log_dir directory?",
        "Y": "Suffix",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "More details on filename construction can be found in what?",
        "Y": "tensorboard.summary.writer.event_file_writer.EventFileWriter",
        "Z": "filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What type of data can be added to summary?",
        "Y": "scalar data",
        "Z": "comment (string) \u2013 Comment log_dir suffix appended to the default\nlog_dir. If log_dir is assigned, this argument has no effect. purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is an example of how to add scalar data to summary?",
        "Y": "Add scalar data to summary",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does global_step (int) represent?",
        "Y": "Global step value",
        "Z": "flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What can be added to summary?",
        "Y": "Add scalar data",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the value to save?",
        "Y": "global_step",
        "Z": "flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is another name for scalar_value?",
        "Y": "float",
        "Z": "scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Filename_suffix (string) \u2013 Suffix added to what?",
        "Y": "all event filenames in the log_dir directory",
        "Z": "filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the data identifier scalar_value?",
        "Y": "float",
        "Z": "flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is an example of a way to add scalar data to summary?",
        "Y": "Add scalar data to summary",
        "Z": "Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What could a new style lead to?",
        "Y": "faster data loading",
        "Z": "global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the expected result of adding scalar data to summary?",
        "Y": "Adds many scalar data to summary",
        "Z": "Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is added to summary?",
        "Y": "image data",
        "Z": "Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "New style could lead to what?",
        "Y": "faster data loading",
        "Z": "new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the expected result of adding new style to summary?",
        "Y": "Adds many scalar data",
        "Z": "Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the purpose of adding scalar data to summary?",
        "Y": "Add scalar data to summary",
        "Z": "Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does add scalar data to summary?",
        "Y": "Add scalar data",
        "Z": "Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the expected result of a new style?",
        "Y": "Adds many scalar data to summary",
        "Z": "scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the parent name for the tags?",
        "Y": "tag_scalar_dict",
        "Z": "main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Data identifier scalar_value (what) \u2013 Value to save global_step (int) \u2013 Global step value to",
        "Y": "float or string/blobname",
        "Z": "flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "New style could lead to faster data loading.",
        "Y": "Adds many scalar data to summary",
        "Z": "new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "scalar_value (what is another name for scalar value) \u2013 Value to save global_step (int)",
        "Y": "float or string/blobname",
        "Z": "scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is another name for global step value to record walltime?",
        "Y": "float",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "global_step (int) \u2013 Global step value to record walltime",
        "Y": "float",
        "Z": "global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the expected result of using new style?",
        "Y": "Adds many scalar data to summary",
        "Z": "new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the parent name for the tags tag_scalar_dict?",
        "Y": "main_tag",
        "Z": "global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is another name for walltime?",
        "Y": "float",
        "Z": "tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the global step value to record?",
        "Y": "Image data global_step",
        "Z": "tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "walltime (what is another name for walltime) \u2013 Optional override default walltime (time.time()) with seconds after",
        "Y": "float",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global",
        "Y": "main_tag",
        "Z": "scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the name of the tensor field?",
        "Y": "new_style",
        "Z": "new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the global step value to record walltime?",
        "Y": "float",
        "Z": "Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the global step value to record bins?",
        "Y": "global_step",
        "Z": "values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does the global step value to record bins determine?",
        "Y": "how the bins are made",
        "Z": "Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Optional override default walltime (time.time()) seconds after epoch of event",
        "Y": "walltime",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is tag?",
        "Y": "string",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What determines the global step value to record bins?",
        "Y": "how the bins are made",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How long after epoch of event is the default walltime?",
        "Y": "seconds after epoch of event",
        "Z": "Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "When is the default walltime overridden?",
        "Y": "seconds after epoch of event",
        "Z": "Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is expected to be added to summary?",
        "Y": "batched image data",
        "Z": "bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Add video data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What value is used to build a histogram?",
        "Y": "global_step",
        "Z": "scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Optional override default walltime (time.time()) seconds after epoch of event?",
        "Y": "walltime",
        "Z": "values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is one way to add a histogram to a summary?",
        "Y": "Add histogram to summary",
        "Z": "Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does global_step determine?",
        "Y": "how the bins are made",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the name of the value to build a histogram?",
        "Y": "global_step",
        "Z": "Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the expected result of using walltime?",
        "Y": "Add image data to summary",
        "Z": "bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Add video data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does this determine?",
        "Y": "how the bins are made",
        "Z": "bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Add video data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What determines how the bins are made?",
        "Y": "values",
        "Z": "values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does adding image data to summary require?",
        "Y": "pillow package",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does tag (string) contain?",
        "Y": "Data identifier img_tensor",
        "Z": "bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Add video data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Add image data to summary. Note that this requires what?",
        "Y": "pillow package",
        "Z": "bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Tag (string) \u2013 What does tag stand for?",
        "Y": "Data identifier",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What determines bins?",
        "Y": "how the bins are made",
        "Z": "bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Tag (string) \u2013 Data identifier what?",
        "Y": "img_tensor",
        "Z": "bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is required to add image data to summary?",
        "Y": "pillow package",
        "Z": "bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Add video data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the expected result of overriding default walltime (time.time()) seconds after epoch of event?",
        "Y": "Add image data",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())",
        "Y": "global_step",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the expected result of adding image data to summary?",
        "Y": "Add image data to summary",
        "Z": "Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the Data identifier?",
        "Y": "img_tensor",
        "Z": "Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does the pillow package require?",
        "Y": "Add image data to summary",
        "Z": "Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What do you need to add to summary?",
        "Y": "Add image data",
        "Z": "Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the data identifier?",
        "Y": "img_tensor",
        "Z": "values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the value of a one-dimensional tensor of size steps whose values are evenly spaced from start to end?",
        "Y": "Warning Not providing a value for steps is deprecated",
        "Z": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from start to end, inclusive. That is, the value are: Warning Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "For backwards compatibility, not providing a value for steps will create a tensor with how many elements?",
        "Y": "100 elements",
        "Z": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from start to end, inclusive. That is, the value are: Warning Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "Is not providing a value for steps reflected in the documented function signature?",
        "Y": "not",
        "Z": "Warning Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "In a future PyTorch release, failing to provide a value for steps will throw what?",
        "Y": "runtime error",
        "Z": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from start to end, inclusive. That is, the value are: Warning Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What type of tensor of size steps does PyTorch create?",
        "Y": "one-dimensional",
        "Z": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from start to end, inclusive. That is, the value are: Warning Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "For what reason will not providing a value for steps create a tensor with 100 elements?",
        "Y": "backwards compatibility",
        "Z": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from start to end, inclusive. That is, the value are: Warning Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "Why is not providing a value for steps deprecated?",
        "Y": "not reflected in the documented function signature",
        "Z": "Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "For backwards compatibility, not providing a value for steps will create what with 100 elements?",
        "Y": "tensor",
        "Z": "Warning Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "Is this behavior reflected in the documented function signature?",
        "Y": "not",
        "Z": "Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "For backwards compatibility, not providing a value for steps will create what?",
        "Y": "a tensor with 100 elements",
        "Z": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from start to end, inclusive. That is, the value are: Warning Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What is the starting value for the set of points end (float)?",
        "Y": "start (float)",
        "Z": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from start to end, inclusive. That is, the value are: Warning Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What is the size of the constructed tensor out?",
        "Y": "the output tensor",
        "Z": "start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What uses the global default dtype when both start and end are real?",
        "Y": "if None",
        "Z": "start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What is start (float)?",
        "Y": "the starting value for the set of points end",
        "Z": "start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "dtype (torch.dpython:type, optional) \u2013 what to perform the computation in?",
        "Y": "the data type",
        "Z": "dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What does torch.get_default_dtype() use when both start and end are real?",
        "Y": "global default dtype",
        "Z": "start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What default uses the global default dtype when both start and end are real?",
        "Y": "if None",
        "Z": "start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What is the ending value for the set of points steps?",
        "Y": "end",
        "Z": "end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What type of dtype is used when both start and end are real?",
        "Y": "Default",
        "Z": "Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What does torch.autograd implement?",
        "Y": "automatic differentiation of arbitrary scalar valued functions",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What keyword is used to declare Tensor s for which gradients should be computed?",
        "Y": "requires_grad=True",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does torch.autograd only support for floating point Tensor types?",
        "Y": "autograd",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Computes the sum of gradients of given tensors with respect to what?",
        "Y": "graph leaves",
        "Z": "Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "As of now, we only support autograd for what?",
        "Y": "floating point Tensor types",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does autograd compute?",
        "Y": "the sum of gradients of given tensors with respect to graph leaves",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Computes and returns the sum of gradients of outputs with respect to what?",
        "Y": "inputs",
        "Z": "Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Warning This API is in what state?",
        "Y": "beta",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is unlikely to change in the beta version of the autograd API?",
        "Y": "function signatures",
        "Z": "This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the higher level API for?",
        "Y": "autograd",
        "Z": "This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Computes and returns what?",
        "Y": "the sum of gradients of outputs with respect to the inputs",
        "Z": "Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What beta is this API in?",
        "Y": "beta",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is planned before we consider this stable?",
        "Y": "major improvements to performances",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does the higher level API for the autograd allow you to compute?",
        "Y": "jacobians, hessians",
        "Z": "Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the function that returns the sum of gradients of outputs with respect to the inputs?",
        "Y": "Computes",
        "Z": "Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is computed and returned with respect to the inputs?",
        "Y": "the sum of gradients of outputs",
        "Z": "Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does this section contain for the autograd that builds on the basic API above and allows you to compute jacobians, hessians",
        "Y": "higher level API",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the current state of the autograd API?",
        "Y": "beta",
        "Z": "This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does this section contain for the autograd?",
        "Y": "higher level API",
        "Z": "This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What level of API does this section contain?",
        "Y": "higher level",
        "Z": "This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does the higher level API for autograd allow you to compute?",
        "Y": "jacobians, hessians",
        "Z": "This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does a functional.jacobian Function compute?",
        "Y": "Jacobian",
        "Z": "functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does a functional.hessian Function compute of a given scalar function?",
        "Y": "Hessian",
        "Z": "functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does a functional.vjp Function compute?",
        "Y": "the dot product between a vector v and the Jacobian of the given function at the point given by the inputs",
        "Z": "functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What computes the dot product between the Jacobian of a given function at the point given by the inputs and a vector v",
        "Y": "functional.jvp Function",
        "Z": "Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What computes the Jacobian of a given function?",
        "Y": "functional.jacobian Function",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does functional.vjp compute between a vector v and the Jacobian of the given function at the point given by the inputs",
        "Y": "the dot product",
        "Z": "functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs?",
        "Y": "functional.vjp Function",
        "Z": "functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What function computes the dot product between the Jacobian of a given function at the point given by the inputs and a vector",
        "Y": "functional.jvp",
        "Z": "Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does a function compute of a given function?",
        "Y": "Jacobian",
        "Z": "Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Function that computes the what of a given function. functional.hessian Function that computes the Hessian of a given s",
        "Y": "Jacobian",
        "Z": "Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Function that computes the Hessian of a given what?",
        "Y": "scalar function",
        "Z": "Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What function computes the Hessian of a given scalar function?",
        "Y": "functional.hessian",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Where does functional.vjp compute the dot product between a vector v and the Jacobian of a given function?",
        "Y": "the point given by the inputs",
        "Z": "Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What function computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs",
        "Y": "function",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is a functional.hessian Function that computes the Hessian of?",
        "Y": "scalar function",
        "Z": "functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Functional.vjp Function that computes the dot product between a vector v and what?",
        "Y": "the Jacobian",
        "Z": "functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What function computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v",
        "Y": "functional.jvp",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does a function compute of a given scalar function?",
        "Y": "Hessian",
        "Z": "Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What computes the dot product between a vector v and the Jacobian of a given function at the point given by the inputs",
        "Y": "functional.vjp Function",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Function that computes what of a given scalar function?",
        "Y": "Hessian",
        "Z": "Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Functional.vjp Function that computes the dot product between a vector v and what of the given function at the point given by the",
        "Y": "the Jacobian",
        "Z": "Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What function computes the dot product between a vector v and the Hessian of a given scalar function at the point",
        "Y": "functional.vhp",
        "Z": "Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the function that computes the dot product between a vector v and the Hessian of a given s",
        "Y": "functional.hvp",
        "Z": "functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Where does a function compute the dot product between a vector v and the Jacobian of a given function?",
        "Y": "the point given by the inputs",
        "Z": "Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Function that computes the dot product between a vector v and what of the given function at the point given by the inputs?",
        "Y": "the Jacobian",
        "Z": "Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does a functional.hvp Function compute the dot product between?",
        "Y": "the Hessian of a given scalar function and a vector v at the point given by the inputs",
        "Z": "functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What computes the dot product between a vector v and the Hessian of a given scalar function at the point given",
        "Y": "functional.vhp Function",
        "Z": "functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What computes the dot product between the Hessian of a given scalar function and a vector v at the point given",
        "Y": "functional.hvp Function",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the dot product between a given function at the point given by the inputs and a vector v?",
        "Y": "Jacobian",
        "Z": "Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What function computes the dot product between the Hessian of a given scalar function and a vector v at the point",
        "Y": "functional.hvp",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is another name for the difference between no-grad and inference mode?",
        "Y": "Locally disabling gradient computation",
        "Z": "functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the function that computes the dot product between a vector v and the Hessian of a given scalar?",
        "Y": "function",
        "Z": "Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Where is the dot product between a vector v and the Hessian of a given scalar function computed?",
        "Y": "the point given by the inputs",
        "Z": "Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the difference between no-grad and inference mode?",
        "Y": "Locally disabling gradient computation",
        "Z": "See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does context-manager do?",
        "Y": "disabled gradient calculation",
        "Z": "functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does functional.hvp function do?",
        "Y": "computes the dot product between the Hessian of a given scalar function and a vector v",
        "Z": "functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is more information on the differences between no-grad and inference mode?",
        "Y": "Locally disabling gradient computation",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the program that disables gradient calculation?",
        "Y": "Context-manager",
        "Z": "functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the dot product between a given scalar function and a vector v?",
        "Y": "Hessian",
        "Z": "functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is a function that computes the dot product between the Hessian of a given scalar function and a vector",
        "Y": "computes the dot product between the Hessian of a given scalar function and a vector v",
        "Z": "Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the entity that disables gradient calculation?",
        "Y": "Context-manager",
        "Z": "See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does a context-manager set gradient calculation to?",
        "Y": "on or off",
        "Z": "See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What mode does a context-manager enable or disable?",
        "Y": "inference mode",
        "Z": "The context managers torch.no_grad(), torch.enable_grad(), and\ntorch.set_grad_enabled() are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the threading module, etc. Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does a context-manager enable or disable when a non-sparse param receives a non-sparse",
        "Y": "inference mode",
        "Z": "Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the initial value of param.grad?",
        "Y": "None",
        "Z": "functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the entity that disabled gradient calculation?",
        "Y": "Context-manager",
        "Z": "See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does a Context-manager that disables gradient calculation do?",
        "Y": "enables gradient calculation",
        "Z": "Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "When does a non-sparse param receive a non-sparse gradient?",
        "Y": "torch.autograd.backward() or torch.Tensor.backward()",
        "Z": "When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param.grad is initially what?",
        "Y": "None",
        "Z": "Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does context-manager enable?",
        "Y": "gradient calculation",
        "Z": "Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param's memory is non-overlapping and dense,.grad is created with what?",
        "Y": "strides matching param",
        "Z": "If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True. Supporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Context-manager that sets gradient calculation to what?",
        "Y": "on or off",
        "Z": "Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param.grad is initially None, what happens to param.grad?",
        "Y": "If param.grad is initially None",
        "Z": "See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does context-manager enable or disable when a non-sparse param receives a non-sparse gradient?",
        "Y": "inference mode",
        "Z": "Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is initial None if a non-sparse param receives a non-sparse gradient?",
        "Y": "param",
        "Z": "Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param.grad is initially None,.grad is created with what strides?",
        "Y": "rowmajor-contiguous",
        "Z": "If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True. Supporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param\u2019s memory is non-overlapping and dense, what is.grad created with?",
        "Y": "rowmajor-contiguous strides",
        "Z": "Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param.grad is initially None,.grad is created with strides matching param (thus matching param\u2019s layout)?",
        "Y": "param\u2019s memory is non-overlapping and dense",
        "Z": "When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param.grad is initially None, what is.grad created with?",
        "Y": "rowmajor-contiguous strides",
        "Z": "When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What type of attribute does param already have?",
        "Y": "non-sparse",
        "Z": "When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param already has a non-sparse attribute, what attribute is created?",
        "Y": ".grad attribute",
        "Z": "Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param.grad is initially None, what strides are created with?",
        "Y": "rowmajor-contiguous",
        "Z": "When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What happens when a non-sparse param receives a non-sparse gradient?",
        "Y": "If param.grad is initially None",
        "Z": "When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is.grad created with?",
        "Y": "rowmajor-contiguous strides",
        "Z": "If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param.grad is initially None,.grad is created with strides matching param?",
        "Y": "param\u2019s memory is non-overlapping and dense",
        "Z": "When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param's memory is dense and non-overlapping, what strides are used to create.grad?",
        "Y": "rowmajor-contiguous",
        "Z": "If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True. Supporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What accumulates into.grad in-place if create_graph=False?",
        "Y": "backward()",
        "Z": "See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param.grad is initially None, what causes.grad to be created with strides matching param?",
        "Y": "If param\u2019s memory is non-overlapping and dense",
        "Z": "If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If create_graph=False, backward() accumulates into what?",
        "Y": "in-place",
        "Z": "Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If the memory of what is dense and non-overlapping,.grad is created with strides matching param?",
        "Y": "param",
        "Z": "If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does backward() replace.grad with?",
        "Y": "tensor",
        "Z": "When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param already has a non-sparse.grad attribute, backward() accumulates into.grad in-place,",
        "Y": "rowmajor-contiguous",
        "Z": "Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What already has a non-sparse.grad attribute?",
        "Y": "param",
        "Z": "If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Where does backward() accumulate into.grad?",
        "Y": "in-place",
        "Z": "If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What replaces.grad with a new tensor.grad + new grad?",
        "Y": "backward()",
        "Z": "functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the default behavior of letting.grads be before the first backward()?",
        "Y": "None",
        "Z": "When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What will not affect.grad layouts?",
        "Y": "Calls to model.zero_grad() or optimizer.zero_grad()",
        "Z": "When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does backward() replace with a new tensor.grad + new grad?",
        "Y": ".grad",
        "Z": "If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the default behavior for letting.grads be None before the first backward()?",
        "Y": "their layout is created according to 1 or 2",
        "Z": "If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Which calls will not affect.grad layouts?",
        "Y": "model.zero_grad() or optimizer.zero_grad()",
        "Z": "If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is a valid alternative to model.zero_grad() or optimizer.zero_grad()?",
        "Y": "None",
        "Z": "Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Resetting all.grads to None before each accumulation phase is a valid alternative to what?",
        "Y": "model.zero_grad() or optimizer.zero_grad()",
        "Z": "When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How many.grads are resetting before each accumulation phase?",
        "Y": "None",
        "Z": "functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What are two alternatives to model.zero_grad()?",
        "Y": "model.zero_grad() or optimizer.zero_grad()",
        "Z": "such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What type of tensor does param.grad have?",
        "Y": "zeroed",
        "Z": "such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How many guarantees your layout is preserved as long as create_graph=False?",
        "Y": "3",
        "Z": "If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What indicates your layout is likely preserved even if create_graph=True?",
        "Y": "4",
        "Z": "If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is a valid alternative to that may improve performance for some networks?",
        "Y": "model.zero_grad() or optimizer.zero_grad()",
        "Z": "such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Set param.grad = a zeroed tensor with desired strides before the first backward() and never reset it to what",
        "Y": "None",
        "Z": "such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What guarantees your layout is preserved as long as create_graph=False?",
        "Y": "3",
        "Z": "If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is a zeroed tensor with desired strides before the first backward()?",
        "Y": "param.grad",
        "Z": "If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If you need manual control over.grad's strides, assign param.grad = a zeroed tensor with desired",
        "Y": "None",
        "Z": "If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is a hard matter in autograd?",
        "Y": "Supporting in-place operations in autograd",
        "Z": "Supporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them. All Tensor s keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What makes autograd very efficient?",
        "Y": "Autograd\u2019s aggressive buffer freeing and reuse",
        "Z": "If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True. Supporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Unless you are operating under heavy memory pressure, you might never need to use in-place operations?",
        "Y": "under heavy memory pressure",
        "Z": "Supporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is a hard matter to support in autograd?",
        "Y": "in-place operations",
        "Z": "Supporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them. All Tensor s keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How much do in-place operations lower?",
        "Y": "memory usage",
        "Z": "Supporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If you\u2019re operating under what type of pressure, you might never need to use in-place operations?",
        "Y": "heavy memory pressure",
        "Z": "Supporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What happens if a tensor was saved for backward in one of the functions, but it was modified in-place afterward?",
        "Y": "an error will be raised once backward pass is started",
        "Z": "All Tensor s keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If you're using in-place functions and not seeing errors, what can you be sure is correct?",
        "Y": "computed gradients",
        "Z": "All Tensor s keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, what will be raised",
        "Y": "an error",
        "Z": "All Tensor s keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What are correct if you're using in-place functions and not seeing any errors?",
        "Y": "computed gradients",
        "Z": "All Tensor s keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Why is the Variable API deprecated?",
        "Y": "Variables are no longer necessary to use autograd with tensors",
        "Z": "Warning The Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors with\nrequires_grad set to True. Below please find a quick guide on what\nhas changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names. In addition, one can now create tensors with requires_grad=True using factory\nmethods such as torch.randn(), torch.zeros(), torch.ones(), and others\nlike the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Autograd automatically supports Tensors with what setting set to True?",
        "Y": "requires_grad",
        "Z": "Warning The Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors with\nrequires_grad set to True. Below please find a quick guide on what\nhas changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names. In addition, one can now create tensors with requires_grad=True using factory\nmethods such as torch.randn(), torch.zeros(), torch.ones(), and others\nlike the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What do Variable(tensor, requires_grad) return instead of Variables?",
        "Y": "Tensors",
        "Z": "The Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors with\nrequires_grad set to True. Below please find a quick guide on what\nhas changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the same thing as tensor.data?",
        "Y": "var.data",
        "Z": "Warning The Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors with\nrequires_grad set to True. Below please find a quick guide on what\nhas changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names. In addition, one can now create tensors with requires_grad=True using factory\nmethods such as torch.randn(), torch.zeros(), torch.ones(), and others\nlike the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is no longer necessary to use autograd with tensors?",
        "Y": "Variables",
        "Z": "Warning The Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors with\nrequires_grad set to True. Below please find a quick guide on what\nhas changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names. In addition, one can now create tensors with requires_grad=True using factory\nmethods such as torch.randn(), torch.zeros(), torch.ones(), and others\nlike the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Autograd automatically supports Tensors with requires_grad set to what?",
        "Y": "True",
        "Z": "Warning The Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors with\nrequires_grad set to True. Below please find a quick guide on what\nhas changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What still work as expected, but return Tensors instead of Variables?",
        "Y": "Variable(tensor) and Variable(tensor, requires_grad)",
        "Z": "Warning The Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors with\nrequires_grad set to True. Below please find a quick guide on what\nhas changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names. In addition, one can now create tensors with requires_grad=True using factory\nmethods such as torch.randn(), torch.zeros(), torch.ones(), and others\nlike the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What methods now work on tensors with the same method names?",
        "Y": "var.backward(), var.detach(), var.register_hook()",
        "Z": "Warning The Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors with\nrequires_grad set to True. Below please find a quick guide on what\nhas changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names. In addition, one can now create tensors with requires_grad=True using factory\nmethods such as torch.randn(), torch.zeros(), torch.ones(), and others\nlike the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What API has been deprecated?",
        "Y": "Variable API",
        "Z": "The Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors with\nrequires_grad set to True. Below please find a quick guide on what\nhas changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Which methods still work as expected, but return Tensors instead of Variables?",
        "Y": "Variable(tensor) and Variable(tensor, requires_grad)",
        "Z": "Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names. In addition, one can now create tensors with requires_grad=True using factory\nmethods such as torch.randn(), torch.zeros(), torch.ones(), and others\nlike the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What can one now create tensors with?",
        "Y": "requires_grad=True",
        "Z": "Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names. In addition, one can now create tensors with requires_grad=True using factory\nmethods such as torch.randn(), torch.zeros(), torch.ones(), and others\nlike the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What do Variable(tensor) and Variable(tensor, requires_grad) do instead of Variables?",
        "Y": "return Tensors",
        "Z": "Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names. In addition, one can now create tensors with requires_grad=True using factory\nmethods such as torch.randn(), torch.zeros(), torch.ones(), and others\nlike the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does torch.randn((2, 3, 4), requires_grad=True) call?",
        "Y": "autograd_tensor",
        "Z": "Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names. In addition, one can now create tensors with requires_grad=True using factory\nmethods such as torch.randn(), torch.zeros(), torch.ones(), and others\nlike the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the default value of torch.Tensor.grad?",
        "Y": "None",
        "Z": "torch.Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. torch.Tensor.requires_grad Is True if gradients need to be computed for this Tensor, False otherwise. torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook. torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is true if gradients need to be computed for this Tensor?",
        "Y": "False",
        "Z": "torch.Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. torch.Tensor.requires_grad Is True if gradients need to be computed for this Tensor, False otherwise. torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook. torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What are all Tensors that have requires_grad which is False?",
        "Y": "leaf Tensors",
        "Z": "torch.Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. torch.Tensor.requires_grad Is True if gradients need to be computed for this Tensor, False otherwise. torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook. torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Where is the new Tensor?",
        "Y": "detached from the current graph",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. torch.Tensor.requires_grad Is True if gradients need to be computed for this Tensor, False otherwise. torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does torch.Tensor.backward compute the gradient of current tensor?",
        "Y": "w.r.t",
        "Z": "torch.Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. torch.Tensor.requires_grad Is True if gradients need to be computed for this Tensor, False otherwise. torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook. torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How is a new Tensor created?",
        "Y": "detached from the current graph",
        "Z": "torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook. torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the Tensor that is detached from the graph that created it?",
        "Y": "a leaf",
        "Z": "torch.Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. torch.Tensor.requires_grad Is True if gradients need to be computed for this Tensor, False otherwise. torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook. torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does torch.Tensor do?",
        "Y": "register_hook",
        "Z": "torch.Tensor.requires_grad Is True if gradients need to be computed for this Tensor, False otherwise. torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Is True if gradients need to be computed for this Tensor?",
        "Y": "False",
        "Z": "Is True if gradients need to be computed for this Tensor, False otherwise. torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does torch.Tensor.register_hook register?",
        "Y": "backward hook",
        "Z": "Is True if gradients need to be computed for this Tensor, False otherwise. torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is False in a Tensor?",
        "Y": "requires_grad",
        "Z": "torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook. torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Where is the new Tensor detached from?",
        "Y": "current graph",
        "Z": "torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook. torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does torch.Tensor.detach_ Detach the Tensor from the graph that created it into?",
        "Y": "leaf",
        "Z": "torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook. torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What type of hook does torch.Tensor.register_hook(hook) register?",
        "Y": "backward",
        "Z": "torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook(hook) Registers a backward hook. torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does the autograd class define formulas for?",
        "Y": "differentiating ops",
        "Z": "Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the Note on how to use this class?",
        "Y": "extending the autograd engine",
        "Z": "Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function\nobject, that performs the computation, and records that it happened.\nThe history is retained in the form of a DAG of functions, with edges\ndenoting data dependencies (input <- output). Then, when backward is\ncalled, the graph is processed in the topological ordering, by calling\nbackward() methods of each Function object, and passing\nreturned gradients on to next Function s. Normally, the only way users interact with functions is by creating\nsubclasses and defining new operations. This is a recommended way of\nextending torch.autograd. Examples: Function.backward Defines a formula for differentiating the operation. Function.forward Performs the operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Where is the Note on extending the autograd engine?",
        "Y": "https://pytorch.org/docs/stable/notes/extending.html",
        "Z": "See the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Every operation performed on what creates a new function object?",
        "Y": "Tensor s",
        "Z": "Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function\nobject, that performs the computation, and records that it happened.\nThe history is retained in the form of a DAG of functions, with edges\ndenoting data dependencies (input <- output). Then, when backward is\ncalled, the graph is processed in the topological ordering, by calling\nbackward() methods of each Function object, and passing\nreturned gradients on to next Function s. Normally, the only way users interact with functions is by creating\nsubclasses and defining new operations. This is a recommended way of\nextending torch.autograd. Examples: Function.backward Defines a formula for differentiating the operation. Function.forward Performs the operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the form of the history of a Tensor s function object?",
        "Y": "DAG of functions",
        "Z": "Every operation performed on Tensor s creates a new function\nobject, that performs the computation, and records that it happened.\nThe history is retained in the form of a DAG of functions, with edges\ndenoting data dependencies (input <- output). Then, when backward is\ncalled, the graph is processed in the topological ordering, by calling\nbackward() methods of each Function object, and passing\nreturned gradients on to next Function s.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What method is used to process a graph when backward is called?",
        "Y": "backward() methods",
        "Z": "Every operation performed on Tensor s creates a new function\nobject, that performs the computation, and records that it happened.\nThe history is retained in the form of a DAG of functions, with edges\ndenoting data dependencies (input <- output). Then, when backward is\ncalled, the graph is processed in the topological ordering, by calling\nbackward() methods of each Function object, and passing\nreturned gradients on to next Function s.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What happens to every operation performed on Tensor s?",
        "Y": "creates a new function object",
        "Z": "Every operation performed on Tensor s creates a new function\nobject, that performs the computation, and records that it happened.\nThe history is retained in the form of a DAG of functions, with edges\ndenoting data dependencies (input <- output). Then, when backward is\ncalled, the graph is processed in the topological ordering, by calling\nbackward() methods of each Function object, and passing\nreturned gradients on to next Function s.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What do the edges of the DAG of functions denote?",
        "Y": "data dependencies",
        "Z": "Every operation performed on Tensor s creates a new function\nobject, that performs the computation, and records that it happened.\nThe history is retained in the form of a DAG of functions, with edges\ndenoting data dependencies (input <- output). Then, when backward is\ncalled, the graph is processed in the topological ordering, by calling\nbackward() methods of each Function object, and passing\nreturned gradients on to next Function s.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What method is called when a graph is processed in the topological ordering?",
        "Y": "backward()",
        "Z": "Every operation performed on Tensor s creates a new function\nobject, that performs the computation, and records that it happened.\nThe history is retained in the form of a DAG of functions, with edges\ndenoting data dependencies (input <- output). Then, when backward is\ncalled, the graph is processed in the topological ordering, by calling\nbackward() methods of each Function object, and passing\nreturned gradients on to next Function s.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the only way users interact with functions?",
        "Y": "creating subclasses and defining new operations",
        "Z": "Normally, the only way users interact with functions is by creating\nsubclasses and defining new operations. This is a recommended way of\nextending torch.autograd. Examples: Function.backward Defines a formula for differentiating the operation. Function.forward Performs the operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is a recommended way of extending functions?",
        "Y": "torch.autograd",
        "Z": "Normally, the only way users interact with functions is by creating\nsubclasses and defining new operations. This is a recommended way of\nextending torch.autograd. Examples: Function.backward Defines a formula for differentiating the operation. Function.forward Performs the operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "To what are the following methods available when creating a new Function?",
        "Y": "ctx",
        "Z": "When creating a new Function, the following methods are available to ctx. function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation. function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable. function._ContextMethodMixin.save_for_backward Saves given tensors for a future call to backward(). function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How are tensors modified?",
        "Y": "in-place operation",
        "Z": "When creating a new Function, the following methods are available to ctx. function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation. function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable. function._ContextMethodMixin.save_for_backward Saves given tensors for a future call to backward(). function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What do function._ContextMethodMixin.mark_non_differentiable mark outputs as?",
        "Y": "non-differentiable",
        "Z": "When creating a new Function, the following methods are available to ctx. function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation. function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable. function._ContextMethodMixin.save_for_backward Saves given tensors for a future call to backward(). function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What function saves given tensors for a future call to backward()?",
        "Y": "backward",
        "Z": "When creating a new Function, the following methods are available to ctx. function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation. function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable. function._ContextMethodMixin.save_for_backward Saves given tensors for a future call to backward(). function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does function._ContextMethodMixin.set_ do?",
        "Y": "materialize_grads",
        "Z": "When creating a new Function, the following methods are available to ctx. function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation. function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable. function._ContextMethodMixin.save_for_backward Saves given tensors for a future call to backward(). function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "The following methods are available when creating a new Function?",
        "Y": "ctx",
        "Z": "When creating a new Function, the following methods are available to ctx. function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation. function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable. function._ContextMethodMixin.save_for_backward Saves given tensors for a future call to backward(). function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "In what operation are given tensors modified?",
        "Y": "in-place",
        "Z": "When creating a new Function, the following methods are available to ctx. function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation. function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable. function._ContextMethodMixin.save_for_backward Saves given tensors for a future call to backward(). function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What do function._ContextMethodMixin.mark_non_differentiable Marks outputs as?",
        "Y": "non-differentiable",
        "Z": "When creating a new Function, the following methods are available to ctx. function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation. function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable. function._ContextMethodMixin.save_for_backward Saves given tensors for a future call to backward(). function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does save_for_backward save?",
        "Y": "given tensors for a future call to backward()",
        "Z": "When creating a new Function, the following methods are available to ctx. function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation. function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable. function._ContextMethodMixin.save_for_backward Saves given tensors for a future call to backward(). function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What sets whether to materialize output grad tensors?",
        "Y": "set_materialize_grads",
        "Z": "When creating a new Function, the following methods are available to ctx. function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation. function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable. function._ContextMethodMixin.save_for_backward Saves given tensors for a future call to backward(). function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How are gradients computed?",
        "Y": "small finite differences against analytical gradients",
        "Z": "Check gradients computed via small finite differences against analytical gradients w.r.t.   Check gradients of gradients computed via small finite differences against analytical gradients w.r.t.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How are gradients computed against analytical gradients w.r.t?",
        "Y": "small finite differences",
        "Z": "Check gradients computed via small finite differences against analytical gradients w.r.t.   Check gradients of gradients computed via small finite differences against analytical gradients w.r.t.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What type of gradients computed via small finite differences against analytical gradients w.r.t?",
        "Y": "Check gradients",
        "Z": "Check gradients computed via small finite differences against analytical gradients w.r.t.   Check gradients of gradients computed via small finite differences against analytical gradients w.r.t.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does Autograd include that lets you inspect the cost of different operators inside your model?",
        "Y": "profiler",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What's the name of the profiler that lets you inspect the cost of different operators inside your model?",
        "Y": "profile",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does nvprof use to register both CPU and GPU activity?",
        "Y": "emit_nvtx",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What lets you inspect the cost of different operators inside your model?",
        "Y": "profiler",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What mode is implemented at the moment?",
        "Y": "CPU-only",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is used to register both CPU and GPU activity?",
        "Y": "emit_nvtx",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does the context manager manage?",
        "Y": "autograd profiler state",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What language does autograd profiler expose events to?",
        "Y": "Python",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Wrap any code into it and it will only report runtime of what?",
        "Y": "PyTorch functions",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is thread local and is automatically propagated into the async tasks enabled?",
        "Y": "profiler",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Context manager that manages what?",
        "Y": "autograd profiler state",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "under the hood it just records events of functions being executed in what language?",
        "Y": "C++",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "wrap any code into it and it will only what?",
        "Y": "report runtime of PyTorch functions",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "setting this to what makes this context manager a no-op?",
        "Y": "False",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Setting enabled to what makes this context manager a no-op?",
        "Y": "False",
        "Z": "enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What disables timing of CUDA events?",
        "Y": "use_cuda",
        "Z": "enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How much overhead does use_cuda add to each tensor operation?",
        "Y": "4us",
        "Z": "enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What enables timing of CUDA events as well using the cudaEvent API?",
        "Y": "use_cuda",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How much overhead is added to each tensor operation?",
        "Y": "4us",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the API that allows timing of CUDA events?",
        "Y": "use_cuda",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Who will estimate the FLOPS value if with_flops is set?",
        "Y": "the profiler",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does with_flops allow one to estimate?",
        "Y": "hardware performance",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What types of operators does with_flops only work for?",
        "Y": "matrix multiplication and 2D convolution operators",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does with_stack do?",
        "Y": "record source information",
        "Z": "with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does FLOPS stand for?",
        "Y": "floating pointer operations per second",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What operators does with_flops only work for?",
        "Y": "matrix multiplication and 2D convolution operators",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the plugin that allows profiling with Kineto profiler?",
        "Y": "use_kineto",
        "Z": "profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does use_cpu do?",
        "Y": "profile CPU events",
        "Z": "with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does profiler.profile.export_chrome_trace export an EventList as?",
        "Y": "Chrome tracing tools file",
        "Z": "with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Use_kineto (bool, optional) enables profiling with Kineto profiler. use_cpu (bool, optional",
        "Y": "experimental",
        "Z": "with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the profile CPU events?",
        "Y": "use_cpu",
        "Z": "use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What enables profiling with Kineto profiler?",
        "Y": "use_kineto",
        "Z": "with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Use_kineto (bool, optional) - enable profiling with Kineto profiler. use_cpu (bool,",
        "Y": "experimental",
        "Z": "with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What profile CPU events?",
        "Y": "use_cpu",
        "Z": "with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the experimental feature that allows profiling with Kineto profiler?",
        "Y": "use_kineto",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does profiler.profile.key_averages average?",
        "Y": "all function events over their keys",
        "Z": "with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does profiler.profile.self_cpu_time_total do?",
        "Y": "profiler.profile.self_cpu_time_total",
        "Z": "with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What profiler.profile.key_averages Averages all function events over their keys?",
        "Y": "profiler.profile.self_cpu_time_total",
        "Z": "use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the total time spent on CPU obtained as?",
        "Y": "sum of all self times across all the events",
        "Z": "profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is profiler.profile.total_?",
        "Y": "average",
        "Z": "profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is used to profile CPU events?",
        "Y": "use_cpu",
        "Z": "use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What returns total time spent on CPU obtained as a sum of all self times across all the events?",
        "Y": "profiler.profile.self_cpu_time_total",
        "Z": "profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What averages all events?",
        "Y": "profiler.profile.total_average",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does profiler.profile.total_average Average?",
        "Y": "all events",
        "Z": "profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does every autograd operation emit?",
        "Y": "NVTX range",
        "Z": "with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the program that makes every autograd operation emit an NVTX range?",
        "Y": "nvprof",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What Averages all events?",
        "Y": "total_average",
        "Z": "use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Profiler.profile.self_cpu_time_total Returns total time spent on what?",
        "Y": "CPU",
        "Z": "profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What exports an EventList as a Chrome tracing tools file?",
        "Y": "profiler.profile.export_chrome_trace",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is an EventList exported as?",
        "Y": "Chrome tracing tools file",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Under what program is the NVTX range useful?",
        "Y": "nvprof",
        "Z": "profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Exports an EventList as a what browser tracing tools file?",
        "Y": "Chrome",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What Averages all function events over their keys?",
        "Y": "profiler.profile.key_averages",
        "Z": "profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Where are all function events averaged?",
        "Y": "over their keys",
        "Z": "Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of all events in profiler.profile?",
        "Y": "total_average",
        "Z": "profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Returns total time spent on what?",
        "Y": "CPU",
        "Z": "Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does profiler.profile.total_average do?",
        "Y": "profiler.profile.total_average Averages all events",
        "Z": "profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Under what program is the context manager useful?",
        "Y": "nvprof",
        "Z": "profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is returned as a sum of all self times across all events?",
        "Y": "total time spent on CPU",
        "Z": "Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What range does every autograd operation emit?",
        "Y": "NVTX",
        "Z": "profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "When is the NVTX range useful?",
        "Y": "when running the program under nvprof",
        "Z": "profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is a Context manager that makes every autograd operation emit an NVTX range?",
        "Y": "Averages all events",
        "Z": "Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the range that every autograd operation emits?",
        "Y": "NVTX",
        "Z": "Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What makes every autograd operation emit an NVTX range?",
        "Y": "Context manager",
        "Z": "Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does the context manager do?",
        "Y": "Averages all events",
        "Z": "Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Context manager that makes every autograd operation emit what?",
        "Y": "NVTX range",
        "Z": "Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "When is the context manager useful?",
        "Y": "when running the program under nvprof",
        "Z": "Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does the context manager make every autograd operation emit?",
        "Y": "NVTX range",
        "Z": "Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the context manager that makes every autograd operation emit an NVTX range?",
        "Y": "nvprof",
        "Z": "Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What program is useful when running the program under?",
        "Y": "nvprof",
        "Z": "It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does nvvp stand for?",
        "Y": "NVIDIA Visual Profiler",
        "Z": "with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "In what language can torch.autograd.profiler.load_nvprof() be used?",
        "Y": "Python",
        "Z": "Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is a context manager used for?",
        "Y": "CUDA profiling",
        "Z": "It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is nvvp?",
        "Y": "NVIDIA Visual Profiler",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is another name for torch.autograd.profiler.load_nvprof()?",
        "Y": "Python REPL",
        "Z": "It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does the context manager do to annotate nvprof traces and wait for the process to exit before inspecting them?",
        "Y": "CUDA profiling",
        "Z": "Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Is there a way to force nvprof to flush the data it collected to disk?",
        "Y": "there\u2019s no way to force nvprof to flush the data it collected to disk",
        "Z": "Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the default value of enabled?",
        "Y": "True",
        "Z": "Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Setting enabled=False makes this context manager a what?",
        "Y": "no-op",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What default setting makes enabled=False a no-op?",
        "Y": "True",
        "Z": "enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is an example of a difficult task when viewing a profile created using emit_nvtx in the Nvidia Visual Profiler?",
        "Y": "Forward-backward correlation",
        "Z": "Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does emit_nvtx append to the ranges it generates?",
        "Y": "sequence number information",
        "Z": "Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "In what program is emit_nvtx used?",
        "Y": "Nvidia Visual Profiler",
        "Z": "Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is difficult when viewing a profile created using emit_nvtx in the Nvidia Visual Profiler?",
        "Y": "Forward-backward correlation",
        "Z": "Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What type of correlation can be difficult when viewing a profile created using emit_nvtx in the Nvidia Visual Profiler?",
        "Y": "Forward-backward",
        "Z": "Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Where is emit_nvtx used?",
        "Y": "Nvidia Visual Profiler",
        "Z": "Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What can be difficult when viewing a profile created using emit_nvtx in the Nvidia Visual Profiler?",
        "Y": "correlating each backward-pass op with the corresponding forward-pass op",
        "Z": "When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the term for a double backward?",
        "Y": "Double-backward",
        "Z": "Double-backward",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the term for a double-backward?",
        "Y": "Double-backward",
        "Z": "Double-backward",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What enable anomaly detection for the autograd engine?",
        "Y": "Context-manager",
        "Z": "Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does running the forward pass with detection enabled allow the backward pass to print?",
        "Y": "the traceback",
        "Z": "Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Any backward computation that generate \u201cnan\u201d value will what?",
        "Y": "raise an error",
        "Z": "Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Warning This mode should be enabled only for what purpose?",
        "Y": "debugging",
        "Z": "Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What type of test will slow down your program execution?",
        "Y": "Example",
        "Z": "Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What will raise an error?",
        "Y": "Any backward computation that generate \u201cnan\u201d value",
        "Z": "Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is an example that sets the anomaly detection for the autograd engine on or off?",
        "Y": "Context-manager",
        "Z": "This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Set_detect_anomaly will enable or disable the autograd anomaly detection based on what?",
        "Y": "argument mode",
        "Z": "This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Set_detect_anomaly can be used as what?",
        "Y": "context-manager or as a function",
        "Z": "Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "For details of the anomaly detection behaviour, see what?",
        "Y": "detect_anomaly",
        "Z": "Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "This mode should be enabled only for what purpose?",
        "Y": "debugging",
        "Z": "Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does the context-manager do?",
        "Y": "sets the anomaly detection for the autograd engine on or off",
        "Z": "Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What will enable or disable the autograd anomaly detection based on its argument mode?",
        "Y": "set_detect_anomaly",
        "Z": "Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "For details of the anomaly detection behaviour, see what above?",
        "Y": "detect_anomaly",
        "Z": "Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Warning This mode should only be enabled for what purpose?",
        "Y": "debugging",
        "Z": "Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What should this mode be enabled only for?",
        "Y": "debugging",
        "Z": "This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the setting that sets the autograd anomaly detection?",
        "Y": "detect_anomaly",
        "Z": "This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does mode (bool) do to enable or disable anomaly detection?",
        "Y": "Flag",
        "Z": "This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "This mode should only be enabled for what purpose?",
        "Y": "debugging",
        "Z": "This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the flag that indicates whether to enable or disable the autograd anomaly detection?",
        "Y": "mode",
        "Z": "Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What algorithm is used to efficiently compute the matrix product of the NNN 2-D tensors?",
        "Y": "matrix chain order algorithm",
        "Z": "Returns the matrix product of the NNN 2-D tensors. This product is efficiently computed\nusing the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms\nof arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NNN\nneeds to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.\nIf NNN is 1, then this is a no-op - the original matrix is returned as is. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "If NNN is greater than or equal to 2, what is the trivial matrix-matrix product returned?",
        "Y": "if equal to 2",
        "Z": "Returns the matrix product of the NNN 2-D tensors. This product is efficiently computed\nusing the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms\nof arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NNN\nneeds to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.\nIf NNN is 1, then this is a no-op - the original matrix is returned as is. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "If NNN is greater than or equal to 2, then this is a no-op - the original matrix is returned as is.",
        "Y": "1",
        "Z": "Returns the matrix product of the NNN 2-D tensors. This product is efficiently computed\nusing the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms\nof arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NNN\nneeds to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.\nIf NNN is 1, then this is a no-op - the original matrix is returned as is. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "What is the warning that returns the matrix product of the NNN 2-D tensors?",
        "Y": "Warning",
        "Z": "Returns the matrix product of the NNN 2-D tensors. This product is efficiently computed\nusing the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms\nof arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NNN\nneeds to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.\nIf NNN is 1, then this is a no-op - the original matrix is returned as is. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "What algorithm computes the matrix product of the NNN 2-D tensors?",
        "Y": "matrix chain order algorithm",
        "Z": "Returns the matrix product of the NNN 2-D tensors. This product is efficiently computed\nusing the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms\nof arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NNN\nneeds to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.\nIf NNN is 1, then this is a no-op - the original matrix is returned as is. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "What is required to return the matrix product of the NNN 2-D tensors?",
        "Y": "NNN needs to be greater than or equal to 2",
        "Z": "Returns the matrix product of the NNN 2-D tensors. This product is efficiently computed\nusing the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms\nof arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NNN\nneeds to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.\nIf NNN is 1, then this is a no-op - the original matrix is returned as is. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "What is the name of the warning message that is sent when NNN is greater than or equal to 2?",
        "Y": "Warning",
        "Z": "Returns the matrix product of the NNN 2-D tensors. This product is efficiently computed\nusing the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms\nof arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NNN\nneeds to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.\nIf NNN is 1, then this is a no-op - the original matrix is returned as is. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "What is deprecated and will be removed in a future PyTorch release?",
        "Y": "torch.chain_matmul()",
        "Z": "torch.chain_matmul() is deprecated and will be removed in a future PyTorch release.\nUse torch.linalg.multi_dot() instead, which accepts a list of two or more tensors\nrather than multiple arguments. matrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined. out (Tensor, optional) \u2013 the output tensor. Ignored if out = None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "What does torch.linalg.multi_dot() replace?",
        "Y": "multiple arguments",
        "Z": "Warning torch.chain_matmul() is deprecated and will be removed in a future PyTorch release.\nUse torch.linalg.multi_dot() instead, which accepts a list of two or more tensors\nrather than multiple arguments. matrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined. out (Tensor, optional) \u2013 the output tensor. Ignored if out = None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "What is a sequence of 2 or more 2-D tensors whose product is to be determined?",
        "Y": "matrices",
        "Z": "torch.chain_matmul() is deprecated and will be removed in a future PyTorch release.\nUse torch.linalg.multi_dot() instead, which accepts a list of two or more tensors\nrather than multiple arguments. matrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined. out (Tensor, optional) \u2013 the output tensor. Ignored if out = None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "What is the output tensor ignored if out =?",
        "Y": "None",
        "Z": "torch.chain_matmul() is deprecated and will be removed in a future PyTorch release.\nUse torch.linalg.multi_dot() instead, which accepts a list of two or more tensors\nrather than multiple arguments. matrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined. out (Tensor, optional) \u2013 the output tensor. Ignored if out = None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "What does torch.linalg.multi_dot() accept instead of multiple arguments?",
        "Y": "a list of two or more tensors",
        "Z": "torch.chain_matmul() is deprecated and will be removed in a future PyTorch release.\nUse torch.linalg.multi_dot() instead, which accepts a list of two or more tensors\nrather than multiple arguments. matrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined. out (Tensor, optional) \u2013 the output tensor. Ignored if out = None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "Ignored if out = what?",
        "Y": "None",
        "Z": "torch.chain_matmul() is deprecated and will be removed in a future PyTorch release.\nUse torch.linalg.multi_dot() instead, which accepts a list of two or more tensors\nrather than multiple arguments. matrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined. out (Tensor, optional) \u2013 the output tensor. Ignored if out = None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "When will torch.chain_matmul() be removed?",
        "Y": "PyTorch",
        "Z": "torch.chain_matmul() is deprecated and will be removed in a future PyTorch release.\nUse torch.linalg.multi_dot() instead, which accepts a list of two or more tensors\nrather than multiple arguments. matrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined. out (Tensor, optional) \u2013 the output tensor. Ignored if out = None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "What does torch.linalg.multi_dot() accept?",
        "Y": "a list of two or more tensors rather than multiple arguments",
        "Z": "torch.chain_matmul() is deprecated and will be removed in a future PyTorch release.\nUse torch.linalg.multi_dot() instead, which accepts a list of two or more tensors\nrather than multiple arguments. matrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined. out (Tensor, optional) \u2013 the output tensor. Ignored if out = None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "Out (Tensor, optional) \u2013 what is ignored if out = None?",
        "Y": "the output tensor",
        "Z": "torch.chain_matmul() is deprecated and will be removed in a future PyTorch release.\nUse torch.linalg.multi_dot() instead, which accepts a list of two or more tensors\nrather than multiple arguments. matrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined. out (Tensor, optional) \u2013 the output tensor. Ignored if out = None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "What is the tangent of the elements of input?",
        "Y": "hyperbolic",
        "Z": "Returns a new tensor with the hyperbolic tangent of the elements\nof input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh"
    },
    {
        "X": "Element-wise arctangent of inputi/otheri with consideration of what?",
        "Y": "quadrant",
        "Z": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b\nwith consideration of the quadrant. Returns a new tensor with the signed angles\nin radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b)\nand vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second\nparameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first\nparameter, is the y-coordinate.)",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "Returns a new tensor with the signed angles in what?",
        "Y": "radians",
        "Z": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b\nwith consideration of the quadrant. Returns a new tensor with the signed angles\nin radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b)\nand vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second\nparameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first\nparameter, is the y-coordinate.)",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "What is the first parameter of inputitextinput_iinputi?",
        "Y": "y",
        "Z": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b\nwith consideration of the quadrant. Returns a new tensor with the signed angles\nin radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b)\nand vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second\nparameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first\nparameter, is the y-coordinate.)",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "The arctangent of inputi/otheritextinput_iinputi /otheri is considered with",
        "Y": "the quadrant",
        "Z": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b\nwith consideration of the quadrant. Returns a new tensor with the signed angles\nin radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b)\nand vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second\nparameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first\nparameter, is the y-coordinate.)",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "What returns a new tensor with the signed angles in radians between vector (otheri,inputi)(text",
        "Y": "Returns a new tensor",
        "Z": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b\nwith consideration of the quadrant. Returns a new tensor with the signed angles\nin radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b)\nand vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second\nparameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first\nparameter, is the y-coordinate.) The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "What is inputitextinput_iinputi's first parameter?",
        "Y": "y",
        "Z": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b\nwith consideration of the quadrant. Returns a new tensor with the signed angles\nin radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b)\nand vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second\nparameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first\nparameter, is the y-coordinate.)",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "What is an example of a broadcastable input and output tensor?",
        "Y": "Example",
        "Z": "The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "What is the goal of this quickstart guide?",
        "Y": "Familiarize yourself with PyTorch concepts and modules",
        "Z": "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What do you learn in this quickstart guide?",
        "Y": "build deep neural networks",
        "Z": "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a step-by-step guide to building a complete ML workflow with PyTorch?",
        "Y": "Bite-size, ready-to-deploy PyTorch code examples",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "How is building a complete ML workflow with PyTorch?",
        "Y": "step-by-step",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "How does this tutorial introduce the fundamental concepts of PyTorch?",
        "Y": "self-contained examples",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is used to create and train a neural network?",
        "Y": "torch.nn",
        "Z": "Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "How do you use torch.nn to create and train a neural network?",
        "Y": "Getting-Started",
        "Z": "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Familiarize yourself with what concepts and modules?",
        "Y": "PyTorch",
        "Z": "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "How do you build models with PyTorch?",
        "Y": "build deep neural networks",
        "Z": "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of PyTorch code examples are included in this quickstart guide?",
        "Y": "Bite-size, ready-to-deploy PyTorch code examples",
        "Z": "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the name of the tutorial that introduces the fundamental concepts of PyTorch?",
        "Y": "Getting-Started",
        "Z": "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What tool can be used to visualize data and model training?",
        "Y": "TensorBoard",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What pre-trained model can TensorBoard fine tune?",
        "Y": "Mask R-CNN",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of data does TensorBoard fine tune?",
        "Y": "Image/Video",
        "Z": "A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is ready-to-deploy?",
        "Y": "PyTorch code examples",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is used to visualize data and model training?",
        "Y": "TensorBoard",
        "Z": "This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What aspect of TensorBoard Finetune a pre-trained Mask R-CNN model?",
        "Y": "Interpretability",
        "Z": "Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of data does TensorBoard finetune?",
        "Y": "Image/Video",
        "Z": "A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a step-by-step guide to building a complete ML workflow with?",
        "Y": "PyTorch",
        "Z": "A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What does this tutorial introduce the fundamental concepts of?",
        "Y": "PyTorch",
        "Z": "This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What do you learn to use to visualize data and model training?",
        "Y": "TensorBoard",
        "Z": "This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "TensorBoard Finetune a pre-trained what?",
        "Y": "Mask R-CNN model",
        "Z": "Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Train a convolutional neural network for image classification using what?",
        "Y": "transfer learning",
        "Z": "Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a convolutional neural network for image classification using transfer learning?",
        "Y": "Image/Video",
        "Z": "This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "This tutorial introduces the fundamental concepts of what?",
        "Y": "PyTorch",
        "Z": "This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the main feature of TensorBoard?",
        "Y": "Interpretability",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Train a convolutional neural network for what?",
        "Y": "image classification",
        "Z": "Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Where can you train a convolutional neural network?",
        "Y": "Image/Video",
        "Z": "Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a convolutional neural network for image classification?",
        "Y": "Image/Video",
        "Z": "This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Getting-Started Learn to use what to visualize data and model training?",
        "Y": "TensorBoard",
        "Z": "Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Apply what to computer vision tasks?",
        "Y": "cutting-edge, attention-based transformer models",
        "Z": "Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of video is used to train a convolutional neural network?",
        "Y": "Image/Video",
        "Z": "Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Getting-Started Learn to use what to visualize data and model training. Interpretability,Getting-Started,TensorBoard",
        "Y": "TensorBoard",
        "Z": "Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Interpretability,Getting-Started,TensorBoard Finetune a pre-trained what?",
        "Y": "Mask R-CNN model",
        "Z": "Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Apply cutting-edge, what to computer vision tasks?",
        "Y": "attention-based transformer models",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What to create and train a neural network?",
        "Y": "torch.nn",
        "Z": "Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What pre-trained model is fine tuned by TensorBoard?",
        "Y": "Mask R-CNN model",
        "Z": "Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of video is used to train a convolutional neural network for image classification using transfer learning?",
        "Y": "Image/Video",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of video train a convolutional neural network for image classification using transfer learning?",
        "Y": "Image/Video",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What does TensorBoard Finetune a pre-trained Mask R-CNN model?",
        "Y": "Interpretability",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the name of the pre-trained model?",
        "Y": "Mask R-CNN",
        "Z": "Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What does GAN stand for?",
        "Y": "generative adversarial network",
        "Z": "Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a generative adversarial network?",
        "Y": "Image/Video",
        "Z": "Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the main feature of a pre-trained Mask R-CNN model?",
        "Y": "Interpretability",
        "Z": "Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of video train a generative adversarial network?",
        "Y": "Image/Video",
        "Z": "Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Learn how to augment your network using what?",
        "Y": "visual attention mechanism",
        "Z": "Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What can you do with a pre-trained Mask R-CNN model?",
        "Y": "Finetune a pre-trained Mask R-CNN model",
        "Z": "Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is GAN?",
        "Y": "generative adversarial network",
        "Z": "Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is an example of a GAN?",
        "Y": "Image/Video",
        "Z": "Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is another name for a generative adversarial network?",
        "Y": "GAN",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of video learns how to augment your network using a visual attention mechanism?",
        "Y": "Image/Video",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is used to train a convolutional neural network for image classification?",
        "Y": "transfer learning",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the name of the video that teaches how to augment your network using a visual attention mechanism?",
        "Y": "Image/Video",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Apply cutting-edge, attention-based transformer models to what?",
        "Y": "computer vision tasks",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Train what for image classification using transfer learning?",
        "Y": "convolutional neural network",
        "Z": "Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a generative adversarial network called?",
        "Y": "GAN",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What library can you use to load and preprocess data from a simple dataset?",
        "Y": "PyTorch's torchaudio library",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of library does PyTorch's torchaudio library use?",
        "Y": "Audio",
        "Z": "Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Apply cutting edge, what to computer vision tasks?",
        "Y": "attention-based transformer models",
        "Z": "Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Learn to load and preprocess data from a simple dataset with what?",
        "Y": "PyTorch's torchaudio library",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of data does PyTorch's torchaudio library provide?",
        "Y": "Audio",
        "Z": "Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What can be applied to computer vision tasks?",
        "Y": "cutting-edge, attention-based transformer models",
        "Z": "Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is applied to computer vision tasks?",
        "Y": "attention-based transformer models",
        "Z": "Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Audio Learn how to correctly format an what type of dataset?",
        "Y": "audio",
        "Z": "Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of network can be used to train/test an audio classifier network on a dataset?",
        "Y": "Audio",
        "Z": "Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What do you use to augment your network?",
        "Y": "visual attention mechanism",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of network can be used to train a generative adversarial network?",
        "Y": "Audio",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of dataset is a dataset?",
        "Y": "audio",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What module is used to train a sequence-to-sequence model?",
        "Y": "nn.Transformer module",
        "Z": "Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript  Convert a model defined in PyTorch into the ONNX format and then run it with ONNX Runtime. Production  Build a simple FX pass that fuses batch norm into convolution to improve performance during inference. FX  Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics FX  Get an overview of Channels Last memory format and understand how it is used to order NCHW tensors in memory preserving dimensions. Memory-Format,Best-Practice,Frontend-APIs",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of data is used to train a generative adversarial network?",
        "Y": "Text",
        "Z": "Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What does PyTorch's torchaudio library train/test on a dataset?",
        "Y": "audio classifier network",
        "Z": "Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of data is used to train a sequence-to-sequence model?",
        "Y": "Text",
        "Z": "Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What library can be used to load and preprocess data from a simple dataset?",
        "Y": "PyTorch's torchaudio library",
        "Z": "Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What do you train/test on a dataset?",
        "Y": "audio classifier network",
        "Z": "Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of data does the nn.Transformer module train?",
        "Y": "Text",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of data does PyTorch's torchaudio library teach?",
        "Y": "Text",
        "Z": "Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What do you learn how to augment your network using?",
        "Y": "visual attention mechanism",
        "Z": "Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript  Convert a model defined in PyTorch into the ONNX format and then run it with ONNX Runtime. Production  Build a simple FX pass that fuses batch norm into convolution to improve performance during inference. FX",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of dataset can PyTorch correctly format?",
        "Y": "audio",
        "Z": "Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript  Convert a model defined in PyTorch into the ONNX format and then run it with ONNX Runtime. Production  Build a simple FX pass that fuses batch norm into convolution to improve performance during inference. FX  Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics FX",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of data does PyTorch's nn.Transformer module train?",
        "Y": "Text",
        "Z": "Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What do you need to learn to load and preprocess data from a simple dataset?",
        "Y": "PyTorch's torchaudio library",
        "Z": "Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Text Build and train a basic character-level to classify word from scratch without the use of torchtext?",
        "Y": "RNN",
        "Z": "Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "How many tutorials are there?",
        "Y": "three",
        "Z": "Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the term for a basic character-level RNN to classify word from scratch without the use of torchtext?",
        "Y": "Text",
        "Z": "Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Text Build and train a basic character-level RNN to classify word from scratch without the use of what?",
        "Y": "torchtext",
        "Z": "Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "How many tutorials are in this series?",
        "Y": "three",
        "Z": "Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript  Convert a model defined in PyTorch into the ONNX format and then run it with ONNX Runtime. Production  Build a simple FX pass that fuses batch norm into convolution to improve performance during inference. FX  Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics FX  Get an overview of Channels Last memory format and understand how it is used to order NCHW tensors in memory preserving dimensions. Memory-Format,Best-Practice,Frontend-APIs",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the first in a series of three tutorials?",
        "Y": "Text",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What can you use to load and preprocess data from a simple dataset?",
        "Y": "PyTorch's torchaudio library",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of dataset can you format?",
        "Y": "audio",
        "Z": "Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the term for a basic character-level RNN without the use of torchtext?",
        "Y": "Text",
        "Z": "Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What library does PyTorch use to load and preprocess data from a simple dataset?",
        "Y": "torchaudio library",
        "Z": "Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript  Convert a model defined in PyTorch into the ONNX format and then run it with ONNX Runtime. Production  Build a simple FX pass that fuses batch norm into convolution to improve performance during inference. FX  Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics FX",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of dataset does PyTorch's torchaudio library format?",
        "Y": "audio",
        "Z": "Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What does PyTorch's torchaudio library not use?",
        "Y": "torchtext",
        "Z": "Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the second in a series of three tutorials?",
        "Y": "leanr how to generate names from languages",
        "Z": "Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript  Convert a model defined in PyTorch into the ONNX format and then run it with ONNX Runtime. Production  Build a simple FX pass that fuses batch norm into convolution to improve performance during inference. FX  Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics FX",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the name of the first in a series of three tutorials?",
        "Y": "Text",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What do you train/test on the dataset?",
        "Y": "audio classifier network",
        "Z": "Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Build and train a basic character-level RNN to classify word from scratch without the use of what?",
        "Y": "torchtext",
        "Z": "Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What do students learn how to generate names from?",
        "Y": "languages",
        "Z": "Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "In a series of three tutorials, what is the second in a series of three tutorials?",
        "Y": "Second",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Learn how to correctly train/test an audio classifier network on the dataset?",
        "Y": "format an audio dataset",
        "Z": "Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What did you learn after using character-level RNN to classify names?",
        "Y": "leanr how to generate names from languages",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Learn how to correctly format a dataset and then train/test an audio classifier network on the dataset?",
        "Y": "audio",
        "Z": "Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of RNN is used to classify words?",
        "Y": "Text",
        "Z": "Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What do you want to generate names from?",
        "Y": "languages",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What are some examples around pytorch?",
        "Y": "Vision, Text, Reinforcement Learning",
        "Z": "A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.  Quick overview to essential PyTorch elements.  Access PyTorch Tutorials from GitHub.  Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a quick overview of essential PyTorch elements?",
        "Y": "Quick overview",
        "Z": "A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.  Quick overview to essential PyTorch elements.  Access PyTorch Tutorials from GitHub.  Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Where can you access PyTorch Tutorials?",
        "Y": "GitHub",
        "Z": "A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.  Quick overview to essential PyTorch elements.  Access PyTorch Tutorials from GitHub.  Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Where can you copy tutorial data to?",
        "Y": "Google Drive",
        "Z": "A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.  Quick overview to essential PyTorch elements.  Access PyTorch Tutorials from GitHub.  Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.?",
        "Y": "A set of examples around pytorch",
        "Z": "A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.  Quick overview to essential PyTorch elements.  Access PyTorch Tutorials from GitHub.  Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the name of a set of examples around pytorch?",
        "Y": "Quick overview",
        "Z": "A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.  Quick overview to essential PyTorch elements.  Access PyTorch Tutorials from GitHub.  Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the name of the file that you can copy tutorial data into?",
        "Y": "Google Drive",
        "Z": "A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.  Quick overview to essential PyTorch elements.  Access PyTorch Tutorials from GitHub.  Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is another name for python -m torch.utils.bottleneck?",
        "Y": "python -m torch.utils.bottleneck",
        "Z": "where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What should you do to ensure that your script exits in a finite amount of time?",
        "Y": "ensure that it exits in a finite amount of time",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the warning that your script will be profiled and exits in a finite amount of time?",
        "Y": "Warning",
        "Z": "Run it on the command line with where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the name of the command to run for more usage instructions?",
        "Y": "python -m torch.utils.bottleneck",
        "Z": "where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is it called when a script is profiled?",
        "Y": "Warning",
        "Z": "where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What will be profiled?",
        "Y": "your script",
        "Z": "Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the name of the warning that your script will be profiled?",
        "Y": "Warning",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the name of the warning that your script will exit in a finite amount of time?",
        "Y": "Warning",
        "Z": "Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What should you do when your script is profiled?",
        "Y": "ensure that it exits in a finite amount of time",
        "Z": "Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is another name for CPU-only-mode?",
        "Y": "CUDA-mode",
        "Z": "Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "If your script is what, looking at the results of the CPU-mode autograd profiler will help?",
        "Y": "CPU-bound",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "If your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output",
        "Y": "CUDA-mode autograd profiler",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the default mode for autograd profiler output?",
        "Y": "CPU-only",
        "Z": "Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "If your script is CPU-bound, what autograd profiler will help?",
        "Y": "CPU-mode",
        "Z": "Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What does it make sense to look for in the output of the CUDA-mode autograd profiler?",
        "Y": "responsible CUDA operators",
        "Z": "To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What should you first check if your script is?",
        "Y": "CPU-bound",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What type of autograd profiler will help if your script is CPU-bound?",
        "Y": "CPU-mode",
        "Z": "To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the CUDA-mode autograd profiler?",
        "Y": "CPU-only",
        "Z": "To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "Which autograd profiler will help if your script is CPU-bound?",
        "Y": "CPU-mode",
        "Z": "To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What might happen to your script depending on the part of the model you're evaluating?",
        "Y": "your script might not be in one of those two extremes",
        "Z": "Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What does torch.autograd.profiler.emit_nvtx() use?",
        "Y": "nvprof",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is very high and often gives a heavily skewed timeline?",
        "Y": "NVTX overhead",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the name of the warning that the NVTX overhead is very high and often gives a heavily skewed timeline?",
        "Y": "Warning",
        "Z": "Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the reality of NVTX?",
        "Y": "more complicated",
        "Z": "Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What does nvprof do if the profiler outputs don't help?",
        "Y": "torch.autograd.profiler.emit_nvtx()",
        "Z": "Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What does the NVTX overhead often give?",
        "Y": "heavily skewed timeline",
        "Z": "Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the NVTX overhead?",
        "Y": "Warning",
        "Z": "Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What does cProfile include in its time reporting?",
        "Y": "CUDA startup time",
        "Z": "Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What should not matter if your bottlenecks result in code much slower than the CUDA startup time?",
        "Y": "if your bottlenecks result in code much slower than the CUDA startup time",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is a multi-GPU case?",
        "Y": "more complicated uses of the profilers",
        "Z": "Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the CUDA buffer allocation cost?",
        "Y": "CUDA startup time",
        "Z": "Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What should not matter if your bottlenecks result in code much slower than?",
        "Y": "CUDA startup time",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is an example of a more complicated use of profilers?",
        "Y": "multi-GPU",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the name of the document?",
        "Y": "Note",
        "Z": "Note",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the meaning of what?",
        "Y": "Note",
        "Z": "Note",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does the stashing logic save and restore for the current device?",
        "Y": "the RNG state",
        "Z": "The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "Does the stashing logic have a way to anticipate if the user will move Tensors to a new device within the run_f",
        "Y": "the logic has no way to anticipate if the user will move Tensors to a new device within the run_fn itself",
        "Z": "The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is never guaranteed if you move Tensors to a new device within run_fn?",
        "Y": "deterministic output compared to non-checkpointed passes",
        "Z": "The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What saves and restores the RNG state for the current device and the device of all cuda Tensor arguments to the run_f",
        "Y": "The stashing logic",
        "Z": "Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "Does the stashing logic have any way to anticipate if the user will move Tensors to a new device within the run_fn",
        "Y": "no way to anticipate if the user will move Tensors to a new device within the run_fn itself",
        "Z": "Note Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is never guaranteed if you move Tensors to a new device?",
        "Y": "deterministic output",
        "Z": "Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "How does Checkpointing work?",
        "Y": "trading compute for memory",
        "Z": "Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In what way does the checkpointed part recompute the intermediate activations?",
        "Y": "backward pass",
        "Z": "Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "Checkpointing can be applied on what?",
        "Y": "any part of a model",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does the checkpointed part do instead of saving intermediate activations?",
        "Y": "recomputes them in backward pass",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "How does checkpointing work?",
        "Y": "trading compute for memory",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "The checkpointed part recomputes intermediate activations in what?",
        "Y": "backward pass",
        "Z": "The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "How does checking work?",
        "Y": "trading compute for memory",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "Checkpointing can be applied to what part of a model?",
        "Y": "any part of a model",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In the forward pass, function will run in what manner?",
        "Y": "torch.no_grad()",
        "Z": "Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What saves the inputs tuple and the function parameter?",
        "Y": "forward pass",
        "Z": "Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In what pass are the saved inputs and function retrieved?",
        "Y": "the backwards pass",
        "Z": "Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is torch.no_grad()?",
        "Y": "not storing the intermediate activations",
        "Z": "Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does the forward pass save?",
        "Y": "the inputs tuple and the function parameter",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What can the output of a function contain?",
        "Y": "Tensor values",
        "Z": "The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What are examples of nested structures?",
        "Y": "custom objects, lists, dicts",
        "Z": "Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What are Tensors nested in?",
        "Y": "custom structures",
        "Z": "The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does Warning Checkpointing currently only support?",
        "Y": "torch.autograd.backward()",
        "Z": "Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "Is torch.autograd.grad() supported?",
        "Y": "not supported",
        "Z": "The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the warning that torch.autograd.grad() is not supported?",
        "Y": "Warning",
        "Z": "The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What currently only supports torch.autograd.backward()?",
        "Y": "Warning Checkpointing",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the status of torch.autograd.grad()?",
        "Y": "not supported",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What won't be equivalent if function invocation during backward does something different than the one during forward?",
        "Y": "the checkpointed version",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the name of the warning if function invocation during backward does something different than the one during forward?",
        "Y": "Warning",
        "Z": "Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is it called when a function invocation during backward does something different than the one during forward?",
        "Y": "Warning",
        "Z": "The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What causes the checkpointed version to not be equivalent?",
        "Y": "global variable",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What will not be equivalent if function invocation during backward does something different than during forward?",
        "Y": "checkpointed version",
        "Z": "Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a warning if function invocation during backward does anything different?",
        "Y": "Warning",
        "Z": "Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "If function invocation during backward does something different than the one during forward, the checkpointed version won\u2019t be equivalent, and unfortunately it",
        "Y": "global variable",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What won\u2019t be equivalent if function invocation during backward does something different than the one during forward?",
        "Y": "checkpointed version",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does the checkpointed version of the checkpointed version do?",
        "Y": "Warning",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What methods are used to detach tensors from the computational graph?",
        "Y": "detach() or torch.no_grad()",
        "Z": "If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What makes all the outputs require gradients?",
        "Y": "checkpoint",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "How can a tensor be circumvented if a tensor is defined to have no gradient in the model?",
        "Y": "detach the tensors outside of the checkpoint function",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is detached from the computational graph by detach() or torch.no_grad()?",
        "Y": "tensors",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a way to circumvent the error when a tensor is defined to have no gradient in the model?",
        "Y": "detach the tensors outside of the checkpoint function",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a way to circumvent the backward pass error?",
        "Y": "detach the tensors outside of the checkpoint function",
        "Z": "If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "How are tensors detached from the computational graph?",
        "Y": "detach() or torch.no_grad()",
        "Z": "If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a way to circumvent the issue of a tensor being defined to have no gradient in the model?",
        "Y": "detach the tensors outside of the checkpoint function",
        "Z": "If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does function describe?",
        "Y": "what to run in the forward pass of the model or part of the model",
        "Z": "The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What should function know how to handle the inputs passed as?",
        "Y": "tuple",
        "Z": "Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What should function use the second input as in LSTM?",
        "Y": "hidden preserve_rng_state",
        "Z": "function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "args \u2013 tuple containing inputs to the function Output of running function on what?",
        "Y": "*args",
        "Z": "function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In LSTM, if user passes what, hidden, function should use the first input as activation and the second input as hidden?",
        "Y": "activation",
        "Z": "function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a tuple containing inputs to the function?",
        "Y": "args",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What omits stashing and restoring the RNG state during each checkpoint?",
        "Y": "preserve_rng_state",
        "Z": "preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What tuple contains inputs to the function Output of running function on *args A helper function for checkpointing sequential models?",
        "Y": "args",
        "Z": "preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a tuple containing inputs to the function Output of running function on *args?",
        "Y": "args",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "A helper function for what kind of sequential models?",
        "Y": "checkpointing",
        "Z": "args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "Output of running function on what?",
        "Y": "*args",
        "Z": "Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What executes a list of modules/functions in order?",
        "Y": "Sequential models",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "How can we divide a sequential model into segments?",
        "Y": "checkpoint",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In what manner will all segments except the last run?",
        "Y": "torch.no_grad()",
        "Z": "function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "The inputs of each checkpointed segment will be saved for re-running the segment in what?",
        "Y": "backward pass",
        "Z": "A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What models execute a list of modules/functions in order (sequentially)?",
        "Y": "Sequential models",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "How do we divide a sequential model into segments?",
        "Y": "checkpoint",
        "Z": "args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "All segments except the last will run in what manner?",
        "Y": "torch.no_grad()",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What will be saved for re-running the segment in the backward pass?",
        "Y": "The inputs of each checkpointed segment",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a helper function for?",
        "Y": "checkpointing sequential models",
        "Z": "A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What do we do to each segment of a sequential model?",
        "Y": "checkpoint",
        "Z": "A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What function shows how checkpointing works?",
        "Y": "checkpoint()",
        "Z": "A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the name of the function that checks sequential models?",
        "Y": "Warning",
        "Z": "A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is used for checkpointing sequential models?",
        "Y": "A helper function",
        "Z": "A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a helper function for checkpointing sequential models?",
        "Y": "*args",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does checkpointing do?",
        "Y": "Warning",
        "Z": "A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What do we do to each segment of a Sequential model?",
        "Y": "checkpoint",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the name of the function that shows how checkpointing works?",
        "Y": "checkpoint()",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the name of the warning that is displayed when a model is checked?",
        "Y": "Warning",
        "Z": "function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What models execute a list of modules/functions in order?",
        "Y": "Sequential models",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "How does checkpoint() work?",
        "Y": "checkpointing",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the name of the warning that is displayed when a sequence model is checked?",
        "Y": "Warning",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "How do functions run?",
        "Y": "sequentially",
        "Z": "Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is input to functions to create chunks in the model input?",
        "Y": "Tensor",
        "Z": "See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the name of checkpointing?",
        "Y": "checkpoint()",
        "Z": "See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does checkpointing currently only support?",
        "Y": "torch.autograd.backward()",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a function to run sequentially?",
        "Y": "A torch.nn.Sequential or the list of modules or functions",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What are segments used to create in the model input?",
        "Y": "Number of chunks",
        "Z": "See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is input to functions preserve_rng_state?",
        "Y": "A Tensor",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the purpose of a torch.nn.Sequential?",
        "Y": "to run sequentially",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What are segments?",
        "Y": "Number of chunks to create in the model input",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the output of running functions sequentially on?",
        "Y": "*inputs Example",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the name of the list of modules or functions to run sequentially?",
        "Y": "A torch.nn.Sequential",
        "Z": "functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What dimension is removed?",
        "Y": "tensor dimension",
        "Z": "Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it. input (Tensor) \u2013 the tensor to unbind dim (int) \u2013 dimension to remove Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind"
    },
    {
        "X": "What is the number of all slices along a given dimension?",
        "Y": "tuple",
        "Z": "Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it. input (Tensor) \u2013 the tensor to unbind dim (int) \u2013 dimension to remove Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind"
    },
    {
        "X": "What is the tensor to unbind?",
        "Y": "dim",
        "Z": "Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it. input (Tensor) \u2013 the tensor to unbind dim (int) \u2013 dimension to remove Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind"
    },
    {
        "X": "Returns what of all slices along a given dimension, already without it?",
        "Y": "tuple",
        "Z": "Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it. input (Tensor) \u2013 the tensor to unbind dim (int) \u2013 dimension to remove Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind"
    },
    {
        "X": "What is the tensor to unbind dim (int) \u2013 dimension to remove?",
        "Y": "input",
        "Z": "Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it. input (Tensor) \u2013 the tensor to unbind dim (int) \u2013 dimension to remove Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind"
    },
    {
        "X": "If upper is True or not provided, uuu is upper triangular such that the returned tensor is input (Tens",
        "Y": "If upper is False",
        "Z": "If upper is False, uuu is lower triangular\nsuch that the returned tensor is If upper is True or not provided, uuu is upper\ntriangular such that the returned tensor is input (Tensor) \u2013 the input 2-D tensor uuu, a upper or lower triangular\nCholesky factor upper (bool, optional) \u2013 whether to return a lower (default) or upper triangular matrix out (Tensor, optional) \u2013 the output tensor for inv Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse"
    },
    {
        "X": "What is the returned tensor?",
        "Y": "input (Tensor)",
        "Z": "Computes the inverse of a symmetric positive-definite matrix AAA using its\nCholesky factor uuu: returns matrix inv. The inverse is computed using\nLAPACK routines dpotri and spotri (and the corresponding MAGMA routines). If upper is False, uuu is lower triangular\nsuch that the returned tensor is If upper is True or not provided, uuu is upper\ntriangular such that the returned tensor is input (Tensor) \u2013 the input 2-D tensor uuu, a upper or lower triangular\nCholesky factor upper (bool, optional) \u2013 whether to return a lower (default) or upper triangular matrix out (Tensor, optional) \u2013 the output tensor for inv Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse"
    },
    {
        "X": "What is the input 2-D tensor uuu?",
        "Y": "Cholesky factor upper",
        "Z": "If upper is False, uuu is lower triangular\nsuch that the returned tensor is If upper is True or not provided, uuu is upper\ntriangular such that the returned tensor is input (Tensor) \u2013 the input 2-D tensor uuu, a upper or lower triangular\nCholesky factor upper (bool, optional) \u2013 whether to return a lower (default) or upper triangular matrix out (Tensor, optional) \u2013 the output tensor for inv Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse"
    },
    {
        "X": "What are Alias for?",
        "Y": "torch.linalg.matrix_power()",
        "Z": "Alias for torch.linalg.matrix_power()",
        "source": "https://pytorch.org/docs/stable/generated/torch.matrix_power.html#torch.matrix_power"
    },
    {
        "X": "Pivoting is done if pivot is set to what?",
        "Y": "True",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What are the pivots returned by the function?",
        "Y": "1-indexed",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What are the returned pivots filled with zeros of the appropriate size?",
        "Y": "tensor",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is not available for CPU?",
        "Y": "LU factorization with pivot = False",
        "Z": "LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is LU factorization with pivot = False available for?",
        "Y": "CUDA",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does the function do to a matrix or batches of matrices A?",
        "Y": "Computes the LU factorization",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Returns a tuple containing what?",
        "Y": "LU factorization and pivots of A",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What are the returned pivots if pivot is False?",
        "Y": "a tensor filled with zeros of the appropriate size",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Is LU factorization with pivot = False available for CPU?",
        "Y": "not available for CPU",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If pivot is False, the returned pivots is a what?",
        "Y": "tensor",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is LU factorization with pivot = False available for CUDA?",
        "Y": "Note",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does LU factorization with pivot = False not do?",
        "Y": "check if the factorization was successful or not if get_infos is True",
        "Z": "LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does this function not check if the factorization was successful or not if get_infos is True?",
        "Y": "Note",
        "Z": "LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does this function not do?",
        "Y": "check if the factorization was successful or not if get_infos is True",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What library has a bug that causes the LU factorization to be repeated for singular matrices?",
        "Y": "MAGMA library",
        "Z": "This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "How can L, U, and P be derived?",
        "Y": "torch.lu_unpack()",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does torch.lu_unpack() do?",
        "Y": "Warning",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does get_infos not check if the factorization was successful or not if get_infos is True?",
        "Y": "the status of the factorization is present in the third element of the return tuple",
        "Z": "This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What happens in batches of square matrices with size less than 32 on a CUDA device?",
        "Y": "the LU factorization is repeated for singular matrices",
        "Z": "This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the result of the bug in the MAGMA library?",
        "Y": "Warning",
        "Z": "Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the message that the function does not check if the factorization was successful or not?",
        "Y": "Warning",
        "Z": "This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the name of the bug in the MAGMA library?",
        "Y": "Warning",
        "Z": "This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Why is the LU factorization repeated for batches of square matrices with size less than 32 on a CUDA device?",
        "Y": "MAGMA library",
        "Z": "In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does the LU factorization have?",
        "Y": "backward support",
        "Z": "The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What controls whether pivoting is done?",
        "Y": "pivot",
        "Z": "pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default value for pivoting?",
        "Y": "True",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is repeated in batches of square matrices with size less than 32 on a CUDA device?",
        "Y": "LU factorization",
        "Z": "In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "The LU factorization does have backward support, but only for square inputs of what?",
        "Y": "full rank",
        "Z": "The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "A (Tensor) \u2013 the tensor to factor of size (,m,n)(*, m,",
        "Y": "pivoting",
        "Z": "In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default setting for pivoting?",
        "Y": "True",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the tensor to factor of size?",
        "Y": "A",
        "Z": "A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What returns an info IntTensor if set to True?",
        "Y": "True get_infos",
        "Z": "pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default value of get_infos?",
        "Y": "None",
        "Z": "This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default value for get_infos?",
        "Y": "True",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does LU factorization have?",
        "Y": "backward support",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default setting for get_infos?",
        "Y": "False",
        "Z": "LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is a tuple of tensors containing Tensor, IntTensor, and IntT",
        "Y": "optional output tuple",
        "Z": "A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What are the elements in the tuple if get_infos is True?",
        "Y": "Tensor, IntTensor, and IntTensor",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What are the elements in the tuple if get_infos is False?",
        "Y": "Tensor, IntTensor",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default value of a tuple of tensors containing a factor of size?",
        "Y": "None",
        "Z": "A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What if set to True, returns an info IntTensor?",
        "Y": "True get_infos",
        "Z": "pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is False out (tuple, optional)?",
        "Y": "optional output tuple",
        "Z": "The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If get_infos is True, then the elements in the tuple are Tensor, IntTensor, and In",
        "Y": "False",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If get_infos is True, then the elements in the tuple are Tensor, IntTensor.",
        "Y": "If get_infos is False",
        "Z": "pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default value for a tuple of tensors containing tensors?",
        "Y": "None",
        "Z": "A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is False out?",
        "Y": "optional output tuple",
        "Z": "get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does a tuple of tensors contain?",
        "Y": "factorization",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What are the elements in a tuple if get_infos is True?",
        "Y": "Tensor, IntTensor, and IntTensor",
        "Z": "get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If get_infos is False, what are the elements in the tuple?",
        "Y": "Tensor, IntTensor",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default value for a tuple of tensors containing factorization?",
        "Y": "None",
        "Z": "Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If get_infos is set to what, returns an info IntTensor?",
        "Y": "True",
        "Z": "get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What returns an info IntTensor?",
        "Y": "True get_infos",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does False out (tuple, optional) return?",
        "Y": "optional output tuple",
        "Z": "In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What are the elements in a tuple if get_infos is False?",
        "Y": "Tensor, IntTensor",
        "Z": "get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What type of tuple is out?",
        "Y": "optional output",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If get_infos is what, the elements in the tuple are Tensor, IntTensor, and Int",
        "Y": "True",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If get_infos is what, then the elements in the tuple are Tensor, IntTensor, and In",
        "Y": "False",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Out (tuple, optional) \u2013 what type of output tuple?",
        "Y": "optional output tuple",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If get_infos is what, then the elements in the tuple are Tensor, IntTensor?",
        "Y": "False",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What stores all the intermediate transpositions of rows?",
        "Y": "pivots",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "The final permutation perm could be reconstructed by applying what?",
        "Y": "swap(perm[i], perm[pivots[i] - 1]",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is a tuple of tensors?",
        "Y": "tuple of tensors",
        "Z": "A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What utation perm could be reconstructed by applying swap(perm[i], perm[pivots[i]",
        "Y": "perm",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the factorization of size?",
        "Y": "factorization",
        "Z": "factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the factorization of size (,m,n)(*, m, n)(,m,n)?",
        "Y": "factorization",
        "Z": "factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What are the pivots of?",
        "Y": "size",
        "Z": "pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does pivots store?",
        "Y": "all the intermediate transpositions of rows",
        "Z": "pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What are the pivots of size?",
        "Y": "pivots",
        "Z": "pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If get_infos is True, what is infos?",
        "Y": "if get_infos is True",
        "Z": "get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Infos (IntTensor, IntTensor, IntTensor) is what?",
        "Y": "optional",
        "Z": "infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If get_infos is True, this is a tensor of size ()(*)() where non-zer",
        "Y": "if get_infos is True",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What are both inputitextinput_iinputi and otheritextother_iotheri?",
        "Y": "weakly positive",
        "Z": "Computes the regularized lower incomplete gamma function: where both inputi\\text{input}_iinputi\u200b and otheri\\text{other}_iotheri\u200b are weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\n\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5) in the equation above is the gamma function, See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "If both inputitextinput_iinputi and otheritextother_iotheri are negative, what",
        "Y": "outi",
        "Z": "where both inputi\\text{input}_iinputi\u200b and otheri\\text{other}_iotheri\u200b are weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\n\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5) in the equation above is the gamma function, See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "What is the gamma function in the equation above?",
        "Y": "gamma function",
        "Z": "where both inputi\\text{input}_iinputi\u200b and otheri\\text{other}_iotheri\u200b are weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\n\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5) in the equation above is the gamma function, See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "What does torch.igammac() support?",
        "Y": "broadcasting to a common shape and float inputs",
        "Z": "Computes the regularized lower incomplete gamma function: where both inputi\\text{input}_iinputi\u200b and otheri\\text{other}_iotheri\u200b are weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\n\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5) in the equation above is the gamma function, See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "What is the name of the function that supports broadcasting to a common shape and float inputs?",
        "Y": "Note",
        "Z": "where both inputi\\text{input}_iinputi\u200b and otheri\\text{other}_iotheri\u200b are weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\n\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5) in the equation above is the gamma function, See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "If both inputitextinput_iinputi and otheritextother_iotheri are what?",
        "Y": "weakly positive",
        "Z": "where both inputi\\text{input}_iinputi\u200b and otheri\\text{other}_iotheri\u200b are weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\n\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5) in the equation above is the gamma function, See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "If both inputitextinput_iinputi and otheritextother_iotheri are weakly positive",
        "Y": "zero",
        "Z": "where both inputi\\text{input}_iinputi\u200b and otheri\\text{other}_iotheri\u200b are weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\n\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5) in the equation above is the gamma function, See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "What does the gamma function support?",
        "Y": "broadcasting to a common shape and float inputs",
        "Z": "where both inputi\\text{input}_iinputi\u200b and otheri\\text{other}_iotheri\u200b are weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\n\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5) in the equation above is the gamma function, See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "What are related functions to the gamma function?",
        "Y": "torch.igammac() and torch.lgamma()",
        "Z": "where both inputi\\text{input}_iinputi\u200b and otheri\\text{other}_iotheri\u200b are weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\n\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5) in the equation above is the gamma function, See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "What is supported to a common shape and float inputs?",
        "Y": "broadcasting",
        "Z": "See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note The backward pass with respect to input is not yet supported.\nPlease open an issue on PyTorch\u2019s Github to request it. input (Tensor) \u2013 the first non-negative input tensor other (Tensor) \u2013 the second non-negative input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "What are two functions that support broadcasting to a common shape and float inputs?",
        "Y": "torch.igammac() and torch.lgamma()",
        "Z": "See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note The backward pass with respect to input is not yet supported.\nPlease open an issue on PyTorch\u2019s Github to request it. input (Tensor) \u2013 the first non-negative input tensor other (Tensor) \u2013 the second non-negative input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "What pass with respect to input is not yet supported?",
        "Y": "backward",
        "Z": "See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note The backward pass with respect to input is not yet supported.\nPlease open an issue on PyTorch\u2019s Github to request it. input (Tensor) \u2013 the first non-negative input tensor other (Tensor) \u2013 the second non-negative input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "Where can you open an issue with regards to the backward pass with respect to input?",
        "Y": "PyTorch\u2019s Github",
        "Z": "See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note The backward pass with respect to input is not yet supported.\nPlease open an issue on PyTorch\u2019s Github to request it. input (Tensor) \u2013 the first non-negative input tensor other (Tensor) \u2013 the second non-negative input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "What is the second non-negative input tensor out?",
        "Y": "output tensor",
        "Z": "See torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape\nand float inputs. Note The backward pass with respect to input is not yet supported.\nPlease open an issue on PyTorch\u2019s Github to request it. input (Tensor) \u2013 the first non-negative input tensor other (Tensor) \u2013 the second non-negative input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "What does torch.load() load an object saved with torch.save() from?",
        "Y": "a file",
        "Z": "Loads an object saved with torch.save() from a file. torch.load() uses Python\u2019s unpickling facilities but treats storages,\nwhich underlie tensors, specially. They are first deserialized on the\nCPU and are then moved to the device they were saved from. If this fails\n(e.g. because the run time system doesn\u2019t have certain devices), an exception\nis raised. However, storages can be dynamically remapped to an alternative\nset of devices using the map_location argument.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What do storages underlie?",
        "Y": "tensors",
        "Z": "torch.load() uses Python\u2019s unpickling facilities but treats storages,\nwhich underlie tensors, specially. They are first deserialized on the\nCPU and are then moved to the device they were saved from. If this fails\n(e.g. because the run time system doesn\u2019t have certain devices), an exception\nis raised. However, storages can be dynamically remapped to an alternative\nset of devices using the map_location argument.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "Where are storages first deserialized?",
        "Y": "the CPU",
        "Z": "torch.load() uses Python\u2019s unpickling facilities but treats storages,\nwhich underlie tensors, specially. They are first deserialized on the\nCPU and are then moved to the device they were saved from. If this fails\n(e.g. because the run time system doesn\u2019t have certain devices), an exception\nis raised. However, storages can be dynamically remapped to an alternative\nset of devices using the map_location argument.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What happens when a storage is moved to the device it was saved from?",
        "Y": "fails",
        "Z": "Loads an object saved with torch.save() from a file. torch.load() uses Python\u2019s unpickling facilities but treats storages,\nwhich underlie tensors, specially. They are first deserialized on the\nCPU and are then moved to the device they were saved from. If this fails\n(e.g. because the run time system doesn\u2019t have certain devices), an exception\nis raised. However, storages can be dynamically remapped to an alternative\nset of devices using the map_location argument.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "Why does torch.load() fail?",
        "Y": "run time system doesn\u2019t have certain devices",
        "Z": "torch.load() uses Python\u2019s unpickling facilities but treats storages,\nwhich underlie tensors, specially. They are first deserialized on the\nCPU and are then moved to the device they were saved from. If this fails\n(e.g. because the run time system doesn\u2019t have certain devices), an exception\nis raised. However, storages can be dynamically remapped to an alternative\nset of devices using the map_location argument.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What argument allows storages to be dynamically remapped to an alternative set of devices?",
        "Y": "map_location",
        "Z": "torch.load() uses Python\u2019s unpickling facilities but treats storages,\nwhich underlie tensors, specially. They are first deserialized on the\nCPU and are then moved to the device they were saved from. If this fails\n(e.g. because the run time system doesn\u2019t have certain devices), an exception\nis raised. However, storages can be dynamically remapped to an alternative\nset of devices using the map_location argument.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What Python facility does torch.load() use?",
        "Y": "unpickling",
        "Z": "torch.load() uses Python\u2019s unpickling facilities but treats storages,\nwhich underlie tensors, specially. They are first deserialized on the\nCPU and are then moved to the device they were saved from. If this fails\n(e.g. because the run time system doesn\u2019t have certain devices), an exception\nis raised. However, storages can be dynamically remapped to an alternative\nset of devices using the map_location argument.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What happens to storages when they are deserialized on the CPU and moved to the device they were saved from?",
        "Y": "If this fails",
        "Z": "torch.load() uses Python\u2019s unpickling facilities but treats storages,\nwhich underlie tensors, specially. They are first deserialized on the\nCPU and are then moved to the device they were saved from. If this fails\n(e.g. because the run time system doesn\u2019t have certain devices), an exception\nis raised. However, storages can be dynamically remapped to an alternative\nset of devices using the map_location argument.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What indicates the location where all tensors should be loaded?",
        "Y": "map_location is a torch.device object or a string containing a device tag",
        "Z": "If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "If map_location is a dict, it will be used to do what?",
        "Y": "remap location tags appearing in the file",
        "Z": "If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package().",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "How can user extensions register their own location tags and tagging and deserialization methods?",
        "Y": "torch.serialization.register_package()",
        "Z": "If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package().",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "If map_location is a what, it indicates the location where all tensors should be loaded?",
        "Y": "torch.device object or a string containing a device tag",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "If map_location is a torch.device object or a string containing a device tag, it will be used to remap",
        "Y": "a dict",
        "Z": "If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What can user extensions use to register their own location tags and deserialization methods?",
        "Y": "torch.serialization.register_package()",
        "Z": "Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What will be used to remap location tags appearing in a file?",
        "Y": "map_location",
        "Z": "Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "How can user extensions register their own location tags and deserialization methods?",
        "Y": "torch.serialization.register_package()",
        "Z": "Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What type of object has to implement read(), readline(), tell(), and seek()?",
        "Y": "a file-like object",
        "Z": "f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What must a file-like object implement?",
        "Y": "read(), readline(), tell(), and seek()",
        "Z": "Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "If map_location is a what, it will be used to remap location tags appearing in the file?",
        "Y": "dict",
        "Z": "Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is a file-like object that has to implement read(), readline(), tell(), and seek()?",
        "Y": "f",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What does user extensions use to register their own location tags and tagging and deserialization methods?",
        "Y": "torch.serialization.register_package()",
        "Z": "User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What can contain a file name map_location?",
        "Y": "a string or os.PathLike object",
        "Z": "User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file)",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What type of object is f?",
        "Y": "a file-like object",
        "Z": "User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What can user extensions use to register their own location tags and tagging and deserialization methods?",
        "Y": "torch.serialization.register_package()",
        "Z": "User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file)",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is f?",
        "Y": "a file-like object",
        "Z": "Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is the pickle_module module used for?",
        "Y": "unpickling metadata and objects",
        "Z": "map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is the name of the warning message sent to pickle_module.load() and pickle_module.Unpickler()?",
        "Y": "Warning",
        "Z": "map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What does map_location specify?",
        "Y": "remap storage locations",
        "Z": "map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "In what language are pickle_load_args used?",
        "Y": "Python 3",
        "Z": "map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is the name of the warning that is passed over to pickle_module.load() and pickle_module.Unpickler()",
        "Y": "Warning",
        "Z": "pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What module is used for unpickling metadata and objects?",
        "Y": "pickle_module",
        "Z": "pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "Pickle_load_args \u2013 optional keyword arguments passed over to pickle_module.load() and pickle_module.Un",
        "Y": "Python 3",
        "Z": "pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is passed over to pickle_module.load() and pickle_module.Unpickler()?",
        "Y": "optional keyword arguments",
        "Z": "pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What are optional keyword arguments passed over to?",
        "Y": "pickle_module.load() and pickle_module.Unpickler()",
        "Z": "pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "Warning torch.load() uses pickle module implicitly, which is known to be what?",
        "Y": "insecure",
        "Z": "pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "Do you load data that could have come from an untrusted source or that could have been tampered with?",
        "Y": "Never load data that could have come from an untrusted source, or that could have been tampered with",
        "Z": "torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What should you never load data that could have come from an untrusted source or that could have been tampered with?",
        "Y": "Never load data that could have come from an untrusted source, or that could have been tampered with",
        "Z": "pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What should you do when loading data that you trust?",
        "Y": "Note",
        "Z": "pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What uses pickle module implicitly?",
        "Y": "torch.load()",
        "Z": "torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is the pickle module known to be?",
        "Y": "insecure",
        "Z": "torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What does torch.load() do?",
        "Y": "Only load data you trust",
        "Z": "torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What does torch.load() load when a file contains GPU?",
        "Y": "tensors",
        "Z": "torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What does torch.load(.., map_location='cpu') mean?",
        "Y": "map_location='cpu'",
        "Z": "When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What should you do to avoid GPU RAM surge when loading a model checkpoint?",
        "Y": "Note",
        "Z": "torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "By default, we decode byte strings as what?",
        "Y": "utf-8",
        "Z": "Note When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note By default, we decode byte strings as utf-8.  This is to avoid a common error\ncase UnicodeDecodeError: 'ascii' codec can't decode byte 0x...\nwhen loading files saved by Python 2 in Python 3.  If this default\nis incorrect, you may use an extra encoding keyword argument to specify how\nthese objects should be loaded, e.g., encoding='latin1' decodes them\nto strings using latin1 encoding, and encoding='bytes' keeps them\nas byte arrays which can be decoded later with byte_array.decode(...). Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is a common error case?",
        "Y": "UnicodeDecodeError",
        "Z": "Note By default, we decode byte strings as utf-8.  This is to avoid a common error\ncase UnicodeDecodeError: 'ascii' codec can't decode byte 0x...\nwhen loading files saved by Python 2 in Python 3.  If this default\nis incorrect, you may use an extra encoding keyword argument to specify how\nthese objects should be loaded, e.g., encoding='latin1' decodes them\nto strings using latin1 encoding, and encoding='bytes' keeps them\nas byte arrays which can be decoded later with byte_array.decode(...). Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What decodes byte strings using latin1 encoding?",
        "Y": "encoding='latin1'",
        "Z": "Note By default, we decode byte strings as utf-8.  This is to avoid a common error\ncase UnicodeDecodeError: 'ascii' codec can't decode byte 0x...\nwhen loading files saved by Python 2 in Python 3.  If this default\nis incorrect, you may use an extra encoding keyword argument to specify how\nthese objects should be loaded, e.g., encoding='latin1' decodes them\nto strings using latin1 encoding, and encoding='bytes' keeps them\nas byte arrays which can be decoded later with byte_array.decode(...). Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is an example of a byte array that can be decoded later with byte_array.decode(...)?",
        "Y": "Example",
        "Z": "Note When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note By default, we decode byte strings as utf-8.  This is to avoid a common error\ncase UnicodeDecodeError: 'ascii' codec can't decode byte 0x...\nwhen loading files saved by Python 2 in Python 3.  If this default\nis incorrect, you may use an extra encoding keyword argument to specify how\nthese objects should be loaded, e.g., encoding='latin1' decodes them\nto strings using latin1 encoding, and encoding='bytes' keeps them\nas byte arrays which can be decoded later with byte_array.decode(...). Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "The median is not unique for what?",
        "Y": "input tensors with an even number of elements",
        "Z": "Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second\ntensor, which must have dtype long, with their indices in the dimension\ndim of input. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "The median is not unique for input tensors with an even number of elements.",
        "Y": "lower",
        "Z": "The median is not unique for input tensors with an even number\nof elements. In this case the lower of the two medians is returned. To\ncompute the mean of both medians, use torch.quantile() with q=0.5 instead. Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What function computes the mean of both medians?",
        "Y": "torch.quantile()",
        "Z": "Returns the median of the values in input. Note The median is not unique for input tensors with an even number\nof elements. In this case the lower of the two medians is returned. To\ncompute the mean of both medians, use torch.quantile() with q=0.5 instead. Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What type of subgradients does torch.quantile() produce?",
        "Y": "deterministic",
        "Z": "The median is not unique for input tensors with an even number\nof elements. In this case the lower of the two medians is returned. To\ncompute the mean of both medians, use torch.quantile() with q=0.5 instead. Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is the value of the output tensors if keepdim is True?",
        "Y": "Note",
        "Z": "The median is not unique for input tensors with an even number\nof elements. In this case the lower of the two medians is returned. To\ncompute the mean of both medians, use torch.quantile() with q=0.5 instead. Warning This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "If dim is squeezed, outputs tensors have what?",
        "Y": "1 fewer dimension than input",
        "Z": "This function produces deterministic (sub)gradients unlike median(dim=0) input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is the median for input tensors with an even number of elements in the dimension dim?",
        "Y": "not unique",
        "Z": "Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is returned if the input tensors have an even number of elements in the dimension dim?",
        "Y": "lower of the two medians",
        "Z": "By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is returned if the input tensor has an even number of elements in the dimension dim?",
        "Y": "lower of the two medians",
        "Z": "input (Tensor) \u2013 the input tensor. Example: Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Which of the two medians is returned if the input tensors have an even number of elements in the dimension dim?",
        "Y": "lower",
        "Z": "Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "If dim is squeezed, output tensors have how many dimension less than input?",
        "Y": "1",
        "Z": "Returns a namedtuple (values, indices) where values contains the median of each row of input\nin the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of what",
        "Y": "1",
        "Z": "By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Do not expect the same result when run on what?",
        "Y": "CPU and GPU",
        "Z": "Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second\ntensor, which must have dtype long, with their indices in the dimension\ndim of input. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Do not expect the gradients to be what?",
        "Y": "deterministic",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is returned if the output tensors have an even number of elements in the dimension dim?",
        "Y": "lower of the two medians",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Int \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not.",
        "Y": "dim",
        "Z": "Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second\ntensor, which must have dtype long, with their indices in the dimension\ndim of input. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Which tensor will be populated with the median values and the second tensor, which must have dtype long, with their",
        "Y": "out",
        "Z": "Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second\ntensor, which must have dtype long, with their indices in the dimension\ndim of input. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "What is an example of a tensor that is populated with the median values and the second tensor?",
        "Y": "Example",
        "Z": "Note The median is not unique for input tensors with an even number\nof elements in the dimension dim. In this case the lower of the\ntwo medians is returned. To compute the mean of both medians in\ninput, use torch.quantile() with q=0.5 instead. Warning indices does not necessarily contain the first occurrence of each\nmedian value found, unless it is unique.\nThe exact implementation details are device-specific.\nDo not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic. input (Tensor) \u2013 the input tensor. dim (int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second\ntensor, which must have dtype long, with their indices in the dimension\ndim of input. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "Where does PyTorch transfer AlexNet from PyTorch to?",
        "Y": "ONNX",
        "Z": "Example: End-to-end AlexNet from PyTorch to ONNX Tracing vs Scripting Type Annotations Write PyTorch model in Torch way Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Tracing vs what type of annotations Write PyTorch model in Torch way Avoid using numpy Avoid using.data",
        "Y": "Scripting",
        "Z": "Tracing vs Scripting Type Annotations Write PyTorch model in Torch way Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do you write PyTorch model in Torch way Avoid using numpy Avoid using.data field?",
        "Y": "Type Annotations",
        "Z": "Type Annotations Write PyTorch model in Torch way Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Using dictionaries to handle what as model inputs Avoid using numpy Avoid using.data field Using dictionaries to handle what",
        "Y": "Named Arguments",
        "Z": "Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the Operator Export Type ONNX?",
        "Y": "Operator Export Type ONNX",
        "Z": "Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_F",
        "Y": "Custom operators",
        "Z": "Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the program that exports a pretrained AlexNet into ONNX?",
        "Y": "torchvision",
        "Z": "Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of an external data format?",
        "Y": "ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH",
        "Z": "ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the external data format Training Functions?",
        "Y": "ONNX_FALLTHROUGH",
        "Z": "ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the future, there will be what as well?",
        "Y": "backends for other frameworks",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will you need to install to run the exported script?",
        "Y": "caffe2",
        "Z": "To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the backend for the exported model?",
        "Y": "ONNX Runtime",
        "Z": "You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another tutorial for?",
        "Y": "exporting the SuperResolution model to ONNX",
        "Z": "Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will there be in the future for other frameworks as well?",
        "Y": "backends",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What means that ONNX operates by executing your model once, and exporting the operators which were actually run during this run?",
        "Y": "trace-based",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does a dynamic model do?",
        "Y": "changes behavior depending on input data",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a trace likely to be valid only for?",
        "Y": "a specific input size",
        "Z": "To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What look reasonable in a model trace?",
        "Y": "traced operators",
        "Z": "You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Which exporter will unroll loops and if conditions?",
        "Y": "trace-based exporter",
        "Z": "The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If you want to export your model with dynamic control flows, you will need to use what?",
        "Y": "script-based exporter",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What looks reasonable in a model trace?",
        "Y": "traced operators",
        "Z": "In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does a trace-based exporter do if your model contains control flows like for loops and if conditions?",
        "Y": "unroll the loops and if conditions",
        "Z": "The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What means that it operates by executing your model once, and exporting the operators which were actually run during this run?",
        "Y": "trace-based",
        "Z": "trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does a trace-based exporter do if your model contains control flows?",
        "Y": "unroll the loops and if conditions",
        "Z": "trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are the torch tensors and torch operators?",
        "Y": "torch.concat",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e., The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What layer need to be defined in init function so that inferencing can handle it properly?",
        "Y": "Dropout layer",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e., The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can the use of the.data field produce?",
        "Y": "incorrect trace graph",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e., The .data field is an old field that is kept for backward compatibility and should be avoided when writing models.\nIt\u2019s usage is dangerous and can make computations wrong, furthermore it can produce an incorrect trace graph and\ntherefore an incorrect ONNX graph. A safer alternative is to use .detach() instead.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the torch operators?",
        "Y": "torch.concat",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: do not convert to numpy types: Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the latest version of opset?",
        "Y": "opset",
        "Z": "Tensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is below the list of supported patterns for RHS indexing?",
        "Y": "unsupported patterns",
        "Z": "Tensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is below the list of supported patterns for LHS indexing?",
        "Y": "unsupported patterns",
        "Z": "Tensor indexing in PyTorch is very flexible and complicated.\nThere are two categories of indexing. Both are largely supported in exporting today.\nIf you are experiencing issues exporting indexing that belongs to the supported patterns below,\nplease double check that you are exporting with the latest opset (opset_version=12). This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: Below is the list of supported patterns for RHS indexing. And below is the list of unsupported patterns for RHS indexing. In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: Below is the list of supported patterns for LHS indexing. And below is the list of unsupported patterns for LHS indexing. If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that\nyou are exporting with the latest opset (opset_version=12).",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Users need to verify their dict inputs carefully, and keep in mind that what is not available?",
        "Y": "dynamic lookups",
        "Z": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted\nbut their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that\ndynamic lookups are not available. PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some\nnumeric differences.  Depending on model structure, these differences\nmay be negligible, but they can also cause major divergences in behavior\n(especially on untrained models.)  We allow Caffe2 to call directly to Torch implementations of operators, to\nhelp you smooth over these differences when precision is important,\nand to also document these differences.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Which backends often have implementations of operators with some numeric differences?",
        "Y": "PyTorch and ONNX backends",
        "Z": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted\nbut their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that\ndynamic lookups are not available. PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some\nnumeric differences.  Depending on model structure, these differences\nmay be negligible, but they can also cause major divergences in behavior\n(especially on untrained models.)  We allow Caffe2 to call directly to Torch implementations of operators, to\nhelp you smooth over these differences when precision is important,\nand to also document these differences.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "FeatureDropout (not supported) Index MaxPool1d MaxPool2d MaxPool3d",
        "Y": "training mode",
        "Z": "FeatureDropout (training mode not supported) Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is pixel_shuffle pow?",
        "Y": "pixel_shuffle pow",
        "Z": "Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is not supported in pixel_shuffle pow prelu?",
        "Y": "single weight shared among input channels",
        "Z": "MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones",
        "Y": "multinomial",
        "Z": "adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the single weight shared among input channels not supported?",
        "Y": "pixel_shuffle pow prelu",
        "Z": "acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is sigmoid sign sin size slice softmax?",
        "Y": "sigmoid sign sin size slice softmax",
        "Z": "addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a sin size slice softmax softplus?",
        "Y": "sigmoid sign",
        "Z": "and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is sigmoid sign sin size slice softmax softplus sort?",
        "Y": "sigmoid sign sin size slice softmax softplus sort",
        "Z": "arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a sin size slice softmax softplus sort split sqrt squeeze?",
        "Y": "sigmoid sign",
        "Z": "asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split sqrt squeeze",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What avg_pool3d is as_strided baddbmm bitshift cat ceil?",
        "Y": "avg_pool3d",
        "Z": "atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split sqrt squeeze stack",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the sigmoid sign?",
        "Y": "t tan",
        "Z": "as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split sqrt squeeze stack std sub (nonzero alpha not supported) sum t tan",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is tan tanh?",
        "Y": "tan tanh",
        "Z": "celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split sqrt squeeze stack std sub (nonzero alpha not supported) sum t tan tanh",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the term for tanh?",
        "Y": "tan tanh",
        "Z": "cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardsigmoid hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split sqrt squeeze stack std sub (nonzero alpha not supported) sum t tan tanh",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "To add export support for operators, developers need to touch the source code of what?",
        "Y": "PyTorch",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do developers install PyTorch from source?",
        "Y": "follow the instructions",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What should be easy to add support for exporting if the wanted operator is standardized in ONNX?",
        "Y": "adding a symbolic function for the operator",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If the operator is standardized in ONNX, what is the operator defined in VariableType.h?",
        "Y": "ATen",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If the wanted operator is what in ONNX, it should be easy to add support for exporting such operator?",
        "Y": "standardized",
        "Z": "To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the operator defined in VariableType.h?",
        "Y": "ATen",
        "Z": "To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of operator is the operator?",
        "Y": "ATen operator",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The first parameter is always the exported what graph?",
        "Y": "ONNX",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must Parameter names EXACTLY match the names in VariableType.h?",
        "Y": "Parameter names must EXACTLY match the names in VariableType.h",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Inputs are always first, then non-tensor arguments.",
        "Y": "tensors",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the symbolic function in the corresponding Function class?",
        "Y": "Create a symbolic function named symbolic",
        "Z": "If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent the ON",
        "Y": "if the operator is already standardized in ONNX",
        "Z": "In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in ONNX, we just need to do what?",
        "Y": "create a node to represent the ONNX operator in the graph",
        "Z": "In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If the operator is a non-ATen operator, the symbolic function has to be added in what?",
        "Y": "PyTorch Function class",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of function is created in the corresponding PyTorch Function class?",
        "Y": "symbolic",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do we try to do with missing symbolic function for elu operator?",
        "Y": "export the model",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in ONNX, what do we need to do to represent the ONNX operator",
        "Y": "create a node",
        "Z": "Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do we try to do with the missing symbolic function for elu operator?",
        "Y": "export the model",
        "Z": "Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do we try to do with the missing symbolic function?",
        "Y": "export the model",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Why does the export fail?",
        "Y": "PyTorch does not support exporting elu operator",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operator does PyTorch not support exporting?",
        "Y": "virtual Tensor elu",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operator is standardized in ONNX?",
        "Y": "Elu",
        "Z": "The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who is able to export elu operator?",
        "Y": "PyTorch",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the const override in VariableType.h?",
        "Y": "virtual Tensor elu",
        "Z": "Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another example of PyTorch exporting elu operator?",
        "Y": "symbolic_opset10.py",
        "Z": "Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is PyTorch able to export?",
        "Y": "elu operator",
        "Z": "Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "PyTorch is able to export elu operator in what languages?",
        "Y": "symbolic_opset9.py, symbolic_opset10.py",
        "Z": "Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What tutorial will allow you to create and register your own custom ops implementation in PyTorch?",
        "Y": "Extending TorchScript with Custom C++ Operators",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where can you export your own custom ops implementation?",
        "Y": "ONNX",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you export a custom operator as?",
        "Y": "one or a combination of existing ONNX ops",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you export the model as in ONNX?",
        "Y": "a custom op",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you specify when exporting a custom opset?",
        "Y": "custom domain and version",
        "Z": "Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If not explicitly specified, the custom opset version is set to what by default?",
        "Y": "1",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will you need to extend the backend of your choice with matching custom ops implementation?",
        "Y": "custom ONNX ops",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of a custom ops implementation?",
        "Y": "Caffe2",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operator does PyTorch not support exporting elu?",
        "Y": "virtual Tensor elu",
        "Z": "The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are examples of PyTorch's exporting elu operator?",
        "Y": "symbolic_opset9.py, symbolic_opset10.py",
        "Z": "The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where can you export a custom ops implementation in PyTorch?",
        "Y": "ONNX",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do you use to specify the custom domain and version?",
        "Y": "custom_opsets dictionary",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who will try to handle implicit scalar datatype casting?",
        "Y": "the exporter",
        "Z": "Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will you need to do if the exporter fails to figure out the right datatype for scalars?",
        "Y": "manually provide the datatype information",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of models are not recorded in ONNX?",
        "Y": "scripted models",
        "Z": "Support for primitive type inputs will be added starting from PyTorch 1.9 release.\nHowever, exporter does not support conversion of models with String inputs. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is ONNX trying to do with scripted models?",
        "Y": "improve the datatype propagation in the exporter",
        "Z": "Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What version of ONNX supports tensor in-place indexed assignment?",
        "Y": "ONNX opset",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What version of ONNX supports data[index] = new_data?",
        "Y": "ONNX opset version >= 11",
        "Z": "Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are we trying to do to prevent manual changes in the future?",
        "Y": "improve the datatype propagation in the exporter",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX?",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What argument in export API enables export of models in ONNX external data format?",
        "Y": "use_external_data_format",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of models can be exported to ONNX with the use_external_data_format argument?",
        "Y": "large models",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Models larger than what size cannot be exported in one file because of the protobuf size limit?",
        "Y": "2GB",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported\nin one file because of the protobuf size limit. Users should set use_external_data_format to\nTrue to successfully export such models.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Export a model into what format?",
        "Y": "ONNX format",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does a TUPLE OF ARGUEMENTS contain?",
        "Y": "A DICTIONARY OF NAMED PARAMETERS",
        "Z": "args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will become inputs of the exported model?",
        "Y": "any Tensor arguments",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If args is a Tensor, this is equivalent to having called it with what?",
        "Y": "1-ary tuple",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are the inputs to the model structured as?",
        "Y": "a tuple consisting of non-keyword arguments",
        "Z": "The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the model below provide an example of?",
        "Y": "return x",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the last value of a tuple consisting of non-keyword arguments?",
        "Y": "DICTIONARY OF NAMED PARAMETERS",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What model provides an example of a conflict when a dictionary of named parameters is used?",
        "Y": "model below",
        "Z": "Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What provides an example of a tuple with a dictionary of named parameters?",
        "Y": "model",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does model() k represent?",
        "Y": "torch.randn(2, 3) x",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What provides an example of a tuple in which dictionary input is the last input of the args tuple?",
        "Y": "model",
        "Z": "The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does model() k mean?",
        "Y": "torch.randn(2, 3) x",
        "Z": "The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "m = Model() k = torch.randn(2, 3) x = torch.tensor(1",
        "Y": "x",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is provided as the last input in the tuple args?",
        "Y": "an empty dictionary",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would the new call to export look like?",
        "Y": "this",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The new call would look like what?",
        "Y": "this",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What = Model() k = torch.randn(2, 3) x = torch.tensor(1)",
        "Y": "m",
        "Z": "m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the last input in the tuple args?",
        "Y": "provide an empty dictionary",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of object does torch.onnx.export(model, (k, x, ), \u2018test.onnx",
        "Y": "a file-like object",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the export function?",
        "Y": "export_params",
        "Z": "In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does model.state_dict().values() specify?",
        "Y": "model.state_dict().values()",
        "Z": "In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does f represent?",
        "Y": "a file-like object",
        "Z": "torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the export function if all parameters are specified?",
        "Y": "export_params",
        "Z": "torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would happen if the export function assumed that the x input was intended to represent the optional dictionary consisting of named arguments?",
        "Y": "would work as intended",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the export function that if specified, all parameters will be exported?",
        "Y": "export_params",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Training (enum, default) \u2013 TrainingMode.EVAL: export the model in inference mode.",
        "Y": "TrainingMode.EVAL",
        "Z": "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What exports the model in inference mode if model.training is False and to a training friendly mode if model.training is",
        "Y": "TrainingMode.PRESERVE",
        "Z": "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What mode is used to export a model in a training friendly mode?",
        "Y": "TrainingMode.TRAINING",
        "Z": "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What if specified, all parameters will be exported. Set this to False if you want to export an untrained model?",
        "Y": "export_params",
        "Z": "f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the training friendly mode that the model is exported in?",
        "Y": "TrainingMode.TRAINING",
        "Z": "export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "All parameters will be exported if specified?",
        "Y": "export_params",
        "Z": "export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If model.training is set to what, the model will be exported in inference mode?",
        "Y": "False",
        "Z": "export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If model.training is true, what is the default?",
        "Y": "False",
        "Z": "verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What mode is used to export the model in a training friendly mode?",
        "Y": "TrainingMode.TRAINING",
        "Z": "verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What exports the internal IR directly instead of converting it to ONNX ops?",
        "Y": "export_raw_ir",
        "Z": "export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Export raw_ir directly instead of converting it to what?",
        "Y": "ONNX ops",
        "Z": "verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If model.training is what, what is the default value for training mode?",
        "Y": "False",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default TrainingMode.PRESERVE?",
        "Y": "TrainingMode.TRAINING",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names",
        "Y": "input_names",
        "Z": "\u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default False value for the internal IR exported by the functions in symbolic_opsetversion>.py?",
        "Y": "export_raw_ir",
        "Z": "input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does export_raw_ir convert the internal IR directly instead of converting it to?",
        "Y": "ONNX ops",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "input_names (list of strings, what list) \u2013 names to assign to the input nodes of the graph?",
        "Y": "default empty list",
        "Z": "input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Operator_export_type (enum, default) \u2013 OperatorExportTypes.ONNX: All ops are exported as regular",
        "Y": "OperatorExportTypes.ONNX",
        "Z": "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is exported as:",
        "Y": "Example graph",
        "Z": "export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default False value of the model exported in aten mode?",
        "Y": "aten",
        "Z": "aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of ir is exported?",
        "Y": "OperatorExportTypes.RAW",
        "Z": "export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the mode that can be exported and implemented by the user for their runtime backend?",
        "Y": "OperatorExportTypes.ONNX_FALLTHROUGH",
        "Z": "export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who can implement a custom ONNX op?",
        "Y": "the user",
        "Z": "In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does OperatorExportTypes.ONNX use to export raw ir?",
        "Y": "OperatorExportTypes.RAW",
        "Z": "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another name for OperatorExportTypes.ONNX_FALLBACK?",
        "Y": "OperatorExportTypes.ONNX_FALLTHROUGH",
        "Z": "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is not supported in the above example graph?",
        "Y": "prim::ListConstruct",
        "Z": "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the namespace for ONNX ops?",
        "Y": "OperatorExportTypes.ONNX",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the ir that is exported?",
        "Y": "OperatorExportTypes.RAW",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another name for OperatorExportTypes.ONNX_ATEN_FALLBACK?",
        "Y": "OperatorExportTypes.ONNX_FALLTHROUGH",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of op can be exported as a custom ONNX op?",
        "Y": "OperatorExportTypes.RAW",
        "Z": "is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can be exported and implemented by the user for their runtime backend?",
        "Y": "OperatorExportTypes.ONNX_FALLTHROUGH",
        "Z": "In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default for ONNX's latest opset?",
        "Y": "one stable opset version",
        "Z": "is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the ir that can be exported in ONNX?",
        "Y": "OperatorExportTypes.RAW",
        "Z": "In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will replace some of the ops that have all constant inputs, with pre-computed constant nodes?",
        "Y": "Constant-folding optimization",
        "Z": "In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the default version of the onnx submodule?",
        "Y": "opset_version",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is applied to the model during export?",
        "Y": "do_constant_folding",
        "Z": "do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the model's example outputs being exported?",
        "Y": "example_outputs",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does strip_doc_string do?",
        "Y": "if True, strips the field \u201cdoc_string\u201d from the exported model",
        "Z": "do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What field is stripped from the exported model?",
        "Y": "dynamic_axes",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does this allow for backends/runtimes that execute these graphs?",
        "Y": "better optimizations",
        "Z": "INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who executes dynamic axes?",
        "Y": "backends/runtimes",
        "Z": "Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If unspecified, the behavior is chosen automatically as follows.",
        "Y": "default None",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is operator_export_type?",
        "Y": "OperatorExportTypes",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to what?",
        "Y": "setting this argument to True",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must be part of graph inputs for ONNX opset version  9?",
        "Y": "initializers",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If ONNX opset version  9, what argument will be ignored?",
        "Y": "opset_version argument is set to a 8 or lower",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who executes the graphs?",
        "Y": "backends/runtimes",
        "Z": "ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If unspecified, what is the default?",
        "Y": "default None",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who executes graphs if initializers are not added as inputs?",
        "Y": "backends/runtimes",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If unspecified, what is the behavior chosen automatically as follows?",
        "Y": "default None",
        "Z": "INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If true, initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs?",
        "Y": "If False",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The torch package contains data structures for what?",
        "Y": "multi-dimensional tensors",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "If the data type of input is a what?",
        "Y": "complex data type",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What efault floating point dtype is set to d?",
        "Y": "d",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the default torch.Tensor type to floating point tensor type t?",
        "Y": "t",
        "Z": "The torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serializing of\nTensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns True if the data type of input is a floating point data type?",
        "Y": "if the data type of input is a floating point data type",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns True if the input is a single element tensor?",
        "Y": "if the input is a single element tensor",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Sets what efault floating point dtype to d?",
        "Y": "d",
        "Z": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does a tensor contain?",
        "Y": "data",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.Tensor type return?",
        "Y": "the total number of elements in the input tensor",
        "Z": "Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the data in the sparse tensor in COO(rdinate) format convert into?",
        "Y": "torch.Tensor",
        "Z": "Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is created with specified size, stride and storage_offset?",
        "Y": "a view of an existing torch.Tensor input",
        "Z": "Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the current floating point torch.dtype?",
        "Y": "current default",
        "Z": "Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of tensor of size steps is created?",
        "Y": "one-dimensional",
        "Z": "Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.   Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}basestart to baseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What fills a tensor of size size filled with?",
        "Y": "fill_value",
        "Z": "Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}basestart to baseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of size size filled with fill_value.   Returns a tensor with the same size as input filled with fill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor with the same size as input filled with what?",
        "Y": "fill_value",
        "Z": "Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}basestart to baseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of size size filled with fill_value.   Returns a tensor with the same size as input filled with fill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens when horizontally stacking the tensors in tensors?",
        "Y": "Creates a new tensor",
        "Z": "Concatenates the given sequence of seq tensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Gathers values along what axis specified by dim. Splits input into multiple tensors horizontally according to indices_or",
        "Y": "axis",
        "Z": "Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Splits input, a tensor with one or more dimensions, into what horizontally according to indices_or_sections?",
        "Y": "multiple tensors",
        "Z": "Concatenates the given sequence of seq tensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How are tensors stacked horizontally?",
        "Y": "column wise",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Splits input into what according to indices_or_sections?",
        "Y": "multiple tensors horizontally",
        "Z": "Splits a tensor into a specific number of chunks.   Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.   Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is used to split a tensor into multiple tensors?",
        "Y": "indices_or_sections",
        "Z": "Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is an out-of-place version of torch.Tensor.scatter_()?",
        "Y": "Alias of torch.vstack()",
        "Z": "Creates a new tensor by horizontally stacking the tensors in tensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What version of torch.Tensor.scatter_() Splits the tensor into chunks?",
        "Y": "Out-of-place",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Gathers values along what axis specified by dim?",
        "Y": "axis",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does it do to split a tensor in sequence horizontally?",
        "Y": "Stack tensors",
        "Z": "Gathers values along an axis specified by dim.   Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.   Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.   Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor with all the dimensions of input of what size removed?",
        "Y": "size 1",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What dimensions does torch.transpose() transpose?",
        "Y": "0 and 1",
        "Z": "Alias for torch.movedim().   Returns a new tensor that is a narrowed version of input tensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a new tensor with the elements of input at the given what?",
        "Y": "indices",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Selects values from input at what indices from indices along the given dim?",
        "Y": "1-dimensional",
        "Z": "Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias of torch.vstack().   Out-of-place version of torch.Tensor.scatter_()   Out-of-place version of torch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions of input of size 1 removed.   Concatenates a sequence of tensors along a new dimension.   Alias for torch.transpose().   Alias for torch.transpose().   Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements of input at the given indices.   Selects values from input at the 1-dimensional indices from indices along the given dim.   Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.   Constructs a tensor by repeating the elements of input.   Returns a tensor that is a transposed version of input.   Removes a tensor dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "From what distribution are binary random numbers drawn?",
        "Y": "Bernoulli",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the",
        "Y": "a tensor filled with random numbers",
        "Z": "Returns the initial seed for generating random numbers as a Python long.   Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Where are binary random numbers drawn from?",
        "Y": "a Bernoulli distribution",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is sampled from the multinomial probability distribution located in the corresponding row of tensor input?",
        "Y": "num_samples indices",
        "Z": "Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor of the same size as input with each element sampled from what distribution with rate parameter given by the corresponding element",
        "Y": "Poisson",
        "Z": "Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned from a uniform distribution on the interval?",
        "Y": "a tensor filled with random numbers",
        "Z": "Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when a tensor input is filled with random numbers from a normal distribution with mean 0 and variance 1?",
        "Y": "a tensor with the same size",
        "Z": "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Tensor.geometric_() - elements drawn from what distribution?",
        "Y": "the geometric distribution",
        "Z": "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when input is filled with random numbers from a normal distribution with mean 0 and variance 1?",
        "Y": "a tensor with the same size",
        "Z": "Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What distribution does tensor.log_normal_() sample from?",
        "Y": "log-normal",
        "Z": "Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the context managers torch.no_grad(), torch.enable_grad(), and torch.set_grad_",
        "Y": "Locally disabling gradient computation",
        "Z": "The context managers torch.no_grad(), torch.enable_grad(), and\ntorch.set_grad_enabled() are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the threading module, etc. Examples:   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Returns True if grad mode is currently enabled.   Context-manager that enables or disables inference mode   Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Alias for torch.acos(). Returns a new tensor with what cosine of the elements of input?",
        "Y": "inverse hyperbolic",
        "Z": "Alias for torch.abs()   Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What adds the scalar other to each element of the input and returns a new resulting tensor?",
        "Y": "Alias for torch.acosh()",
        "Z": "Computes the absolute value of each element in input.   Alias for torch.abs()   Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Alias for what function returns a new tensor with the arctangent of the elements of input?",
        "Y": "torch.atan()",
        "Z": "Computes the absolute value of each element in input.   Alias for torch.abs()   Computes the inverse cosine of each element in input.   Alias for torch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs what element-wise division of tensor1 by tensor2?",
        "Y": "multiplication",
        "Z": "Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Which function returns a new tensor with the inverse hyperbolic sine of the elements of input?",
        "Y": "Alias for torch.asin()",
        "Z": "Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Alias for return a new tensor with the inverse hyperbolic tangent of the elements of input?",
        "Y": "torch.atanh()",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Computes the bitwise AND of input and other?",
        "Y": "Computes the bitwise AND",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Computes the bitwise OR of input and other?",
        "Y": "Computes the bitwise OR of input and other",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the bitwise NOT of the given input tensor. Computes the bitwise OR of input and other.",
        "Y": "Computes the bitwise AND",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the bitwise what of input and other?",
        "Y": "XOR",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What Alias for torch.atan(). Returns a new tensor with the inverse hyperbolic tangent of",
        "Y": "torch.atanh()",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.clamp() do?",
        "Y": "Clamps all elements in input into the range",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the function that clamps all elements in input into the range?",
        "Y": "Alias for torch.clamp()",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the element-wise arctangent of inputi/otheritextinput_i / text",
        "Y": "Element-wise arctangent",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the bitwise AND of input and other. Computes the bitwise XOR of input and other.",
        "Y": "Computes the bitwise AND of input and other",
        "Z": "Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Alias for computes the element-wise conjugate of the given input tensor?",
        "Y": "torch.clamp()",
        "Z": "Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the function that computes the element-wise conjugate of the given input tensor?",
        "Y": "torch.clamp()",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Returns the indices of the minimum value(s) of the flattened tensor or along a dimension?",
        "Y": "the indices of the minimum value(s) of the flattened tensor or along a dimension",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the maximum value of all elements in the input tensor.",
        "Y": "minimum value",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the log of summed exponentials of each row of the input tensor in the given dimension dim. Returns the mean value",
        "Y": "p-norm",
        "Z": "Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the maximum value of all elements in the input tensor. Returns what value of all elements in the input tensor?",
        "Y": "minimum value",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the maximum value of all elements in the input tensor. Returns the minimum value of each slice of the input tensor",
        "Y": "True",
        "Z": "Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the maximum value of all elements in the input tensor. Returns the minimum value of all elements in what?",
        "Y": "input tensor",
        "Z": "Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Returns the sum of all elements in the input tensor return?",
        "Y": "the product of all elements in the input tensor",
        "Z": "Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.   Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is used to test if all elements in input evaluate to True?",
        "Y": "input tensor",
        "Z": "Tests if all elements in input evaluate to True.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input - other)   Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "This is a variant of what function that \"ignores\" NaN values?",
        "Y": "torch.quantile()",
        "Z": "Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What will be used if unbiased is True?",
        "Y": "Bessel\u2019s correction",
        "Z": "Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "If unbiased is True, what will be used to calculate the standard deviation?",
        "Y": "Bessel\u2019s correction",
        "Z": "Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "This is a variant of what function that ignores NaN values?",
        "Y": "torch.quantile()",
        "Z": "Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   If unbiased is True, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the sum of all elements in the input tensor. Eliminates all but the first element from every consecutive group of equivalent elements.",
        "Y": "unique elements",
        "Z": "Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   If unbiased is True, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens to every consecutive group of equivalent elements in the input tensor?",
        "Y": "Eliminates all but the first element",
        "Z": "Returns the median of the values in input.   Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what of the values in input, ignoring NaN values?",
        "Y": "the median",
        "Z": "Returns the median of the values in input, ignoring NaN values.   Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimension dim.   This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.   If unbiased is True, Bessel\u2019s correction will be used.   If unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   If unbiased is True, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What can be done from provided tensors?",
        "Y": "Create a block diagonal matrix",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens to the given tensors according to Broadcasting semantics?",
        "Y": "Broadcasts",
        "Z": "Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the indices of what to which each value in the input belongs?",
        "Y": "buckets",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What shape does broadcasts input to?",
        "Y": "shape",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does broadcast_tensors() return of the buckets to which each value in the input belongs?",
        "Y": "the indices",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the given sequence of tensors?",
        "Y": "Do cartesian product",
        "Z": "Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when values is the cumulative minimum of elements of input in the dimension dim?",
        "Y": "the cumulative product of elements of input in the dimension dim",
        "Z": "Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when a namedtuple (values, indices) is called?",
        "Y": "the cumulative sum of elements of input in the dimension dim",
        "Z": "Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does it do when it returns a copy of input?",
        "Y": "Compute combinations of length rrr of the given tensor",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when a namedtuple (values, indices) is the cumulative minimum of elements of input in the dimension dim?",
        "Y": "the cumulative product of elements of input in the dimension dim",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned if input is a vector?",
        "Y": "the cumulative sum of elements of input in the dimension dim",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "If input is a vector, then returns a 2-D square tensor.",
        "Y": "1-D tensor",
        "Z": "Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does return a copy of input do?",
        "Y": "Compute combinations of length rrr of the given tensor",
        "Z": "Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "If input is a vector, then returns a 2-D square tensor Creates a tensor whose diagonals of",
        "Y": "1-D tensor",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a partial view of input with its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the",
        "Y": "2-D square tensor",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned when a namedtuple returns the cumulative product of elements of input in the dimension dim?",
        "Y": "the cumulative sum of elements of input in the dimension dim",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the difference along the given dimension?",
        "Y": "n-th forward",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens to the outer-product of vectors vec1 and vec2?",
        "Y": "adds it to the matrix input",
        "Z": "Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Solves a linear system of equations with what to be inverted given its Cholesky factor matrix uuu?",
        "Y": "positive semidefinite matrix",
        "Z": "Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the batch matrix-matrix product return?",
        "Y": "matrix product of the NNN 2-D tensors",
        "Z": "Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Alias for torch.linalg.det() Calculates what of a square matrix or batches of square matrices?",
        "Y": "log determinant",
        "Z": "Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the function that calculates the log determinant of a square matrix or batches of square matrices?",
        "Y": "torch.linalg.slogdet()",
        "Z": "Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Alias for torch.linalg.det() calculate of a square matrix or batches of square matrices?",
        "Y": "log determinant",
        "Z": "Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the function that calculates the log determinant of a square matrix?",
        "Y": "torch.linalg.slogdet()",
        "Z": "Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the LU solve of the linear system?",
        "Y": "Ax=bAx = bAx=b",
        "Z": "Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the result of the LU factorization of a tensor into tensors L and U and a permut",
        "Y": "LU_data, LU_pivots",
        "Z": "Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does LU_data, LU_pivots = (P @ L @ U)) calculate?",
        "Y": "Matrix product of two tensors",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the dot product for 1D tensors. Alias of what function?",
        "Y": "torch.outer()",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.   Computes the singular value decomposition of either a matrix or batch of matrices input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does LU_data, LU_pivots = (P @ L @ U)) represent?",
        "Y": "Matrix product of two tensors",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the low-level function for calling LAPACK's geqrf directly?",
        "Y": "torch.outer()",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does torch.outer() do?",
        "Y": "Computes the dot product for 1D tensors",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned by LU_data, LU_pivots?",
        "Y": "Matrix product of two tensors",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Alias for torch.linalg.matrix_power() return?",
        "Y": "numerical rank of a 2-D tensor",
        "Z": "Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the Alias that computes the dot product for 1D tensors?",
        "Y": "torch.outer()",
        "Z": "Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does LU_data, LU_pivots return?",
        "Y": "Matrix product of two tensors",
        "Z": "Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes what of a square matrix or of each square matrix in a batch?",
        "Y": "the matrix exponential",
        "Z": "Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What release is FX under?",
        "Y": "Beta",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are recorded on Proxies?",
        "Y": "Operations",
        "Z": "FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the intermediate representation consist of?",
        "Y": "Nodes",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can more information about the IR be found?",
        "Y": "documentation for Graph",
        "Z": "The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we create valid Python code matching the Graph's semantics?",
        "Y": "Graph IR",
        "Z": "Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a torch.nn.Module instance that holds a Graph as well as a forward method generated from the Graph",
        "Y": "GraphModule",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the pipeline of components of FX?",
        "Y": "Python-to-Python transformation pipeline",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can components be used in the Python-to-Python transformation pipeline of FX?",
        "Y": "components can be used separately",
        "Z": "Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be used in isolation to capture a form of the code for analysis (and not transformation) purposes?",
        "Y": "symbolic tracing",
        "Z": "Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Code generation can be used for what?",
        "Y": "programmatically generating models",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How many uses are there for FX?",
        "Y": "many uses",
        "Z": "Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can you find examples of FX transformations?",
        "Y": "examples repository",
        "Z": "Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be used separately?",
        "Y": "components",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can you find examples of FX transforms?",
        "Y": "examples repository",
        "Z": "Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does an FX transform take in?",
        "Y": "torch.nn.Module",
        "Z": "Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can you pass the torch.nn.Module to?",
        "Y": "TorchScript",
        "Z": "Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the FX transform that you can pass it to?",
        "Y": "TorchScript",
        "Z": "What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does your transform take in?",
        "Y": "torch",
        "Z": "Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a data structure that represents a method on a GraphModule?",
        "Y": "Graph",
        "Z": "Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of a Graph?",
        "Y": "a short example",
        "Z": "Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of a GraphModule?",
        "Y": "a short example",
        "Z": "It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the inputs to a GraphModule?",
        "Y": "operations",
        "Z": "Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the return value from a GraphModule?",
        "Y": "output",
        "Z": "Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {}",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What method does the module MyModule call to print out a table showing the nodes of the Graph?",
        "Y": "Graph.print_tabular()",
        "Z": "Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the output value from a GraphModule?",
        "Y": "return",
        "Z": "Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {}",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What method is used to print out a table showing the nodes of a Graph?",
        "Y": "Graph.print_tabular()",
        "Z": "Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph. Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {}",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A Graph is a data structure that represents a method on what?",
        "Y": "GraphModule",
        "Z": "Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the \u2018dim\u2019 of a GraphModule?",
        "Y": "-1",
        "Z": "Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is: What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the output (i.e. return) value from the method?",
        "Y": "operations",
        "Z": "What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "All three concepts are represented by what?",
        "Y": "Node instances",
        "Z": "What are the operations that run inside the method? What is the output (i.e. return) value from the method? All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What module is defined for demonstration purposes?",
        "Y": "MyModule",
        "Z": "All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the module we define for demonstration purposes?",
        "Y": "MyModule",
        "Z": "Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does opcode name target args kwargs placeholder x x?",
        "Y": "opcode name target args kwargs placeholder x x",
        "Z": "opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the inputs to a method specified via special placeholder nodes?",
        "Y": "inputs",
        "Z": "(x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the target of a single placeholder node?",
        "Y": "x",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do the placeholder nodes represent in a method?",
        "Y": "operations",
        "Z": "get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do the get_attr, call_function, call_module, and call_method nodes represent?",
        "Y": "operations in the method",
        "Z": "{} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can a full treatment of the semantics of all of the nodes be found?",
        "Y": "Node documentation",
        "Z": "(x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do the placeholder nodes represent within a method?",
        "Y": "operations",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Which nodes represent the operations in the method?",
        "Y": "get_attr, call_function, call_module, and call_method",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the return value of a method?",
        "Y": "return value",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the return value of the method specified by a special output node?",
        "Y": "Graph",
        "Z": "<built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the return value of a method specified by a special output node?",
        "Y": "return value",
        "Z": "placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is get_attr linear_weight?",
        "Y": "linear.weight",
        "Z": "get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The return value in a Graph is specified by what?",
        "Y": "a special output node",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the term for x, linear_weight?",
        "Y": "linear.weight",
        "Z": "linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the _weight of the built-in function add?",
        "Y": "linear",
        "Z": "<built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the operations within the method?",
        "Y": "operations",
        "Z": "() {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is x?",
        "Y": "linear_weight",
        "Z": "(x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we explore now that we know the basics of how code is represented in FX?",
        "Y": "how we would edit a Graph",
        "Z": "opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can several example transformations be found?",
        "Y": "examples repository",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the add_1 method?",
        "Y": "linear_1",
        "Z": "linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is built-in method sum...> (relu_1)?",
        "Y": "call_function sum_1",
        "Z": "(add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can a full treatment of the semantics of all of the operations in a method be found?",
        "Y": "Node documentation",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the linear_1 method?",
        "Y": "call_method relu_1 relu",
        "Z": "call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Which nodes represent the operations in a method?",
        "Y": "get_attr, call_function, call_module, and call_method",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is one way to build a new graph?",
        "Y": "directly manipulate your old one",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we do with the Graph we obtain from symbolic tracing?",
        "Y": "modify it",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the call we want to replace torch.add() with?",
        "Y": "torch.mul()",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of using the APIs to append a call to a Graph?",
        "Y": "torch.relu()",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "FX provides another level of what on top of direct graph manipulation?",
        "Y": "automation",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What API is essentially a \u201cfind/replace\u201d tool for editing Graphs?",
        "Y": "replace_pattern()",
        "Z": "FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the replace_pattern() API do?",
        "Y": "trace through those functions",
        "Z": "We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can tedious graph manipulation code get as the transformations get more complex?",
        "Y": "unwieldy",
        "Z": "We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the usage of replace_pattern?",
        "Y": "Basic",
        "Z": "We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What provides another level of automation on top of direct graph manipulation?",
        "Y": "FX",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can replace_pattern() greatly automate?",
        "Y": "graph manipulation code",
        "Z": "For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How many op Conv/Batch Norm fusions does replace_pattern() replace?",
        "Y": "one",
        "Z": "For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can happen to tedious graph manipulation code as the transformations get more complex?",
        "Y": "can get unwieldy",
        "Z": "FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is used in symbolic tracing?",
        "Y": "Proxy machinery",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What would a transformation write that would transform every F.relu(x) call into (x > 0) * x?",
        "Y": "decomposed PyTorch functions into smaller operations",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What would a transformation that decomposed PyTorch functions into smaller operations do?",
        "Y": "every F.relu(x) call into (x > 0) * x",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What would be performed to insert comparison and multiplication after the F.relu?",
        "Y": "graph rewriting",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be used to automate the process of rewriting a graph?",
        "Y": "Proxy objects",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are Proxy objects called?",
        "Y": "arugments",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do Proxy objects do with the operations that are performed on them?",
        "Y": "append them to the Graph",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we use to invoke PyTorch code?",
        "Y": "Proxy objects",
        "Z": "To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do Proxy objects do?",
        "Y": "capture the operations that are performed on them and append them to the Graph",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Proxys allow you to specify your rewrite rules as what?",
        "Y": "native Python code",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can you find an example of using Proxys for Graph manipulation?",
        "Y": "here",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we want to run and record the torch.Tensor shape and dtype properties on the nodes?",
        "Y": "GraphModule",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is not that complicated but it can be very useful?",
        "Y": "full interpreter",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What class does the Interpreter class include?",
        "Y": "Transformer",
        "Z": "As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the class that can be used to create a new GraphModule?",
        "Y": "Shape Propagation Performance Profiler",
        "Z": "As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we do when our code is not quite right?",
        "Y": "debugging",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers. Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What did we try to check with the == equality operator?",
        "Y": "equality of the values of two deep learning models",
        "Z": "Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What should comparison of floating point values use to account for the non-commutativity of floating point operations?",
        "Y": "margin of error",
        "Z": "Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What gives us an approximate comparison taking into account a relative and absolute tolerance threshold?",
        "Y": "torch.allclose()",
        "Z": "If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers. Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the first tool in our toolbox to check if transformed modules are behaving as we expect compared to a reference implementation?",
        "Y": "torch.allclose()",
        "Z": "Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Why is using traditional debugging techniques like print statements or pdb not as straightfoward?",
        "Y": "FX generates the forward() function on GraphModules",
        "Z": "Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is used to step into the GraphModules when the forward pass is invoked?",
        "Y": "pdb",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you want to run the same code multiple times, what can be a bit tedious to step to the right code with?",
        "Y": "pdb",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Why is using traditional debugging techniques like print statements or pdb not as straightforward?",
        "Y": "FX generates the forward() function on GraphModules",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is used to step into the GraphModule when the forward pass is invoked?",
        "Y": "pdb",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you want to run the same code multiple times, it can be a bit tedious to step to the right code with what?",
        "Y": "pdb",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an easier way to examine modules and parameters using GraphModule.to_folder()?",
        "Y": "to_folder",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a way to examine modules and parameters in GraphModule?",
        "Y": "to_folder",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can we modify the code within foo/module.py?",
        "Y": "adding print statements or using pdb",
        "Z": "Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is another way to examine modules and parameters in GraphModule?",
        "Y": "to_folder",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can we look at the code within GraphModule?",
        "Y": "foo/module.py",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What section in the documentation will we check first?",
        "Y": "Limitations of Symbolic Tracing",
        "Z": "Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the goal after we verify that tracing is working as expected?",
        "Y": "figuring out what went wrong during our GraphModule transformation",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can we find a quick answer to the question of what went wrong in GraphModule?",
        "Y": "Writing Transformations",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we compare our traced Module before and after we've applied our transformations?",
        "Y": "utility functions",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A simple visual comparison is enough to trace down what?",
        "Y": "a bug",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.) The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be a good next step if it's still not clear what's going wrong?",
        "Y": "pdb",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What debugger is used to find what goes wrong?",
        "Y": "pdb",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is broken on a pdb session?",
        "Y": "transform_graph(traced)",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we want to see in the print_tabular method?",
        "Y": "input_nodes and users",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What method can we edit to print different attributes of the Nodes in the Graph?",
        "Y": "print_tabular",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of a program that uses symbolic tracing?",
        "Y": "let\u2019s examine the following program",
        "Z": "FX uses a system of symbolic tracing (a.k.a symbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is tracing in that it executes the program (really a\ntorch.nn.Module or function) to record operations. It is\nsymbolic in that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What works for most neural net code, but has some limitations?",
        "Y": "symbolic tracing",
        "Z": "Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What happens when you pass a new input tensor to the traced function?",
        "Y": "x can change",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What method walks back through your code to show you where this situation happens?",
        "Y": "The traceback",
        "Z": "Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the term for loops or if statements whose value cannot change across invocations?",
        "Y": "static control flow",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are examples of dynamic control flow?",
        "Y": "loops or if statements",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do PyTorch programs use to make decisions about a model's architecture?",
        "Y": "hyper-parameters",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the function that walks back through your code to show you where this situation happens?",
        "Y": "The traceback",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of control flow is supported by PyTorch?",
        "Y": "static control flow",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the if-statement if?",
        "Y": "self.do_activation",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is supported by PyTorch?",
        "Y": "static control flow",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a valid pattern that is supported by?",
        "Y": "symbolic tracing",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can static control flow be made to support symbolic tracing?",
        "Y": "by removing the data dependencies on input values",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can semantically static control flow be made to support symbolic tracing?",
        "Y": "by removing the data dependencies on input values",
        "Z": "The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the method that can be traced as calls to?",
        "Y": "Customizing Tracing with the Tracer class",
        "Z": "Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of a function that is not covered by __torch_function__?",
        "Y": "For example:",
        "Z": "Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What functions are not covered by __torch_function__?",
        "Y": "builtin Python functions or those in the math module",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Nondeterministic constructors (rand, randn) will have a single what type of value embedded in the trace?",
        "Y": "random",
        "Z": "Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the result of nondeterministic constructors having a single random value embedded in the trace?",
        "Y": "likely not the intended behavior",
        "Z": "Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the torch.fx.wrap function contain?",
        "Y": "Type annotations",
        "Z": "Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do arguments to deterministic constructors refer to?",
        "Y": "dynamic input sizes",
        "Z": "Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is used to preserve Python 3-style type annotations?",
        "Y": "symbolic tracing",
        "Z": "The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of function can typically not trace through concrete_args due to the presence of control flow?",
        "Y": "FX",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What values will be ignored by concrete_args?",
        "Y": "different values of b",
        "Z": "Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we use concrete_args to do?",
        "Y": "to specialize on the value of b to trace through this",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a Module or function to be traced and converted into?",
        "Y": "Graph representation",
        "Z": "Symbolic tracing API Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of function can typically not trace through this because of the presence of control flow?",
        "Y": "FX",
        "Z": "Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a module or function to be traced and converted into?",
        "Y": "Graph representation",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What representation is a module or function to be traced and converted into?",
        "Y": "Graph",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What enable C-level patching of functions?",
        "Y": "enable_cpatching",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be passed in, but they will be ignored?",
        "Y": "different values of b",
        "Z": "For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can concrete_args do to our function?",
        "Y": "eliminate data-structure handling",
        "Z": "FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a Module created from the recorded operations from root?",
        "Y": "GraphModule",
        "Z": "FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can a GraphModule function be used as?",
        "Y": "a decorator",
        "Z": "Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d?",
        "Y": "GraphModule",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an nn.Module generated from?",
        "Y": "fx.Graph",
        "Z": "GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When a graph is reassigned, what will be automatically regenerated?",
        "Y": "code and forward",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What should you construct if you edit the contents of the graph without reassigning the graph attribute itself?",
        "Y": "GraphModule",
        "Z": "GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the function or name of the global function to insert into the graph when it\u2019s called GraphModule?",
        "Y": "fn_or_name",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the function that can be used as a decorator?",
        "Y": "GraphModule",
        "Z": "This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a function that can be used as a decorator?",
        "Y": "GraphModule",
        "Z": "This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the module that is created when a graph is reassigned?",
        "Y": "GraphModule",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will be automatically regenerated when graph is reassigned?",
        "Y": "code and forward",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What should you construct to update the generated code?",
        "Y": "GraphModule",
        "Z": "Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be an nn.Module instance or a Dict mapping strings to any attribute type?",
        "Y": "root",
        "Z": "A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If root is a GraphModule, what attribute type will references to Module-based objects be copied over from the respective place within the",
        "Y": "Module",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the root of a GraphModule?",
        "Y": "dict",
        "Z": "Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will be copied over into the appropriate place within the GraphModule\u2019s module hierarchy?",
        "Y": "The object mapped to by the Dict",
        "Z": "Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the nodes this GraphModule should use for code generation?",
        "Y": "graph",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When graph is reassigned, code and forward will be what?",
        "Y": "automatically regenerated",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a GraphModule?",
        "Y": "GraphModule",
        "Z": "concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Root can either be an nn.Module instance or a Dict mapping strings to what?",
        "Y": "any attribute type",
        "Z": "Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If root is a GraphModule, what type of object will references to Module-based objects be copied over from the respective place within root",
        "Y": "Module",
        "Z": "Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What contains the nodes this GraphModule should use for code generation?",
        "Y": "graph",
        "Z": "Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Root can be either an nn.Module instance or a Dict mapping strings to what?",
        "Y": "any attribute type",
        "Z": "root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will be copied over from the respective place within root's Module hierarchy into the GraphModule's module hierarchy?",
        "Y": "Module-based objects",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If root is a Module, the qualified name found in a Node's target will be looked up directly in what?",
        "Y": "dict",
        "Z": "root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will be copied over into the appropriate place within the GraphModule's module hierarchy?",
        "Y": "The object mapped to by the Dict",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What name denotes the name of this GraphModule for debugging purposes?",
        "Y": "class_name",
        "Z": "root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do error messages report as originating from GraphModule?",
        "Y": "root\u2019s original name or a name that makes sense within the context of your transform",
        "Z": "graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If it's unset, all error messages will report as originating from what?",
        "Y": "GraphModule",
        "Z": "graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Installs what if they are subpaths of target?",
        "Y": "empty Modules",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A Module is considered \u201cused\u201d if any of the following is true: 1. It has what that are used?",
        "Y": "children",
        "Z": "m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does delete_submodule do to the given submodule from self?",
        "Y": "Deletes",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does delete_submodule do to an nn.Module?",
        "Y": "Deletes all unused submodules",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does a return value of False mean?",
        "Y": "the target was not a valid reference to a submodule",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the return value of False that means that the target was not a valid reference to a submodule?",
        "Y": "bool",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does bool Return the Python code generated from the Graph underlying this GraphModule?",
        "Y": "Deletes all unused submodules",
        "Z": "bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A Module is considered used if any of the following is true: 1. It has what that are used 2. Its forward is called directly via a",
        "Y": "children",
        "Z": "bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does return the Python code generated from the underlying this GraphModule?",
        "Y": "bool Return the Graph",
        "Z": "bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the Python code generated from the GraphModule do?",
        "Y": "Deletes all unused submodules from self",
        "Z": "Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What happens to a module when it is deleted?",
        "Y": "if target is not a valid target",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute?",
        "Y": "bool",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the submodule we want to delete?",
        "Y": "delete",
        "Z": "A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "This method can be called to clean up an nn.Module without manually calling what on each unused submodule?",
        "Y": "delete_submodule",
        "Z": "This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does delete_submodule dump out to folder with?",
        "Y": "module_name",
        "Z": "This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the module dump out to folder with?",
        "Y": "module_name",
        "Z": "Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Dumps out module to folder with what name so that it can be imported with from folder> import module_name> folder",
        "Y": "module_name",
        "Z": "The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we want to do with the submodule we want to delete?",
        "Y": "delete",
        "Z": "The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will the generated code of this GraphModule be if it is not recompiled?",
        "Y": "out of date",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Dumps out module to folder with what name so that it can be imported from folder> import module_name> folder?",
        "Y": "module_name",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Graph is the main data structure used in the FX Intermediate Representation. It consists of what?",
        "Y": "a series of Node s",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The list of Node s, taken together, constitute what?",
        "Y": "a valid Python function",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will the generated code of this GraphModule be if it is not recompiled after editing the contained graph?",
        "Y": "out of date",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Graph is the main data structure used in the FX Intermediate Representation. It consists of a series of what?",
        "Y": "Node s",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The list of Node s, taken together, constitute a valid what?",
        "Y": "Python function",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "For the semantics of operations represented in the Graph, please see what?",
        "Y": "Node",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of a tensor split?",
        "Y": "Example",
        "Z": "Splits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension dim is not divisible by\nsplit_size. If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. tensor (Tensor) \u2013 tensor to split. split_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or\nlist of sizes for each chunk dim (int) \u2013 dimension along which to split the tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "What language does setuptools.Extension work for?",
        "Y": "C++",
        "Z": "Creates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Example Creates a setuptools.Extension for what language?",
        "Y": "CUDA/C++",
        "Z": "Creates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is an example of a setuptools.Extension for CUDA/C++?",
        "Y": "Compute capabilities",
        "Z": "Creates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What extension does setuptools.Extension build?",
        "Y": "CUDA/C++",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus",
        "Y": "custom",
        "Z": "Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "When will the extension need to be recompiled?",
        "Y": "a new card is installed",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is a compute capability that's newer than the newest version for which your nvcc can build fully-compiled binaries",
        "Y": "CC",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does Pytorch make nvcc fall back to building kernels with?",
        "Y": "PTX",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a",
        "Y": "Creates a setuptools.Extension for CUDA/C++",
        "Z": "Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "When does the extension need to be recompiled?",
        "Y": "a new card is installed",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Pytorch will make nvcc fall back to building kernels with the newest version of what?",
        "Y": "PTX",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is used to override the default behavior?",
        "Y": "TORCH_CUDA_ARCH_LIST",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "The extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus what?",
        "Y": "PTX",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the extension compiled to run on all archs of the cards visible during the building process of the extension?",
        "Y": "Compute capabilities",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If what is installed the extension may need to be recompiled?",
        "Y": "a new card",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is used to override the default behavior of Pytorch?",
        "Y": "TORCH_CUDA_ARCH_LIST",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What option causes extension kernel binaries to include PTX instructions for the specified CC?",
        "Y": "+PTX",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension .",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC?",
        "Y": "PTX",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension .",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the +PTX option improve your binary's?",
        "Y": "forward compatibility",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension .",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What can reduce performance on newer CCs?",
        "Y": "relying on older PTX to provide forward compat by runtime-compiling for newer CCs",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension .",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off doing what?",
        "Y": "specifying them individually",
        "Z": "Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What GPUs would you want your extension to run on?",
        "Y": "8.0 and 8.6",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What can relying on older PTX to provide forward compat by runtime-compiling for newer CCs modestly reduce",
        "Y": "performance",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If you know exact CC(s) of the GPUs you want to target, you're always better off doing what?",
        "Y": "specifying them individually",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension .",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What build extension?",
        "Y": "custom setuptools",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension .",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What compiler does setuptools.build_ext support?",
        "Y": "CUDA",
        "Z": "A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the backend that builds using the Ninja backend?",
        "Y": "use_ninja",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the standard distutils backend do if Ninja is not available?",
        "Y": "Note",
        "Z": "A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does the setuptools.build_ext subclass support in general?",
        "Y": "CUDA files",
        "Z": "This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "BuildExtension is allowed to supply a dictionary for what?",
        "Y": "extra_compile_args",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is Ninja's advantage over setuptools.build_ext?",
        "Y": "greatly speeds up compilation",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How much resources does #CPUS + 2 workers use on some systems?",
        "Y": "too many resources",
        "Z": "This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does Ninja greatly speed up compared to the standard setuptools.build_ext?",
        "Y": "compilation",
        "Z": "use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does using #CPUS + 2 workers on some systems use up?",
        "Y": "too many resources",
        "Z": "use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How many resources does the Ninja backend use to build the extension?",
        "Y": "One can",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does nvcc support with mixed compilation?",
        "Y": "CUDA",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What can be passed along with other sources?",
        "Y": "CUDA source files",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What compiler will CUDA source files be detected and compiled with instead of the C++ compiler?",
        "Y": "nvcc",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is passed as a library directory?",
        "Y": "CUDA lib64 directory",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How can you pass additional flags to nvcc?",
        "Y": "extra_cuda_cflags",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is used to find the CUDA install directory?",
        "Y": "Various heuristics",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the safest option for finding the CUDA install directory?",
        "Y": "setting the CUDA_HOME environment variable",
        "Z": "To compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting the CXX environment variable. To pass\nadditional arguments to the compilation process, extra_cflags or\nextra_ldflags can be provided. For example, to compile your extension\nwith optimizations, pass extra_cflags=['-O3']. You can also use\nextra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the optional list of linker flags to forward to the build?",
        "Y": "extra_ldflags",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included. is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is an optional list of include directories to forward to the build?",
        "Y": "extra_include_paths",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included. is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and",
        "Y": "verbose",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included. is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What determines whether CUDA headers and libraries are added to the build?",
        "Y": "with_cuda",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included. is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "When does with_cuda determine whether CUDA headers and libraries are added to the build?",
        "Y": "If set to None",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included. is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Set it to what value to force CUDA headers and libraries to be included?",
        "Y": "True",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included. is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What imports the produced shared library as a Python module?",
        "Y": "is_python_module",
        "Z": "is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module. Returns nothing. (The shared library is loaded into the process as\na side effect.) Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.) If is_python_module is True Example Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior of load_inline() is\nidentical to load(). See the\ntests\nfor good examples of using this function.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If False, load the constructed extension into the process as a plain dynamic library. If True, build a standalone executable.",
        "Y": "is_standalone",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included. is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does is_standalone do if False?",
        "Y": "build a standalone executable",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included. is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does is_standalone return as a Python module?",
        "Y": "PyTorch extension",
        "Z": "extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward\nto the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set to None (default), this value is\nautomatically determined based on the existence of .cu or\n.cuh in sources. Set it to True` to force CUDA headers\nand libraries to be included. is_python_module \u2013 If True (default), imports the produced shared\nlibrary as a Python module. If False, behavior depends on\nis_standalone. is_standalone \u2013 If False (default) loads the constructed extension\ninto the process as a plain dynamic library. If True, build a\nstandalone executable.  Returns the loaded PyTorch extension as a Python module.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "The same shape as input, except along what axis?",
        "Y": "axis",
        "Z": "Repeat elements of a tensor. Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat. input (Tensor) \u2013 the input tensor. repeats (Tensor or int) \u2013 The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. dim (int, optional) \u2013 The dimension along which to repeat values.\nBy default, use the flattened input array, and return a flat output\narray. Repeated tensor which has the same shape as input, except along the given axis. Tensor Example: If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be\ntensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times,\n1 appears n2 times, 2 appears n3 times, etc.",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "What is added to each element of the input input and returns a new resulting tensor?",
        "Y": "scalar",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What type of input must other be a real number?",
        "Y": "FloatTensor or DoubleTensor",
        "Z": "Adds the scalar other to each element of the input input\nand returns a new resulting tensor. If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What is returned when each element of the tensor other is multiplied by the scalar alpha and added to each element of",
        "Y": "resulting tensor",
        "Z": "Adds the scalar other to each element of the input input\nand returns a new resulting tensor. If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "What is the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor?",
        "Y": "Example",
        "Z": "Adds the scalar other to each element of the input input\nand returns a new resulting tensor. If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be added to each element of input out (Tensor, optional) \u2013 the output tensor. Example: Each element of the tensor other is multiplied by the scalar\nalpha and added to each element of the tensor input.\nThe resulting tensor is returned. The shapes of input and other must be\nbroadcastable. If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor alpha (Number) \u2013 the scalar multiplier for other out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "If one tensor has fewer dimensions than the other, it is unsqueezed until what?",
        "Y": "it has the same number of dimensions",
        "Z": "If input is a (a0\u00d7a1\u00d7\u22ef\u00d7an)(a_0 \\times a_1 \\times \\dots \\times a_n)(a0\u200b\u00d7a1\u200b\u00d7\u22ef\u00d7an\u200b) tensor and other is a\n(b0\u00d7b1\u00d7\u22ef\u00d7bn)(b_0 \\times b_1 \\times \\dots \\times b_n)(b0\u200b\u00d7b1\u200b\u00d7\u22ef\u00d7bn\u200b) tensor, the result will be a\n(a0\u2217b0\u00d7a1\u2217b1\u00d7\u22ef\u00d7an\u2217bn)(a_0*b_0 \\times a_1*b_1 \\times \\dots \\times a_n*b_n)(a0\u200b\u2217b0\u200b\u00d7a1\u200b\u2217b1\u200b\u00d7\u22ef\u00d7an\u200b\u2217bn\u200b) tensor with the following entries: where kt=it\u2217bt+jtk_t = i_t * b_t + j_tkt\u200b=it\u200b\u2217bt\u200b+jt\u200b for 0\u2264t\u2264n0 \\leq t \\leq n0\u2264t\u2264n.\nIf one tensor has fewer dimensions than the other it is unsqueezed until it has the same number of dimensions. Supports real-valued and complex-valued inputs. Note This function generalizes the typical definition of the Kronecker product for two matrices to two tensors,\nas described above. When input is a (m\u00d7n)(m \\times n)(m\u00d7n) matrix and other is a\n(p\u00d7q)(p \\times q)(p\u00d7q) matrix, the result will be a (p\u2217m\u00d7q\u2217n)(p*m \\times q*n)(p\u2217m\u00d7q\u2217n) block matrix: where input is A\\mathbf{A}A and other is B\\mathbf{B}B. input (Tensor) \u2013  other (Tensor) \u2013",
        "source": "https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron"
    },
    {
        "X": "What types of inputs does the Kronecker function support?",
        "Y": "real-valued and complex-valued inputs",
        "Z": "If input is a (a0\u00d7a1\u00d7\u22ef\u00d7an)(a_0 \\times a_1 \\times \\dots \\times a_n)(a0\u200b\u00d7a1\u200b\u00d7\u22ef\u00d7an\u200b) tensor and other is a\n(b0\u00d7b1\u00d7\u22ef\u00d7bn)(b_0 \\times b_1 \\times \\dots \\times b_n)(b0\u200b\u00d7b1\u200b\u00d7\u22ef\u00d7bn\u200b) tensor, the result will be a\n(a0\u2217b0\u00d7a1\u2217b1\u00d7\u22ef\u00d7an\u2217bn)(a_0*b_0 \\times a_1*b_1 \\times \\dots \\times a_n*b_n)(a0\u200b\u2217b0\u200b\u00d7a1\u200b\u2217b1\u200b\u00d7\u22ef\u00d7an\u200b\u2217bn\u200b) tensor with the following entries: where kt=it\u2217bt+jtk_t = i_t * b_t + j_tkt\u200b=it\u200b\u2217bt\u200b+jt\u200b for 0\u2264t\u2264n0 \\leq t \\leq n0\u2264t\u2264n.\nIf one tensor has fewer dimensions than the other it is unsqueezed until it has the same number of dimensions. Supports real-valued and complex-valued inputs. Note This function generalizes the typical definition of the Kronecker product for two matrices to two tensors,\nas described above. When input is a (m\u00d7n)(m \\times n)(m\u00d7n) matrix and other is a\n(p\u00d7q)(p \\times q)(p\u00d7q) matrix, the result will be a (p\u2217m\u00d7q\u2217n)(p*m \\times q*n)(p\u2217m\u00d7q\u2217n) block matrix: where input is A\\mathbf{A}A and other is B\\mathbf{B}B. input (Tensor) \u2013  other (Tensor) \u2013",
        "source": "https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron"
    },
    {
        "X": "This function generalizes the typical definition of what for two matrices to two tensors?",
        "Y": "Kronecker product",
        "Z": "If input is a (a0\u00d7a1\u00d7\u22ef\u00d7an)(a_0 \\times a_1 \\times \\dots \\times a_n)(a0\u200b\u00d7a1\u200b\u00d7\u22ef\u00d7an\u200b) tensor and other is a\n(b0\u00d7b1\u00d7\u22ef\u00d7bn)(b_0 \\times b_1 \\times \\dots \\times b_n)(b0\u200b\u00d7b1\u200b\u00d7\u22ef\u00d7bn\u200b) tensor, the result will be a\n(a0\u2217b0\u00d7a1\u2217b1\u00d7\u22ef\u00d7an\u2217bn)(a_0*b_0 \\times a_1*b_1 \\times \\dots \\times a_n*b_n)(a0\u200b\u2217b0\u200b\u00d7a1\u200b\u2217b1\u200b\u00d7\u22ef\u00d7an\u200b\u2217bn\u200b) tensor with the following entries: where kt=it\u2217bt+jtk_t = i_t * b_t + j_tkt\u200b=it\u200b\u2217bt\u200b+jt\u200b for 0\u2264t\u2264n0 \\leq t \\leq n0\u2264t\u2264n.\nIf one tensor has fewer dimensions than the other it is unsqueezed until it has the same number of dimensions. Supports real-valued and complex-valued inputs. Note This function generalizes the typical definition of the Kronecker product for two matrices to two tensors,\nas described above. When input is a (m\u00d7n)(m \\times n)(m\u00d7n) matrix and other is a\n(p\u00d7q)(p \\times q)(p\u00d7q) matrix, the result will be a (p\u2217m\u00d7q\u2217n)(p*m \\times q*n)(p\u2217m\u00d7q\u2217n) block matrix: where input is A\\mathbf{A}A and other is B\\mathbf{B}B. input (Tensor) \u2013  other (Tensor) \u2013",
        "source": "https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron"
    },
    {
        "X": "What type of operations will act deterministically when mode=True?",
        "Y": "normally-nondeterministic",
        "Z": "The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the torch.nn.Conv2d when called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When is torch.nn.ConvTranspose1d called?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is torch.nn.ConvTranspose2d called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When is ConvTranspose3d called?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is index_add() called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What type of operations will throw a RuntimeError when mode=True?",
        "Y": "normally-nondeterministic",
        "Z": "The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is AdaptiveAvgPool3d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When attempting to differentiate a CUDA tensor torch, what is MaxPool3d?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max'",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is AdaptiveMaxPool2d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When attempting to differentiate a CUDA tensor torch, what is FractionalMaxPool2d?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When attempting to differentiate a CUDA tensor torch, what is FractionalMaxPool3d?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What does median() have when called on a CUDA tensor torch?",
        "Y": "indices output",
        "Z": "bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When input dimension is larger than one and called on a CUDA tensor that requires what?",
        "Y": "grad",
        "Z": "torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When attempting to differentiate a CUDA tensor torch, what does ReplicationPad2d do?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When is NLLLoss called on a CUDA tensor torch?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When mode='max' torch, what is EmbeddingBag?",
        "Y": "CUDA tensor",
        "Z": "torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When is torch.Tensor.scatter_add_() called on a CUDA tensor torch?",
        "Y": "CUDA",
        "Z": "torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm()",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What documentation describes nondeterministic CUDA operations?",
        "Y": "CUDA",
        "Z": "torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm()",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is used to split indices_or_sections?",
        "Y": "dimension",
        "Z": "input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What is the default dimension along which to split the tensor?",
        "Y": "0",
        "Z": "input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "How many dices_or_sections are there?",
        "Y": "n",
        "Z": "indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What is dim?",
        "Y": "dimension along which to split the tensor",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What is the default value of indices_or_sections?",
        "Y": "0",
        "Z": "indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "Default: if what, defaults to the dtype of input. device (torch.device, optional) \u2013 the",
        "Y": "None",
        "Z": "Returns a tensor with the same size as input that is filled with\nrandom numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).\ntorch.rand_like(input) is equivalent to\ntorch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like"
    },
    {
        "X": "If None, what does torch.rand_like(input) do?",
        "Y": "defaults",
        "Z": "Returns a tensor with the same size as input that is filled with\nrandom numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).\ntorch.rand_like(input) is equivalent to\ntorch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like"
    },
    {
        "X": "If unbiased is True, Bessel's correction will be used. Otherwise, the sample deviation is calculated, without any correction.",
        "Y": "If unbiased is True",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "What is calculated, without any correction?",
        "Y": "the sample deviation",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "Whether to use Bessel\u2019s correction (N=1delta N = 1N=1)?",
        "Y": "unbiased",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "If unbiased is True, Bessel\u2019s correction will be used. Otherwise, what is calculated, without any correction?",
        "Y": "the sample deviation",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor. Calculates the standard deviation of all elements in the input tensor. If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "What beta is Quantization currently in?",
        "Y": "beta",
        "Z": "Warning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators. PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantization refers to techniques for performing computations and storing tensors at what?",
        "Y": "lower bitwidths than floating point precision",
        "Z": "Warning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators. PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "A quantized model executes some or all of the operations on tensors with what instead of floating point values?",
        "Y": "integers",
        "Z": "Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "A quantized model executes some or all of the operations on tensors with integers rather than floating point values. This allows for what",
        "Y": "more compact model representation",
        "Z": "Warning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators. PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What quantization does PyTorch support?",
        "Y": "INT8",
        "Z": "Warning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators. PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How much faster is hardware support for INT8 computations compared to FP32?",
        "Y": "2 to 4 times faster",
        "Z": "Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantization is primarily a technique to what?",
        "Y": "speed up inference",
        "Z": "Warning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators. PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "When is quantization in beta?",
        "Y": "in beta",
        "Z": "Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What platform supports multiple approaches to quantizing a deep learning model?",
        "Y": "PyTorch",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In most cases, the model is trained in what?",
        "Y": "FP32",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does PyTorch support that models quantization errors in both the forward and backward passes using fake-quantization modules?",
        "Y": "quantization aware training",
        "Z": "Warning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators. PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where is the entire computation carried out in PyTorch?",
        "Y": "floating point",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack')",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does PyTorch convert the trained model into at the end of quantization aware training?",
        "Y": "lower precision",
        "Z": "Warning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators. PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "At lower level, PyTorch provides a way to represent quantized what?",
        "Y": "tensors",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack')",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What can PyTorch perform operations with quantized tensors?",
        "Y": "directly construct models",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where should a model be moved to test quantized functionality?",
        "Y": "CPU",
        "Z": "At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What must be ensured when preparing a quantized model?",
        "Y": "qconfig and the engine used for quantized computations match the backend on which the model will be executed",
        "Z": "x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What backend does Quantization currently support?",
        "Y": "fbgemm",
        "Z": "When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How do you set the qconfig for a model to run on ARM?",
        "Y": "by calling:",
        "Z": "x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where should the model be moved in order to test the quantized functionality?",
        "Y": "CPU",
        "Z": "The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the qconfig for a model to run on ARM?",
        "Y": "torch.quantization.get_default_qconfig",
        "Z": "ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the qconfig for post training quantization?",
        "Y": "torch.quantization.get_default_qconfig",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantization-aware training supports both what?",
        "Y": "CPU and CUDA",
        "Z": "Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the backend set to qnnpack?",
        "Y": "torch.backends.quantized.engine = 'qnnpack'",
        "Z": "Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "fbgemm is for use on what platform?",
        "Y": "x86",
        "Z": "When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the backend used to quantize a model to run on ARM?",
        "Y": "qnnpack",
        "Z": "Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the backend used for quantization on ARM?",
        "Y": "qnnpack",
        "Z": "When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the qconfig for quantization aware training?",
        "Y": "torch.quantization.get_default_qat_qconfig('qnnpack')",
        "Z": "for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "FX Graph Mode Quantization is a new automated quantization framework in what?",
        "Y": "PyTorch",
        "Z": "FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does FX Graph Mode Quantization improve upon?",
        "Y": "Eager Mode Quantization",
        "Z": "In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What domain library will FX Graph Mode Quantization integrate into?",
        "Y": "torchvision",
        "Z": "FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What do users need to know to make FX Graph Mode Quantization work?",
        "Y": "torch.fx",
        "Z": "FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is another option if FX Graph Mode Quantization does not work?",
        "Y": "eager mode quantization",
        "Z": "New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is Eager Mode Quantization compared to?",
        "Y": "FX Graph Mode Quantization",
        "Z": "New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is Supported Supported Quantizing Functionals/Torch Ops Manual Automatic Support for Customization Limited Support?",
        "Y": "Operator Fusion Manual Automatic Quant/DeQuant Placement Manual Automatic Quantizing Modules",
        "Z": "Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of Quantization is supported in Post Training Quantization?",
        "Y": "Dynamic",
        "Z": "Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does Manual Automatic Support for Customization Limited Support support?",
        "Y": "Manual Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support Post Training Quantization",
        "Z": "Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support Post Training Quantization?",
        "Y": "Automatic Support for Customization Limited Support Fully Supported Quantization Mode Support Post Training Quantization",
        "Z": "Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Static, Dynamic, Weight Only Quantiztion Aware Training: Static Post Training Quantization: Static, Dynamic, Weight Only",
        "Y": "Dynamic",
        "Z": "Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the post training quantization mode supported by Limited Support Fully Supported Quantization Mode Support?",
        "Y": "Dynamic",
        "Z": "Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantization Mode Support Post Training Quantization: Static, Dynamic, Weight Only Quantiztion Aware Training: Static Post Training Quantization",
        "Y": "Dynamic",
        "Z": "Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of quantization does Eager Mode Quantization support?",
        "Y": "static quantization",
        "Z": "Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does torch.nn.Module need some refactors to make compatible with?",
        "Y": "FX Graph Mode Quantization",
        "Z": "torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an example of quantization in Eager Mode Quantization?",
        "Y": "API",
        "Z": "There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the term for weights quantized with activations read/stored in floating point and quantized for compute?",
        "Y": "dynamic quantization",
        "Z": "dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is another name for quantization aware training?",
        "Y": "static quantization",
        "Z": "dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of models with small batch size are LSTM and dynamic quantization used for?",
        "Y": "Transformer type models",
        "Z": "Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What blog post provides a more comprehensive overview of the tradeoffs between static and dynamic quantization?",
        "Y": "Pytorch",
        "Z": "static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "When are activations dynamically quantized?",
        "Y": "inference",
        "Z": "quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is used in situations where the model execution time is dominated by?",
        "Y": "loading weights from memory",
        "Z": "dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the model execution time dominated by?",
        "Y": "loading weights from memory",
        "Z": "Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the technique that quantizes the weights and activations of a model?",
        "Y": "static quantization",
        "Z": "To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What quantizes the weights and activations of the model?",
        "Y": "Static quantization",
        "Z": "Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does static quantization fuse activations into where possible?",
        "Y": "preceding layers",
        "Z": "Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does static quantization require to determine optimal quantization parameters for activations?",
        "Y": "calibration with a representative dataset",
        "Z": "Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is used when memory bandwidth and compute savings are important?",
        "Y": "Post Training Quantization",
        "Z": "Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is another name for Post Training Quantization?",
        "Y": "PTQ",
        "Z": "Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is PTQ also known as?",
        "Y": "static quantization",
        "Z": "dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the term for quantization aware training?",
        "Y": "static quantization",
        "Z": "Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial. Quantization Aware Training models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.  Quantization Aware Training is also known as\nQAT. Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What model allows for higher accuracy compared to other quantization methods?",
        "Y": "Quantization Aware Training",
        "Z": "Quantization Aware Training models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.  Quantization Aware Training is also known as\nQAT. Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "During training, all calculations are done in what?",
        "Y": "floating point",
        "Z": "Quantization Aware Training models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.  Quantization Aware Training is also known as\nQAT. Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are quantized after model conversion?",
        "Y": "weights and activations",
        "Z": "Quantization Aware Training models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.  Quantization Aware Training is also known as\nQAT. Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is Quantization Aware Training commonly used with?",
        "Y": "CNNs",
        "Z": "Quantization Aware Training models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.  Quantization Aware Training is also known as\nQAT. Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is also known as QAT?",
        "Y": "Quantization Aware Training",
        "Z": "Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial. Quantization Aware Training models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.  Quantization Aware Training is also known as\nQAT. Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the quantization aware training tutorial?",
        "Y": "QAT",
        "Z": "Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial. Quantization Aware Training models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.  Quantization Aware Training is also known as\nQAT. Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is Quantization Aware Training also known as?",
        "Y": "QAT",
        "Z": "Quantization Aware Training models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.  Quantization Aware Training is also known as\nQAT. Diagram: API Example: To learn more about quantization aware training, please see the QAT\ntutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How many ways can quantization types supported by FX Graph Mode be classified?",
        "Y": "two",
        "Z": "Quantization types supported by FX Graph Mode can be classified in two ways: Post Training Quantization (apply quantization after training, quantization parameters are calculated based on sample calibration data) Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What simulates quantization during training?",
        "Y": "Quantization Aware Training",
        "Z": "Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "PyTorch supports both per tensor and per channel what?",
        "Y": "asymmetric linear quantization",
        "Z": "PyTorch supports both per tensor and per channel asymmetric linear\nquantization. Per tensor means that all the values within the tensor are\nscaled the same way. Per channel means that for each dimension, typically\nthe channel dimension of a tensor, the values\nin the tensor are scaled and offset by a different value (effectively\nthe scale and offset become vectors). This allows for lesser error in converting tensors\nto quantized values. The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What means that for each dimension, typically the channel dimension of a tensor, the values in the tensor are scaled and",
        "Y": "Per channel",
        "Z": "PyTorch supports both per tensor and per channel asymmetric linear\nquantization. Per tensor means that all the values within the tensor are\nscaled the same way. Per channel means that for each dimension, typically\nthe channel dimension of a tensor, the values\nin the tensor are scaled and offset by a different value (effectively\nthe scale and offset become vectors). This allows for lesser error in converting tensors\nto quantized values. The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does this allow for in converting tensors to quantized values?",
        "Y": "lesser error",
        "Z": "PyTorch supports both per tensor and per channel asymmetric linear\nquantization. Per tensor means that all the values within the tensor are\nscaled the same way. Per channel means that for each dimension, typically\nthe channel dimension of a tensor, the values\nin the tensor are scaled and offset by a different value (effectively\nthe scale and offset become vectors). This allows for lesser error in converting tensors\nto quantized values. The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the mapping performed by converting?",
        "Y": "floating point tensors",
        "Z": "PyTorch supports both per tensor and per channel asymmetric linear\nquantization. Per tensor means that all the values within the tensor are\nscaled the same way. Per channel means that for each dimension, typically\nthe channel dimension of a tensor, the values\nin the tensor are scaled and offset by a different value (effectively\nthe scale and offset become vectors). This allows for lesser error in converting tensors\nto quantized values. The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In what programming language do we need to be able to represent quantized data in Tensors?",
        "Y": "PyTorch",
        "Z": "The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantized Tensors allow for what in a quantized format?",
        "Y": "serialization of data",
        "Z": "The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Many operations for quantized tensors are available under the same API as full float version in what?",
        "Y": "torch",
        "Z": "8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the fused version of NN modules that impact quantization?",
        "Y": "torch.nn.intrinsic.quantized",
        "Z": "Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where are many operations for quantized tensors available under the same API as full float version?",
        "Y": "torch",
        "Z": "Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does the user need to do before quantization?",
        "Y": "Specify which parts of the model need to be quantized",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does setting None mean that the model.conv layer will not be quantized?",
        "Y": "model.conv",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What do static quantization techniques do?",
        "Y": "quantize activations",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is used to convert operations that require output requantization to module form?",
        "Y": "torch.nn.ReLU",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does the user need to do to quantize activations?",
        "Y": "Specify where activations are quantized and de-quantized",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does None mean that the model.conv layer will not be quantized?",
        "Y": "model.conv",
        "Z": "Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What API does the user use to combine operations/modules into a single module?",
        "Y": "torch.quantization.fuse_modules()",
        "Z": "For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a common workaround to quantize the tensor?",
        "Y": "torch.quantization.QuantStub",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is an example of an error similar to: If you see an error similar to: This means that you are trying to pass a quantized Ten",
        "Y": "e2e",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In what mode is torch.quantization.DeQuantStub used?",
        "Y": "Eager mode quantization",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of example does torch.quantization.DeQuantStub provide?",
        "Y": "e2e",
        "Z": "If you see an error similar to: This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: If you see an error similar to: This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are some helper functions for quantizing the input to your model and performing critical fusions?",
        "Y": "conv+relu",
        "Z": "torch.quantization This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What combined modules are implemented by torch.nn.intrinsic?",
        "Y": "conv + relu",
        "Z": "torch.quantization This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In what platform are the key nn modules Conv2d() and Linear() implemented?",
        "Y": "FP32",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What format does this module convert your model from?",
        "Y": "FP32",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are the combined (fused) modules that can then be quantized?",
        "Y": "conv + relu",
        "Z": "This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What module implements versions of those fused operations needed for quantization aware training?",
        "Y": "torch.nn.intrinsic.qat",
        "Z": "torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are the combined (fused) modules implemented by torch.nn.intrinsic?",
        "Y": "conv + relu",
        "Z": "torch.quantization This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What quantization effect does the rounding simulate?",
        "Y": "INT8",
        "Z": "torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What should p be?",
        "Y": "scalar or tensor",
        "Z": "Fills each location of self with an independent sample from\nBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). self can have integral\ndtype. p should either be a scalar or tensor containing probabilities to be\nused for drawing the binary random number. If it is a tensor, the ith\\text{i}^{th}ith element of self tensor\nwill be set to a value sampled from\nBernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]})Bernoulli(p_tensor[i]). In this case p must have\nfloating point dtype. See also bernoulli() and torch.bernoulli()",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"
    },
    {
        "X": "What can be logged for one experiment with TensorBoard?",
        "Y": "Lots of information",
        "Z": "Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs. The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the name of the result that can be logged in the TensorBoard interface?",
        "Y": "Expected result",
        "Z": "Before going further, more details on TensorBoard can be found at\nhttps://www.tensorflow.org/tensorboard/ Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs. The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "To avoid cluttering the UI and have better result clustering, what can we do by naming them hierarchically?",
        "Y": "group plots",
        "Z": "This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What plot will be grouped together in the TensorBoard interface?",
        "Y": "Loss/train",
        "Z": "Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs. The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What plot will be grouped together?",
        "Y": "Loss/train",
        "Z": "The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What will be grouped separately in the TensorBoard interface?",
        "Y": "\u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d",
        "Z": "This can then be visualized with TensorBoard, which should be installable\nand runnable with: Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does the SummaryWriter class allow a training program to do?",
        "Y": "call methods",
        "Z": "Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What class will write out events and summaries to the event file?",
        "Y": "SummaryWriter",
        "Z": "Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. Expected result: Writes entries directly to event files in the log_dir to be\nconsumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining. Creates a SummaryWriter that will write out events and summaries\nto the event file.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does this argument have no effect?",
        "Y": "If log_dir is assigned",
        "Z": "comment (string) \u2013 Comment log_dir suffix appended to the default\nlog_dir. If log_dir is assigned, this argument has no effect. purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "When logging crashes at step T+XT+XT+X and restarts at step what?",
        "Y": "TTT",
        "Z": "comment (string) \u2013 Comment log_dir suffix appended to the default\nlog_dir. If log_dir is assigned, this argument has no effect. purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Data identifier scalar_value (float or string/blobname) \u2013 Value to save what?",
        "Y": "global_step",
        "Z": "comment (string) \u2013 Comment log_dir suffix appended to the default\nlog_dir. If log_dir is assigned, this argument has no effect. purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "When logging crashes at step T+XT+XT+X and restarts at step TTT, events whose global_step larger or",
        "Y": "purge_step",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What should crashed and resumed experiments have?",
        "Y": "same log_dir",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the Data identifier scalar_value?",
        "Y": "float or string/blobname",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Size of the queue for pending events and summaries before one of the \u2018add\u2019 calls forces a flush to disk?",
        "Y": "max_queue",
        "Z": "max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "max_queue (int) \u2013 Size of the queue for pending events and summaries before one of the \u2018add\u2019 calls",
        "Y": "ten items",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "flush_secs (int) \u2013 How often to flush the pending events and summaries to disk?",
        "Y": "every two minutes",
        "Z": "flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Data identifier scalar_value (what are two examples of data identifiers?",
        "Y": "float or string/blobname",
        "Z": "max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "what is added to all event filenames in the log_dir directory?",
        "Y": "filename_suffix",
        "Z": "flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "More details on filename construction in what?",
        "Y": "tensorboard.summary.writer.event_file_writer.EventFileWriter",
        "Z": "flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Suffix added to all event filenames in the log_dir directory?",
        "Y": "filename_suffix",
        "Z": "filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is expected result: Adds many scalar data to summary?",
        "Y": "Add scalar data",
        "Z": "Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Data identifier",
        "Y": "scalar_value",
        "Z": "tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Add what to summary?",
        "Y": "image data",
        "Z": "Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Values to build histogram",
        "Y": "global_step",
        "Z": "Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time",
        "Y": "scalar_value",
        "Z": "scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "new_style (boolean) \u2013 Whether to use new style (tensor field) or old style (simple_value",
        "Y": "epoch of event",
        "Z": "global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "walltime (what) \u2013 Optional override default walltime (time.time()) with seconds after epoch of event new",
        "Y": "float",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013",
        "Y": "global_step",
        "Z": "new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Whether to use new style (tensor field) or old style (simple_value field)?",
        "Y": "new_style",
        "Z": "new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What determines the Global step value to record bins?",
        "Y": "how the bins are made",
        "Z": "Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Optional override default walltime (what) seconds after epoch of event",
        "Y": "time.time()",
        "Z": "new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does add to summary?",
        "Y": "Adds many scalar data",
        "Z": "Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Global step value to record bins (string) \u2013 One of \u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019",
        "Y": "how the bins are made",
        "Z": "Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Add image data to summary.",
        "Y": "pillow package",
        "Z": "Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Optional override default walltime (time.time()) seconds after epoch of event Examples:",
        "Y": "Add image data",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Values to build histogram?",
        "Y": "global_step",
        "Z": "values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What determines global step value to record bins?",
        "Y": "how the bins are made",
        "Z": "global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the Data identifier for torch.Tensor, numpy.array, or string/blobname?",
        "Y": "img_tensor",
        "Z": "tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the name of the image data?",
        "Y": "global_step",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the Data identifier for torch.Tensor, numpy.array, or string/blobname",
        "Y": "img_tensor",
        "Z": "Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is expected result: Add image data to summary?",
        "Y": "Add histogram",
        "Z": "Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What determines global_step value to record bins?",
        "Y": "how the bins are made",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Expected result: what to summary?",
        "Y": "Add image data",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What can you use to convert a batch of tensor into 3xHxW format?",
        "Y": "torchvision.utils.make_grid()",
        "Z": "bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Add video data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "As long as what is passed, e.g. CHW, HWC, HW?",
        "Y": "corresponding dataformats argument",
        "Z": "Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "corresponding dataformats argument is passed, e.g. CHW, what?",
        "Y": "HWC",
        "Z": "bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is an example of a tensor with a corresponding dataformats argument?",
        "Y": "Expected result",
        "Z": "bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result:",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Add batched image data to summary",
        "Y": "pillow package",
        "Z": "Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "tag (string) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional",
        "Y": "img_tensor",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "what to convert a batch of tensor into 3xHxW format?",
        "Y": "torchvision.utils.make_grid()",
        "Z": "Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "as long as what is passed?",
        "Y": "corresponding dataformats argument",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "corresponding dataformats argument is passed, e.g. what?",
        "Y": "CHW, HWC, HW",
        "Z": "Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is expected result of adding image data to summary?",
        "Y": "Add batched image data",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "tag (string) \u2013 Data identifier",
        "Y": "img_tensor",
        "Z": "walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the expected result of adding batched image data to summary?",
        "Y": "Add batched image data to summary",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Add batched image data to summary.",
        "Y": "pillow package",
        "Z": "Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Tag (string) \u2013 Data identifier what (torch.Tensor, numpy.array, or string/",
        "Y": "img_tensor",
        "Z": "Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "what is used to convert a batch of tensor into 3xHxW format?",
        "Y": "torchvision.utils.make_grid()",
        "Z": "Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Add batched image data to summary. Note that this requires what?",
        "Y": "pillow package",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "Image data what (int) \u2013 Global step value to record walltime (float)",
        "Y": "global_step",
        "Z": "Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "what is the expected result of adding batched image data to summary?",
        "Y": "Add image data",
        "Z": "Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "tag (string) \u2013 Data identifier what (torch.Tensor, numpy.array, or string/",
        "Y": "img_tensor",
        "Z": "Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "as long as what is passed, e.g. CHW, HWC, HW?",
        "Y": "corresponding dataformats argument",
        "Z": "Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is expected result of adding batched image data to summary?",
        "Y": "Add batched image data",
        "Z": "Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "global step value to record walltime (float) \u2013 Global step value to record walltime (float)",
        "Y": "global_step",
        "Z": "Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "what does this require the pillow package?",
        "Y": "Add image data",
        "Z": "Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "use what to convert a batch of tensor into 3xHxW format?",
        "Y": "torchvision.utils.make_grid()",
        "Z": "Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "as long as what is passed, e.g. CHW, HWC, HW",
        "Y": "corresponding dataformats argument",
        "Z": "Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "what is expected result of adding batched image data to summary?",
        "Y": "Add batched image data",
        "Z": "Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "global step value to record walltime (float)",
        "Y": "global_step",
        "Z": "Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does if None use when both start and end are real?",
        "Y": "global default dtype",
        "Z": "Creates a one-dimensional tensor of size steps whose values are evenly\nspaced from start to end, inclusive. That is, the value are: Warning Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "For backwards compatibility, not providing a value for steps will what?",
        "Y": "create a tensor with 100 elements",
        "Z": "Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What is the starting value for the set of points end?",
        "Y": "start",
        "Z": "Not providing a value for steps is deprecated. For backwards\ncompatibility, not providing a value for steps will create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value for steps will throw a\nruntime error. start (float) \u2013 the starting value for the set of points end (float) \u2013 the ending value for the set of points steps (int) \u2013 size of the constructed tensor out (Tensor, optional) \u2013 the output tensor. dtype (torch.dpython:type, optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen both start and end are real,\nand corresponding complex dtype when either is complex. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "What type of tensor types does autograd only support?",
        "Y": "floating point Tensor types",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does autograd compute and return?",
        "Y": "the sum of gradients of outputs with respect to the inputs",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is very unlikely to change in this API?",
        "Y": "function signatures",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does this API work with?",
        "Y": "user-provided functions",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What can you use to capture arguments that are not Tensors or Tensors that don't have requires_grad set?",
        "Y": "lambda",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does f(input, constant, flag=flag) contain?",
        "Y": "boolean flag",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Computes and returns what with respect to the inputs?",
        "Y": "the sum of gradients of outputs",
        "Z": "Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What do user-provided functions take as input and return only Tensors?",
        "Y": "only Tensors",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does functional.jacobian function do?",
        "Y": "computes the Jacobian of a given function",
        "Z": "Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What can you use to capture arguments that are not Tensors or Tensors that don\u2019t have requires_grad set?",
        "Y": "lambda",
        "Z": "Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What function computes the Jacobian of a given function?",
        "Y": "functional.hessian",
        "Z": "Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does this section contain?",
        "Y": "higher level API",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does functional.vjp compute?",
        "Y": "the dot product between a vector v and the Jacobian of the given function at the point given by the inputs",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does this API work with that take only Tensors as input and return only Tensors?",
        "Y": "user-provided functions",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the group that disabled gradient calculation?",
        "Y": "Context-manager",
        "Z": "functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Function that computes what of a given function. functional.hessian Function that computes the Hessian of a given scal",
        "Y": "Jacobian",
        "Z": "Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Who disabled gradient calculation?",
        "Y": "Context-manager",
        "Z": "Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the function that disables gradient calculation?",
        "Y": "Context-manager",
        "Z": "Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the function that computes the dot product between a vector v and the Hessian of a given scalar function",
        "Y": "computes the dot product between a vector v and the Hessian of a given scalar function",
        "Z": "Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param.grad is initially None, what is accumulated as follows?",
        "Y": "If param.grad is initially None",
        "Z": "Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "When is param.grad initially None?",
        "Y": "If param\u2019s memory is non-overlapping and dense",
        "Z": "Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If param's memory is non-overlapping and dense, what is.grad created with?",
        "Y": "rowmajor-contiguous strides",
        "Z": "See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the function that computes the dot product between the Hessian of a given scalar function and a",
        "Y": "Locally disabling gradient computation",
        "Z": "Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What makes Autograd very efficient?",
        "Y": "aggressive buffer freeing and reuse",
        "Z": "Supporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them. All Tensor s keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Under what conditions might you never need to use in-place operations?",
        "Y": "heavy memory pressure",
        "Z": "Supporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them. All Tensor s keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What are some of the factory methods used to create tensors with requires_grad=True?",
        "Y": "torch.randn(), torch.zeros(), torch.ones()",
        "Z": "Warning The Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors with\nrequires_grad set to True. Below please find a quick guide on what\nhas changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected,\nbut they return Tensors instead of Variables. var.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors\nwith the same method names. In addition, one can now create tensors with requires_grad=True using factory\nmethods such as torch.randn(), torch.zeros(), torch.ones(), and others\nlike the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does torch.autograd define formulas for?",
        "Y": "differentiating ops",
        "Z": "Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function\nobject, that performs the computation, and records that it happened.\nThe history is retained in the form of a DAG of functions, with edges\ndenoting data dependencies (input <- output). Then, when backward is\ncalled, the graph is processed in the topological ordering, by calling\nbackward() methods of each Function object, and passing\nreturned gradients on to next Function s. Normally, the only way users interact with functions is by creating\nsubclasses and defining new operations. This is a recommended way of\nextending torch.autograd. Examples: Function.backward Defines a formula for differentiating the operation. Function.forward Performs the operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What are the edges of the DAG of functions denoting?",
        "Y": "data dependencies",
        "Z": "Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function\nobject, that performs the computation, and records that it happened.\nThe history is retained in the form of a DAG of functions, with edges\ndenoting data dependencies (input <- output). Then, when backward is\ncalled, the graph is processed in the topological ordering, by calling\nbackward() methods of each Function object, and passing\nreturned gradients on to next Function s. Normally, the only way users interact with functions is by creating\nsubclasses and defining new operations. This is a recommended way of\nextending torch.autograd. Examples: Function.backward Defines a formula for differentiating the operation. Function.forward Performs the operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How is the graph processed when backward is called?",
        "Y": "by calling backward() methods of each Function object, and passing returned gradients on to next Function s",
        "Z": "Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function\nobject, that performs the computation, and records that it happened.\nThe history is retained in the form of a DAG of functions, with edges\ndenoting data dependencies (input <- output). Then, when backward is\ncalled, the graph is processed in the topological ordering, by calling\nbackward() methods of each Function object, and passing\nreturned gradients on to next Function s. Normally, the only way users interact with functions is by creating\nsubclasses and defining new operations. This is a recommended way of\nextending torch.autograd. Examples: Function.backward Defines a formula for differentiating the operation. Function.forward Performs the operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is a recommended way of extending?",
        "Y": "torch.autograd",
        "Z": "Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function\nobject, that performs the computation, and records that it happened.\nThe history is retained in the form of a DAG of functions, with edges\ndenoting data dependencies (input <- output). Then, when backward is\ncalled, the graph is processed in the topological ordering, by calling\nbackward() methods of each Function object, and passing\nreturned gradients on to next Function s. Normally, the only way users interact with functions is by creating\nsubclasses and defining new operations. This is a recommended way of\nextending torch.autograd. Examples: Function.backward Defines a formula for differentiating the operation. Function.forward Performs the operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "autograd includes what that lets you inspect the cost of different operators inside your model - both on the CPU and GPU?",
        "Y": "profiler",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "profile",
        "Y": "CPU-only",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "nvprof based",
        "Y": "emit_nvtx",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "what manages autograd profiler state and holds a summary of results?",
        "Y": "Context manager",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "it just records events of functions being executed in what language?",
        "Y": "C++",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "profiler is what?",
        "Y": "thread local",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation.",
        "Y": "use_cuda",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "approximately how much overhead does each tensor operation add?",
        "Y": "4us",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each ten",
        "Y": "use_cuda",
        "Z": "enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What enables timing of CUDA events as well as using the cudaEvent API?",
        "Y": "use_cuda",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What information will be collected if shapes recording is set?",
        "Y": "input dimensions",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What allows one to see which dimensions have been used under the hood and further group by them using?",
        "Y": "prof.key_averages",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What data might shape recording skew?",
        "Y": "profiling data",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is recommended to validate the timing?",
        "Y": "separate runs with and without shape recording",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the most likely skew for bottom most events?",
        "Y": "negligible",
        "Z": "enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Why might the total self cpu time be artificially increased for higher level functions?",
        "Y": "the shape collection",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is used to group input dimensions?",
        "Y": "prof.key_averages",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Shape recording might skew what?",
        "Y": "profiling data",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What might be artificially increased because of shape collection?",
        "Y": "the total self cpu time",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the option to record source information for the ops?",
        "Y": "with_stack",
        "Z": "with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does use_kineto do?",
        "Y": "enable profiling with Kineto profiler",
        "Z": "profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the event profiler that can be used to lower the overhead for GPU-only profiling?",
        "Y": "use_cpu",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What profiler.profile.self_cpu_time_total does?",
        "Y": "profiler.profile.self_cpu_time_total",
        "Z": "with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is bool, optional, default=False?",
        "Y": "record_shapes",
        "Z": "record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How will arguments be listed in the nvtx range wrapping each autograd op?",
        "Y": "in the order they are received by the backend op",
        "Z": "record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Which side of the nvtx range may not match the order in which arguments were passed?",
        "Y": "Python",
        "Z": "record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What may increase the overhead of nvtx range creation?",
        "Y": "shape recording",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does two things: Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing",
        "Y": "Context-manager",
        "Z": "Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does running the forward pass with detection enabled do?",
        "Y": "will allow the backward pass to print the traceback of the forward operation that created the failing backward function",
        "Z": "Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function. Any backward computation that generate \u201cnan\u201d value will raise an error. Warning This mode should be enabled only for debugging as the different tests\nwill slow down your program execution. Example Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection\nbased on its argument mode.\nIt can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. mode (bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "With consideration of what is the arctangent of inputi/otheri/textother_iinputi /other",
        "Y": "the quadrant",
        "Z": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b\nwith consideration of the quadrant. Returns a new tensor with the signed angles\nin radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b)\nand vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second\nparameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first\nparameter, is the y-coordinate.) The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "Inputitextinput_iinputi, the first parameter, is what -coordinate?",
        "Y": "y",
        "Z": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b\nwith consideration of the quadrant. Returns a new tensor with the signed angles\nin radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b)\nand vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second\nparameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first\nparameter, is the y-coordinate.) The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "What is the first input tensor other?",
        "Y": "input (Tensor)",
        "Z": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b\nwith consideration of the quadrant. Returns a new tensor with the signed angles\nin radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b)\nand vector (1,0)(1, 0)(1,0). (Note that otheri\\text{other}_{i}otheri\u200b, the second\nparameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b, the first\nparameter, is the y-coordinate.) The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "Learn how to load data, train and save your models in this quickstart guide.",
        "Y": "build deep neural networks",
        "Z": "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What are PyTorch code examples?",
        "Y": "Bite-size, ready-to-deploy PyTorch code examples",
        "Z": "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a quickstart guide to building a complete ML workflow with PyTorch?",
        "Y": "A step-by-step guide to building a complete ML workflow",
        "Z": "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a generative adversarial network used to generate new celebrities?",
        "Y": "Image/Video",
        "Z": "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.  Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the purpose of PyTorch?",
        "Y": "A step-by-step guide to building a complete ML workflow",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of video learn how to augment your network using a visual attention mechanism?",
        "Y": "Image/Video",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Learn to load and preprocess data from a simple dataset with PyTorch's what library?",
        "Y": "torchaudio library",
        "Z": "A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What library does PyTorch's torchaudio library contain?",
        "Y": "Audio",
        "Z": "Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What programming language does this tutorial introduce?",
        "Y": "PyTorch",
        "Z": "Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Learn to load and preprocess data from a simple dataset with PyTorch's what?",
        "Y": "torchaudio library",
        "Z": "Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Getting-Started Use what to visualize data and model training. Interpretability,Getting-Started,TensorBoard Finet",
        "Y": "TensorBoard",
        "Z": "Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is one of the things that TensorBoard Finetune a pre-trained Mask R-CNN model?",
        "Y": "Interpretability",
        "Z": "Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of dataset does PyTorch's torchaudio library help you format?",
        "Y": "audio",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a sequence-to-sequence model that uses the nn.Transformer module?",
        "Y": "Text",
        "Z": "Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What do you learn how to generate names from?",
        "Y": "languages",
        "Z": "Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the second tutorial in a series of three tutorials?",
        "Y": "leanr how to generate names from languages",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "In a series of three tutorials, what is the third and final tutorial on doing \"NLP From Scratch\"?",
        "Y": "Second",
        "Z": "Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the final tutorial on doing \"NLP From Scratch\"?",
        "Y": "leanr how to generate names from languages",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What library does PyTorch use to classify text?",
        "Y": "torchtext library",
        "Z": "Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Audio Learn how to correctly format a dataset?",
        "Y": "audio",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the third and final tutorial on doing \u201cNLP From Scratch\u201d?",
        "Y": "Second",
        "Z": "Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What library is used to build the dataset and classify text?",
        "Y": "torchtext library",
        "Z": "Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Text Train a language translation model from scratch using what?",
        "Y": "Transformer",
        "Z": "Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript  Convert a model defined in PyTorch into the ONNX format and then run it with ONNX Runtime. Production  Build a simple FX pass that fuses batch norm into convolution to improve performance during inference. FX  Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics FX  Get an overview of Channels Last memory format and understand how it is used to order NCHW tensors in memory preserving dimensions. Memory-Format,Best-Practice,Frontend-APIs",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What does DQN stand for?",
        "Y": "Deep Q Learning",
        "Z": "Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What does PyTorch train to play Mario?",
        "Y": "Double Q-learning agent",
        "Z": "Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript  Convert a model defined in PyTorch into the ONNX format and then run it with ONNX Runtime. Production  Build a simple FX pass that fuses batch norm into convolution to improve performance during inference. FX  Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics FX  Get an overview of Channels Last memory format and understand how it is used to order NCHW tensors in memory preserving dimensions. Memory-Format,Best-Practice,Frontend-APIs",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Learn how to correctly format a dataset?",
        "Y": "audio",
        "Z": "Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a tool that can be used as an initial step for debugging bottlenecks in your program?",
        "Y": "torch.utils.bottleneck",
        "Z": "torch.utils.bottleneck is a tool that can be used as an initial step for\ndebugging bottlenecks in your program. It summarizes runs of your script with\nthe Python profiler and PyTorch\u2019s autograd profiler. Run it on the command line with where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What does torch.utils.bottleneck summarize runs of your script with?",
        "Y": "autograd profiler",
        "Z": "torch.utils.bottleneck is a tool that can be used as an initial step for\ndebugging bottlenecks in your program. It summarizes runs of your script with\nthe Python profiler and PyTorch\u2019s autograd profiler. Run it on the command line with where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What does torch.utils.bottleneck run on the command line with?",
        "Y": "script.py",
        "Z": "torch.utils.bottleneck is a tool that can be used as an initial step for\ndebugging bottlenecks in your program. It summarizes runs of your script with\nthe Python profiler and PyTorch\u2019s autograd profiler. Run it on the command line with where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the nature of CUDA kernels?",
        "Y": "asynchronous",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "Ops that do synchronize appear to be what under regular CPU-mode profilers?",
        "Y": "Ops that do synchronize appear to be extremely expensive",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What may be helpful in cases where timings are incorrect?",
        "Y": "CUDA-mode autograd profiler",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What does the CUDA-mode autograd profiler do?",
        "Y": "Note",
        "Z": "where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the name of the command line where [args] are any number of arguments?",
        "Y": "script.py",
        "Z": "Run it on the command line with where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is python -m torch.utils.bottleneck -h?",
        "Y": "python -m torch.utils.bottleneck -h",
        "Z": "where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "Ops that do synchronize appear to be what?",
        "Y": "Ops that do synchronize appear to be extremely expensive",
        "Z": "Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What are the two types of autograd profiler outputs?",
        "Y": "CPU-only-mode or CUDA-mode",
        "Z": "Run it on the command line with where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What will help you decide if your script is CPU-bound?",
        "Y": "CPU-mode autograd profiler",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "If your script spends most of its time executing on the GPU, then it makes sense to start looking for what in the output of the CU",
        "Y": "responsible CUDA operators",
        "Z": "Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "If your script spends most of its time executing on the GPU, what should you look for in the output of the CUDA-mode auto",
        "Y": "CUDA operators",
        "Z": "Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the reality of the CUDA-mode autograd profiler?",
        "Y": "more complicated",
        "Z": "where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What does the NVTX overhead give a heavily skewed timeline?",
        "Y": "Warning",
        "Z": "To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What are the two extremes of autograd profiler output?",
        "Y": "CPU-only-mode or CUDA-mode",
        "Z": "To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the reality of profiling CUDA code?",
        "Y": "more complicated",
        "Z": "Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What does cProfile include when profiling CUDA code?",
        "Y": "CUDA startup time",
        "Z": "where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "How is checkpointing implemented?",
        "Y": "rerunning a forward-pass segment",
        "Z": "Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What can be advanced by rerunning a forward-pass segment for each checkpointed segment during backward?",
        "Y": "persistent states like the RNG state",
        "Z": "Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What do checkpointed passes using RNG have as compared to non-checkpointed passes?",
        "Y": "deterministic output",
        "Z": "Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What can the logic to stash and restore RNG states incur depending on the runtime of checkpointed operations?",
        "Y": "moderate performance hit",
        "Z": "Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "If deterministic output compared to non-checkpointed passes is not required, supply what to checkpoint or checkpoint_sequ",
        "Y": "preserve_rng_state=False",
        "Z": "Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does not save intermediate activations of the entire computation graph for computing backward?",
        "Y": "the checkpointed part",
        "Z": "Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "The checkpointed part can be applied on what?",
        "Y": "any part of a model",
        "Z": "The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does Checkpointing trade?",
        "Y": "compute for memory",
        "Z": "Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In the backwards pass, what is computed on function again?",
        "Y": "the forward pass",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does checking work by?",
        "Y": "trading compute for memory",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "Which part of the computation graph does not save intermediate activations?",
        "Y": "the checkpointed part",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does Checkpointing currently only support?",
        "Y": "torch.autograd.backward()",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is not supported by Checkpointing?",
        "Y": "torch.autograd.grad()",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does detach() or torch.no_grad() do to a checkpointed segment?",
        "Y": "tensors detached from the computational graph",
        "Z": "The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What causes an error when a tensor is defined to have no gradient in the model?",
        "Y": "makes all the outputs require gradients",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a way to circumvent the backward error?",
        "Y": "detach the tensors outside of the checkpoint function",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "Checkpointing currently only supports what function?",
        "Y": "torch.autograd.backward()",
        "Z": "Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is not supported by checkpointing?",
        "Y": "torch.autograd.grad()",
        "Z": "Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "If function invocation during backward does anything different than the one during forward, the checkpointed version won't be equivalent, and unfortunately it",
        "Y": "global variable",
        "Z": "Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What makes all outputs require gradients?",
        "Y": "checkpoint",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In what model should function know how to handle the inputs passed as the tuple?",
        "Y": "LSTM",
        "Z": "Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In LSTM, if user passes what, the function should use the first input as activation and the second input as hidden?",
        "Y": "activation",
        "Z": "If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a tuple containing inputs to the function Output of running function on *args A helper function for checkpointing",
        "Y": "args",
        "Z": "If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a way to circumvent a tensor error when a tensor is defined to have no gradient in the model",
        "Y": "detach the tensors outside of the checkpoint function",
        "Z": "If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In LSTM, if user passes what, function should use the first input as activation and the second input as hidden.",
        "Y": "activation",
        "Z": "function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the tuple containing inputs to the function Output of running function on *args?",
        "Y": "args",
        "Z": "function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is another name for how checkpointing works?",
        "Y": "checkpoint()",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the name of the warning that is given when a model is checked?",
        "Y": "Warning",
        "Z": "function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What are the functions to run sequentially?",
        "Y": "A torch.nn.Sequential or the list of modules or functions",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a Tensor that is input to functions?",
        "Y": "Number of chunks to create in the model input",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is output of running function on *args?",
        "Y": "A helper function",
        "Z": "Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is output of running functions sequentially on?",
        "Y": "*inputs",
        "Z": "A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "How do we divide a model in segments?",
        "Y": "checkpoint",
        "Z": "Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "The inverse of a symmetric positive-definite matrix AAA is computed using what?",
        "Y": "MAGMA routines",
        "Z": "Computes the inverse of a symmetric positive-definite matrix AAA using its\nCholesky factor uuu: returns matrix inv. The inverse is computed using\nLAPACK routines dpotri and spotri (and the corresponding MAGMA routines). If upper is False, uuu is lower triangular\nsuch that the returned tensor is If upper is True or not provided, uuu is upper\ntriangular such that the returned tensor is input (Tensor) \u2013 the input 2-D tensor uuu, a upper or lower triangular\nCholesky factor upper (bool, optional) \u2013 whether to return a lower (default) or upper triangular matrix out (Tensor, optional) \u2013 the output tensor for inv Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse"
    },
    {
        "X": "If uuu is lower triangular such that the returned tensor is?",
        "Y": "If upper is False",
        "Z": "Computes the inverse of a symmetric positive-definite matrix AAA using its\nCholesky factor uuu: returns matrix inv. The inverse is computed using\nLAPACK routines dpotri and spotri (and the corresponding MAGMA routines). If upper is False, uuu is lower triangular\nsuch that the returned tensor is If upper is True or not provided, uuu is upper\ntriangular such that the returned tensor is input (Tensor) \u2013 the input 2-D tensor uuu, a upper or lower triangular\nCholesky factor upper (bool, optional) \u2013 whether to return a lower (default) or upper triangular matrix out (Tensor, optional) \u2013 the output tensor for inv Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse"
    },
    {
        "X": "What does the function A. Returns a tuple containing the LU factorization and pivots of A?",
        "Y": "Computes the LU factorization of a matrix or batches of matrices",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is LU factorization with pivot = False?",
        "Y": "not available for CPU",
        "Z": "Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Why does this function not check if the factorization was successful or not if get_infos is True?",
        "Y": "the status of the factorization is present in the third element of the return tuple",
        "Z": "Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What happens when batches of square matrices have size less than 32 on a CUDA device?",
        "Y": "LU factorization is repeated for singular matrices",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the result of torch.lu_unpack()?",
        "Y": "Warning",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "LU factorization with pivot = False is available for what?",
        "Y": "CUDA",
        "Z": "Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What happens when batches of square matrices with size less than 32 are on a CUDA device?",
        "Y": "LU factorization is repeated for singular matrices",
        "Z": "Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the tensor to factor of size (,m,n)(*, m, n)(,m",
        "Y": "A (Tensor)",
        "Z": "Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If pivot is False, what are the returned pivots?",
        "Y": "a tensor filled with zeros of the appropriate size",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "A (Tensor) is what to factor of size (,m,n)(*, m, n)(",
        "Y": "tensor",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does pivot do?",
        "Y": "controls whether pivoting is done",
        "Z": "A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default setting for LU factorization with pivot = False?",
        "Y": "False",
        "Z": "LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Why is the LU factorization repeated in batches of square matrices with size less than 32 on a CUDA device?",
        "Y": "the LU factorization is repeated for singular matrices",
        "Z": "This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does pivot (bool, optional) do?",
        "Y": "controls whether pivoting is done",
        "Z": "Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If get_infos is True, then the elements in the tuple are Tensor, IntTensor?",
        "Y": "If get_infos is False",
        "Z": "A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default value for a tuple?",
        "Y": "False out",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What factorization is repeated in batches of square matrices with size less than 32 on a CUDA device?",
        "Y": "LU",
        "Z": "In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is done when the tensor to factor of size (,m,n)(*, m, n)(",
        "Y": "pivoting",
        "Z": "L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Out (tuple, optional) \u2013 what?",
        "Y": "optional output tuple",
        "Z": "out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "How many arguments does map_location have?",
        "Y": "two",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is the storage argument?",
        "Y": "initial deserialization of the storage",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What identifies the device it was saved from?",
        "Y": "location tag",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What are the builtin location tags for CPU tensors?",
        "Y": "'cpu'",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What type of tensors are 'cuda:device_id'?",
        "Y": "CUDA",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What should map_location return?",
        "Y": "None or a storage",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "If map_location returns None or a storage, it will be used as the final deserialized object, already moved to the right device?",
        "Y": "a storage",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What will fall back to the default behavior if map_location wasn't specified?",
        "Y": "torch.load()",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "If map_location is a string containing a device tag, it indicates the location where all tensors should be loaded?",
        "Y": "torch.device object or a string containing a device tag",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded.",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "Map_location indicates the location where all what should be loaded?",
        "Y": "tensors",
        "Z": "If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "Who can register their own location tags and deserialization methods using torch.serialization.register_package()?",
        "Y": "User extensions",
        "Z": "Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is the name of the warning message that is passed over to pickle_module.load() and pickle_module.Unpickler",
        "Y": "Warning",
        "Z": "If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What will map_location be used to remap location tags appearing in the file?",
        "Y": "if map_location is a dict",
        "Z": "Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is called when a file contains GPU tensors?",
        "Y": "torch.load()",
        "Z": "Note When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note By default, we decode byte strings as utf-8.  This is to avoid a common error\ncase UnicodeDecodeError: 'ascii' codec can't decode byte 0x...\nwhen loading files saved by Python 2 in Python 3.  If this default\nis incorrect, you may use an extra encoding keyword argument to specify how\nthese objects should be loaded, e.g., encoding='latin1' decodes them\nto strings using latin1 encoding, and encoding='bytes' keeps them\nas byte arrays which can be decoded later with byte_array.decode(...). Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What do you want to avoid when loading a model checkpoint?",
        "Y": "GPU RAM surge",
        "Z": "Note When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note By default, we decode byte strings as utf-8.  This is to avoid a common error\ncase UnicodeDecodeError: 'ascii' codec can't decode byte 0x...\nwhen loading files saved by Python 2 in Python 3.  If this default\nis incorrect, you may use an extra encoding keyword argument to specify how\nthese objects should be loaded, e.g., encoding='latin1' decodes them\nto strings using latin1 encoding, and encoding='bytes' keeps them\nas byte arrays which can be decoded later with byte_array.decode(...). Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is a common error case when decoding byte strings as utf-8?",
        "Y": "UnicodeDecodeError",
        "Z": "Note When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note By default, we decode byte strings as utf-8.  This is to avoid a common error\ncase UnicodeDecodeError: 'ascii' codec can't decode byte 0x...\nwhen loading files saved by Python 2 in Python 3.  If this default\nis incorrect, you may use an extra encoding keyword argument to specify how\nthese objects should be loaded, e.g., encoding='latin1' decodes them\nto strings using latin1 encoding, and encoding='bytes' keeps them\nas byte arrays which can be decoded later with byte_array.decode(...). Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What decodes strings using latin1 encoding?",
        "Y": "encoding='latin1'",
        "Z": "Note When you call torch.load() on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can call torch.load(.., map_location='cpu')\nand then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint. Note By default, we decode byte strings as utf-8.  This is to avoid a common error\ncase UnicodeDecodeError: 'ascii' codec can't decode byte 0x...\nwhen loading files saved by Python 2 in Python 3.  If this default\nis incorrect, you may use an extra encoding keyword argument to specify how\nthese objects should be loaded, e.g., encoding='latin1' decodes them\nto strings using latin1 encoding, and encoding='bytes' keeps them\nas byte arrays which can be decoded later with byte_array.decode(...). Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "Where does the AlexNet end-to-end from PyTorch to?",
        "Y": "ONNX",
        "Z": "Example: End-to-end AlexNet from PyTorch to ONNX Tracing vs Scripting Type Annotations Write PyTorch model in Torch way Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can the ONNX exporter be?",
        "Y": "both trace-based and script-based exporter",
        "Z": "In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Tracing vs what?",
        "Y": "Scripting Type Annotations",
        "Z": "Tracing vs Scripting Type Annotations Write PyTorch model in Torch way Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the field used to write a PyTorch model?",
        "Y": ".data field",
        "Z": "Write PyTorch model in Torch way Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What field does numpy avoid?",
        "Y": ".data field",
        "Z": "Avoid using numpy Avoid using .data field Using dictionaries to handle Named Arguments as model inputs Indexing Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What Operator Export Type is ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTH",
        "Y": "Operator Export Type ONNX",
        "Z": "Getter Setter TorchVision support Limitations Supported operators Adding support for operators ATen operators Non-ATen operators Custom operators Operator Export Type ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the exporter that exports a pretrained AlexNet into ONNX?",
        "Y": "ONNX",
        "Z": "ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH Frequently Asked Questions Use external data format Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What types of exporters can the ONNX exporter be?",
        "Y": "trace-based and script-based exporter",
        "Z": "Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What happens when a model is dynamic?",
        "Y": "changes behavior depending on input data",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of exporter will unroll loops and if conditions?",
        "Y": "trace-based exporter",
        "Z": "Training Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Functions Here is a simple script which exports what into ONNX?",
        "Y": "a pretrained AlexNet",
        "Z": "Functions Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the future, there will be what?",
        "Y": "backends for other frameworks",
        "Z": "You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does this script export into ONNX?",
        "Y": "a pretrained AlexNet",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will there be in the future?",
        "Y": "backends for other frameworks",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a binary protobuf file that contains both the network structure and parameters of the model you exported?",
        "Y": "alexnet.onnx",
        "Z": "The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "A trace is likely to be valid only for what?",
        "Y": "a specific input size",
        "Z": "You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do we recommend examining the model trace and making sure?",
        "Y": "traced operators look reasonable",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will unroll the loops and if conditions?",
        "Y": "trace-based exporter",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do we allow mixing?",
        "Y": "tracing and scripting",
        "Z": "You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you do to suit the particular requirements of a part of a model?",
        "Y": "compose tracing and scripting",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the exporter that unrolls the for loop?",
        "Y": "trace-based exporter",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the backend for the exported script?",
        "Y": "Caffe2",
        "Z": "You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: Then, you can run: To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you run the exported model with?",
        "Y": "ONNX Runtime",
        "Z": "You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another tutorial for ONNX Runtime?",
        "Y": "exporting the SuperResolution model to ONNX",
        "Z": "You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another tutorial for using the ONNX backend?",
        "Y": "exporting the SuperResolution model to ONNX",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What two things do we allow mixing?",
        "Y": "tracing and scripting",
        "Z": "Once these are installed, you can use the backend for ONNX Runtime: Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. The ONNX exporter can be both trace-based and script-based exporter. trace-based means that it operates by executing your model once, and exporting the operators which\nwere actually run during this run.  This means that if your model is\ndynamic, e.g., changes behavior depending on input data, the export\nwon\u2019t be accurate.  Similarly, a trace is likely to be valid only\nfor a specific input size (which is one reason why we require explicit inputs\non tracing.)  We recommend examining the model trace and making sure\nthe traced operators look reasonable.  If your model contains control flows like\nfor loops and if conditions, trace-based exporter will unroll the loops and if conditions,\nexporting a static graph that is exactly the same as this run.  If you want\nto export your model with dynamic control flows, you will need to use the script-based exporter. script-based means that the model you are trying to export is a ScriptModule.\nScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language,\nthat creates serializable and optimizable models from PyTorch code. We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: With trace-based exporter, we get the result ONNX graph which unrolls the for loop: To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: Now the exported ONNX graph becomes: The dynamic control flow is captured correctly. We can verify in backends with different loop range.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How many ways are there to handle models that consist of named parameters or keyword arguments as inputs?",
        "Y": "two",
        "Z": "There are two ways to handle models which consist of named parameters or keyword arguments as inputs: The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed For example, in the model: There are two ways of exporting the model: Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example, Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a dictionary always the last argument in a tuple args?",
        "Y": "last argument in the args tuple",
        "Z": "There are two ways to handle models which consist of named parameters or keyword arguments as inputs: The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed For example, in the model: There are two ways of exporting the model: Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example, Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In cases where the last input is also of a dictionary type, it is mandatory to have what as the last argument in the args t",
        "Y": "empty dictionary",
        "Z": "There are two ways to handle models which consist of named parameters or keyword arguments as inputs: The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed For example, in the model: There are two ways of exporting the model: Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example, Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is provided as the last input in the args tuple args?",
        "Y": "an empty dictionary",
        "Z": "There are two ways to handle models which consist of named parameters or keyword arguments as inputs: The first method is to pass all the inputs in the same order as required by the model and pass None\nvalues for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where\nthe key represents the name of the argument in the model signature and the value represents the value\nof the argument to be passed For example, in the model: There are two ways of exporting the model: Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example, Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Adding export support for operators is an what?",
        "Y": "advance usage",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Developers need to touch the source code of what?",
        "Y": "PyTorch",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What should be easy to add support for exporting an operator?",
        "Y": "adding a symbolic function for the operator",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class.",
        "Y": "ATen operator",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where is the ATen operator/function defined?",
        "Y": "VariableType.h",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must EXACTLY match the names in VariableType.h?",
        "Y": "Parameter names",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Inputs are always what?",
        "Y": "first, then non-tensor arguments",
        "Z": "Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in ONNX, we only need to create a node to represent what in",
        "Y": "ONNX operator",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding what?",
        "Y": "PyTorch Function class",
        "Z": "Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the symbolic function have to be added in the corresponding PyTorch Function class?",
        "Y": "Create a symbolic",
        "Z": "Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch.\nPlease follow the instructions\nfor installing PyTorch from source.\nIf the wanted operator is standardized in ONNX, it should be easy to add\nsupport for exporting such operator (adding a symbolic function for the operator).\nTo confirm whether the operator is standardized or not, please check the\nONNX operator list. If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do you create a symbolic function?",
        "Y": "Create a symbolic function named symbolic in the corresponding Function class",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Parameter names except the first must what?",
        "Y": "EXACTLY match the names in forward",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of functions are Symbolic functions?",
        "Y": "Symbolic functions",
        "Z": "If the operator is an ATen operator, which means you can find the declaration\nof the function in torch/csrc/autograd/generated/VariableType.h\n(available in generated code in PyTorch install dir), you should add the symbolic\nfunction in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below: Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example\ntorch/onnx/symbolic_opset9.py.\nMake sure the function has the same name as the ATen operator/function\ndefined in VariableType.h. The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Which parameter is always the exported ONNX graph?",
        "Y": "first",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in what language, we only need to create a node to represent the ONN",
        "Y": "ONNX",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do you create a symbolic function in the corresponding Function class?",
        "Y": "Create a symbolic function named symbolic",
        "Z": "In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Parameter names except the first must EXACTLY match the names in what?",
        "Y": "forward",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in what language, we just need to create a node to represent the ONN",
        "Y": "ONNX",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Python methods are implemented via what?",
        "Y": "C++-Python bindings",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of handling missing symbolic function for?",
        "Y": "elu operator",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the export fail to export because PyTorch does not support exporting elu operator?",
        "Y": "virtual Tensor elu",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operator does this example handle missing symbolic function for?",
        "Y": "elu operator",
        "Z": "If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the virtual Tensor elu?",
        "Y": "virtual Tensor elu",
        "Z": "The first parameter is always the exported ONNX graph.\nParameter names must EXACTLY match the names in VariableType.h,\nbecause dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operator is elu?",
        "Y": "ATen operator",
        "Z": "Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is checked to confirm that Elu is standardized in ONNX?",
        "Y": "ONNX operator list",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must EXACTLY match the names in forward and output tuple size to match the outputs of forward?",
        "Y": "elu",
        "Z": "Parameter ordering does NOT necessarily match what is in VariableType.h,\ntensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do we check to confirm that Elu is standardized in ONNX?",
        "Y": "ONNX operator list",
        "Z": "In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where can you find examples of PyTorch's exporting elu operator?",
        "Y": "symbolic_opset9.py",
        "Z": "In the symbolic function, if the operator is already standardized in ONNX,\nwe only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How do you add a symbolic function in the corresponding Function class?",
        "Y": "Create a symbolic function named symbolic",
        "Z": "If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in ONNX, we just need to what?",
        "Y": "create a node",
        "Z": "If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is checked to confirm that elu is standardized in ONNX?",
        "Y": "ONNX operator list",
        "Z": "If the input argument is a tensor, but ONNX asks for a scalar, we have to\nexplicitly do the conversion. The helper function _scalar can convert a\nscalar tensor into a python scalar, and _if_scalar_type_as can turn a\nPython scalar into a PyTorch tensor. If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are examples of PyTorch exporting elu operator?",
        "Y": "symbolic_opset9.py, symbolic_opset10.py",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Is the interface for specifying operator definitions experimental or experimental?",
        "Y": "experimental",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent what operator",
        "Y": "ONNX operator",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operator does an example handle missing symbolic function for?",
        "Y": "elu operator",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the operator that elu is an ATen operator?",
        "Y": "virtual Tensor elu",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Where can you export your custom ops implementation?",
        "Y": "ONNX",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you export the custom operator as?",
        "Y": "one or a combination of existing ONNX ops",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can you export a custom operator as in ONNX?",
        "Y": "custom op",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does custom opset specify?",
        "Y": "custom domain and version",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What do custom ONNX ops need to extend?",
        "Y": "backend",
        "Z": "Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface. Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: Depending on the custom operator, you can export it as one or a combination of existing ONNX ops.\nYou can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain\nand version (custom opset) using the custom_opsets dictionary at export. If not\nexplicitly specified, the custom opset version is set to 1 by default.\nUsing custom ONNX ops, you will need to extend the backend of your choice\nwith matching custom ops implementation, e.g. Caffe2 custom ops,\nONNX Runtime custom ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are some examples of PyTorch's elu operator?",
        "Y": "symbolic_opset9.py, symbolic_opset10.py",
        "Z": "If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions: Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX,\nwe just need to create a node to represent the ONNX operator in the graph. Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: Now PyTorch is able to export elu operator. There are more examples in\nsymbolic_opset9.py,\nsymbolic_opset10.py. The interface for specifying operator definitions is experimental;\nadventurous users should note that the APIs will probably\nchange in a future interface.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How often does the ONNX format exporter run a model?",
        "Y": "runs your model once",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does model (torch.nn.Module) represent?",
        "Y": "the model to be exported",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The last value of a tuple consisting of named parameters and the corresponding inputs are structured as what?",
        "Y": "key-value pairs",
        "Z": "Export a model into ONNX format.  This exporter runs your model\nonce in order to get a trace of its execution to be exported;\nat the moment, it supports a limited set of dynamic models (e.g., RNNs.) model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If certain named argument is not present in the dictionary, it is assigned what?",
        "Y": "default value",
        "Z": "The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would a case in which dictionary input is the last input of the args tuple cause when a dictionary of named parameters is",
        "Y": "a conflict",
        "Z": "model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the return value of the model to be exported?",
        "Y": "x m",
        "Z": "model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The last value of a tuple consisting of non-keyword arguments is structured as what?",
        "Y": "key-value pairs",
        "Z": "args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would the call to export API look like?",
        "Y": "torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)",
        "Z": "m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What function assumes that the x input is intended to represent the optional dictionary consisting of named arguments?",
        "Y": "the export function",
        "Z": "args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is placed to prevent this from being an issue?",
        "Y": "constraint",
        "Z": "model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another name for a dictionary consisting of named arguments (optional))?",
        "Y": "torch.Tensor",
        "Z": "args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If dictionary input is the last input of the args tuple, what would happen when a dictionary of named parameters is used?",
        "Y": "a conflict",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is placed to provide an empty dictionary as the last input in the tuple args in such cases?",
        "Y": "constraint",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The new call to export API would now assume that the x input is intended to represent the optional dictionary consisting of named parameters and the corresponding input",
        "Y": "The new call",
        "Z": "args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013  a dictionary to specify the input to the corresponding named parameter:\n- KEY: str, named parameter\n- VALUE: corresponding input\nargs can be structured either as: ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is another name for a TUPLE OF ARGUMENTS?",
        "Y": "torch.Tensor",
        "Z": "ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of arguments will be hard-coded into the exported model?",
        "Y": "non-Tensor",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are the corresponding inputs in a tuple?",
        "Y": "key-value pairs",
        "Z": "ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What model provides an example of a tuple with a dictionary of named parameters?",
        "Y": "model below",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is provided as the last input in the tuple args in such cases?",
        "Y": "empty dictionary",
        "Z": "In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What type of object has to implement fileno that returns a file descriptor?",
        "Y": "a file-like object",
        "Z": "ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a binary Prototype?",
        "Y": "A binary Proto",
        "Z": "ONLY A TUPLE OF ARGUMENTS or torch.Tensor: The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The inputs to the model are structured as what?",
        "Y": "key-value pairs",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the return value of Model() k?",
        "Y": "x m",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the file-like object that has to implement fileno that returns a file descriptor?",
        "Y": "export_params",
        "Z": "The inputs to the model, e.g., such that model(*args) is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported model;\nany Tensor arguments will become inputs of the exported model, in the order they\noccur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor. A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values()",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a bool, default True value for a tuple of arguments with a dictionary of named parameters?",
        "Y": "export_params",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What value does Model() k return?",
        "Y": "x m",
        "Z": "The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If specified, all parameters will be exported. Set this to False if you want to export an untrained model.",
        "Y": "export_params",
        "Z": "f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Set this to what if you want to export an untrained model?",
        "Y": "False",
        "Z": "export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will we print out of the trace being exported?",
        "Y": "debug description",
        "Z": "f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Training (enum, default) \u2013 TrainingMode.EVAL: export the model in inference mode in what mode?",
        "Y": "TrainingMode.EVAL",
        "Z": "The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does TrainingMode.EVAL stand for?",
        "Y": "Training",
        "Z": "The inputs to the model are structured as a tuple consisting of\nnon-keyword arguments and the last value of this tuple being a dictionary\nconsisting of named parameters and the corresponding inputs as key-value pairs.\nIf certain named argument is not present in the dictionary, it is assigned\nthe default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Cases in which dictionary input is the last input of the args tuple would cause what?",
        "Y": "a conflict",
        "Z": "Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will we print out if the trace is exported?",
        "Y": "debug description",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What : export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign",
        "Y": "TrainingMode.TRAINING",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Names to assign to the input nodes of the model?",
        "Y": "input_names",
        "Z": "Cases in which an dictionary input is the last input of the args tuple\nwould cause a conflict when a dictionary of named parameters is used.\nThe model below provides such an example. \u2026\nreturn x m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What would the export function assume that the x input is intended to represent?",
        "Y": "the optional dictionary consisting of named arguments",
        "Z": "torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does torch.onnx.export represent?",
        "Y": "a file-like object",
        "Z": "m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What = Model() k = torch.randn(2, 3) x = torch.tensor(1",
        "Y": "m",
        "Z": "m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Names to assign to the output nodes of the graph in order what?",
        "Y": "output_names",
        "Z": "m = Model()\nk = torch.randn(2, 3)\nx = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is intended to represent the optional dictionary consisting of named arguments?",
        "Y": "the x input",
        "Z": "In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In what mode is the model exported?",
        "Y": "aten mode",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If using aten mode, what is exported by the functions in symbolic_opsetversion?",
        "Y": "all the ops original exported",
        "Z": "In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How would this work?",
        "Y": "as intended",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does the export function assume that the x input is intended to represent?",
        "Y": "the optional dictionary consisting of named arguments",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "The constraint is placed to provide what as the last input in the tuple args in such cases?",
        "Y": "empty dictionary",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does model, (k, x, ), \u2018test.onnx\u2019 mean?",
        "Y": "torch.onnx.export",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "In what mode is the model exported in?",
        "Y": "aten mode",
        "Z": "f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is bool, default False?",
        "Y": "export_raw_ir",
        "Z": "output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does export_raw_ir do?",
        "Y": "use operator_export",
        "Z": "This would work as intended. However, the export function\nwould now assume that the x input is intended to represent the optional\ndictionary consisting of named arguments. In order to prevent this from being\nan issue a constraint is placed to provide an empty dictionary as the last\ninput in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does torch.onnx.export do?",
        "Y": "torch.onnx.export",
        "Z": "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does export the internal IR directly instead of converting it to ONNX ops?",
        "Y": "operator_export_type",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type",
        "Y": "internal IR",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does OperatorExport do?",
        "Y": "OperatorExportTypes",
        "Z": "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019) f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "A file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name",
        "Y": "f",
        "Z": "f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What operator does OperatorExportTypes.ONNX_ATEN use?",
        "Y": "Operator",
        "Z": "f \u2013 a file-like object (has to implement fileno that returns a file descriptor)\nor a string containing a file name.  A binary Protobuf will be written\nto this file. export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If model.training is what, export the model in inference mode if model.training is what?",
        "Y": "False",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the exporter that exports the internal IR directly instead of converting it to ONNX ops?",
        "Y": "export_raw_ir",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace). Operator",
        "Y": "ONNX",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is not supported in ONNX or its symbolic is missing?",
        "Y": "ATen op",
        "Z": "export_params (bool, default True) \u2013 if specified, all parameters will\nbe exported.  Set this to False if you want to export an untrained model.\nIn this case, the exported model will first take all of its parameters\nas arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug\ndescription of the trace being exported. training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as:",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Which OperatorExportTypes.RAW : Export raw ir?",
        "Y": "OperatorExportTypes.RAW",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the operator that is not supported in ONNX?",
        "Y": "ONNX_FALLTHROUGH",
        "Z": "training (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode.\nTrainingMode.PRESERVE: export the model in inference mode if model.training is\nFalse and to a training friendly mode if model.training is True.\nTrainingMode.TRAINING: export the model in a training friendly mode. input_names (list of strings, default empty list) \u2013 names to assign to the\ninput nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the\noutput nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the\nmodel in aten mode. If using aten mode, all the ops original exported\nby the functions in symbolic_opset<version>.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type]\nexport the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does OperatorExportTypes.ONNX call the raw ir?",
        "Y": "OperatorExportTypes.RAW",
        "Z": "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013  OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "OperatorExportTypes.ONNX: All ops are exported as regular what?",
        "Y": "ONNX ops",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does OperatorExportTypes.RAW do?",
        "Y": "Export raw ir",
        "Z": "In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall",
        "Y": "custom ONNX op",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What can the user use to implement the op?",
        "Y": "runtime backend",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is not supported, hence exporter falls through?",
        "Y": "prim::ListConstruct",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What must be _onnx_main_opset or in _onnx_stable_opset?",
        "Y": "opset_version",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "By default we export to what version of the ONNX submodule?",
        "Y": "one stable opset version",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Right now, supported stable opset version is what?",
        "Y": "9.",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does example_outputs stand for?",
        "Y": "example_outputs",
        "Z": "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops\n(with ONNX namespace).\nOperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace).\nOperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported\nin ONNX or its symbolic is missing, fall back on ATen op. Registered ops\nare exported to ONNX regularly.\nExample graph: is exported as: In the above example, aten::triu is not supported in ONNX, hence\nexporter falls back on this op.\nOperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported\nin ONNX, fall through and export the operator as is, as a custom\nONNX op. Using this mode, the op can be exported and implemented by\nthe user for their runtime backend.\nExample graph: is exported as: In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If there is more than one item, it should be passed in tuple format, e.g. what = (x, y",
        "Y": "example_outputs",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If there is more than one item, what should be passed as the example output?",
        "Y": "only one item",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "When does strip_doc_string (bool, default True) strip the field \"doc_string\" from the exported model?",
        "Y": "if True",
        "Z": "opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "By default we export to how many stable opset versions?",
        "Y": "one stable opset version",
        "Z": "In the above example, prim::ListConstruct is not supported, hence\nexporter falls through. opset_version (int, default is 9) \u2013 by default we export the model to the\nopset version of the onnx submodule. Since ONNX\u2019s latest opset may\nevolve before next stable release, by default we export to one stable\nopset version. Right now, supported stable opset version is 9.\nThe opset_version must be _onnx_main_opset or in _onnx_stable_opsets\nwhich are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding\noptimization is applied to the model during export. Constant-folding\noptimization will replace some of the ops that have all constant\ninputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, list of Tensors, Tensor, int, float, bool, default None) \u2013 Model\u2019s example outputs being exported. \u2018example_outputs\u2019 must be provided when exporting\na ScriptModule or TorchScript Function. If there is more than one item, it should be passed\nin tuple format, e.g.: example_outputs = (x, y, z). Otherwise, only one item should\nbe passed as the example output, e.g. example_outputs=x.\nexample_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field\n\u201cdoc_string\u201d from the exported model, which information about the stack\ntrace. dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is dynamic_axes?",
        "Y": "a dictionary to specify dynamic axes of input/output",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "A list of integers specifying the dynamic axes of provided input. In this scenario automated names will be generated and applied to dynamic axes",
        "Y": "(1)",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What specifies the dynamic axes of provided input?",
        "Y": "A list of integers",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will be generated and applied to dynamic axes during export?",
        "Y": "automated names",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "An inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to",
        "Y": "(2)",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to be applied on",
        "Y": "An inner dictionary",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is an example of an inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output to the",
        "Y": "Example",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If true, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph.",
        "Y": "If True",
        "Z": "INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph.",
        "Y": "If False",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If False, initializers are not added as inputs to the graph, and only the non-parameter inputs are added as input",
        "Y": "better optimizations",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who executes these graphs?",
        "Y": "backends/runtimes",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If unspecified (what is the default value of dynamic_axes) then the behavior is chosen automatically as follows?",
        "Y": "default None",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to what?",
        "Y": "True",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If opset_version is set to what, this argument will be set to what?",
        "Y": "8 or lower",
        "Z": "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013  a dictionary to specify dynamic axes of input/output, such that:\n- KEY:  input and/or output names\n- VALUE: index of dynamic axes for given key and potentially the name to be used for\nexported dynamic axes. In general the value is defined according to one of the following\nways or a combination of both:\n(1). A list of integers specifying the dynamic axes of provided input. In this scenario\nautomated names will be generated and applied to dynamic axes of provided input/output\nduring export.\nOR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in\ncorresponding input/output TO the name that is desired to be applied on such axis of\nsuch input/output during export. Example. if we have the following shape for inputs and outputs: Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What are not added as inputs to the graph if False?",
        "Y": "initializers",
        "Z": "Then dynamic axes can be defined either as: ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If opset_version argument is set to a 8 or lower, what argument will be ignored?",
        "Y": "if opset_version argument is set to a 8 or lower",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is a dictionary to indicate custom opset domain and version at export?",
        "Y": "custom_opsets",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "If model contains a custom opset, it is optional to specify the domain and what in the dictionary?",
        "Y": "opset version",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What will be run as part of the export to ensure the exported model is a valid ONNX model?",
        "Y": "enable_onnx_checker",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the name of the MIXED MODE of (1) and (2)?",
        "Y": "keep_initializers_as_inputs",
        "Z": "ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who executes graphs?",
        "Y": "backends/runtimes",
        "Z": "ONLY INDICES: INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What is the onnx model checker run as part of the export?",
        "Y": "ensure the exported model is a valid ONNX model",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "Who executes graphs that are not initialized as inputs?",
        "Y": "backends/runtimes",
        "Z": "INDICES WITH CORRESPONDING NAMES: MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What does this allow for backends/runtimes that execute graphs?",
        "Y": "better optimizations",
        "Z": "MIXED MODE OF (1) and (2): keep_initializers_as_inputs (bool, default None) \u2013  If True, all the\ninitializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding\netc.) by backends/runtimes that execute these graphs. If\nunspecified (default None), then the behavior is chosen\nautomatically as follows. If operator_export_type is\nOperatorExportTypes.ONNX, the behavior is equivalent to setting\nthis argument to False. For other values of operator_export_type,\nthe behavior is equivalent to setting this argument to True. Note\nthat for ONNX opset version < 9, initializers MUST be part of graph\ninputs. Therefore, if opset_version argument is set to a 8 or\nlower, this argument will be ignored. custom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate\ncustom opset domain and version at export. If model contains a custom opset,\nit is optional to specify the domain and opset version in the dictionary:\n- KEY: opset domain name\n- VALUE: opset version\nIf the custom opset is not provided in this dictionary, opset version is set\nto 1 by default. enable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run\nas part of the export, to ensure the exported model is a valid ONNX model.",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "What counterpart allows you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0?",
        "Y": "CUDA",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns True if the input is a what?",
        "Y": "single element tensor",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Creation ops are listed under Random sampling and include: torch.rand() torch.rand() torch.rand_like() torch.r",
        "Y": "Random sampling",
        "Z": "It has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0   Returns True if obj is a PyTorch tensor.   Returns True if obj is a PyTorch storage object.   Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.   Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.   Returns True if the input is a single element tensor which is not equal to zero after type conversions.   Sets the default floating point dtype to d.   Get the current default floating point torch.dtype.   Sets the default torch.Tensor type to floating point tensor type t.   Returns the total number of elements in the input tensor.   Set options for printing.   Disables denormal floating numbers on CPU. Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Constructs a tensor with what?",
        "Y": "data",
        "Z": "Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.   Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}basestart to baseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Create a view of an existing torch.Tensor input with what?",
        "Y": "specified size, stride and storage_offset",
        "Z": "Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.   Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}basestart to baseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor filled with what value 1 with the same size as input?",
        "Y": "scalar",
        "Z": "Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.   Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}basestart to baseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is created whose values are evenly spaced from basestarttext?",
        "Y": "a one-dimensional tensor of size steps",
        "Z": "Note Random sampling creation ops are listed under Random sampling and\ninclude:\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\nYou may also use torch.empty() with the In-place random sampling\nmethods to create torch.Tensor s with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Convert the data into a torch.Tensor.   Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.   Creates a Tensor from a numpy.ndarray.   Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 0, with the same size as input.   Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.   Returns a tensor filled with the scalar value 1, with the same size as input.   Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [start, end) taken with common difference step beginning from start.   Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.   Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}basestart to baseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor of the same size as input with each element sampled from a what distribution?",
        "Y": "Poisson",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Random integers generated uniformly between low (inclusive) and high (exclusive) return a tensor with the same shape as Tens",
        "Y": "exclusive",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution quasirandom.SobolEngine The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the what?",
        "Y": "standard normal distribution",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution quasirandom.SobolEngine The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean what?",
        "Y": "0 and variance 1",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the in-place version of torch.bernoulli()?",
        "Y": "torch.Tensor.bernoulli_()",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Draws binary random numbers (0 or 1) from what distribution?",
        "Y": "Bernoulli distribution",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor with what size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1",
        "Y": "same size",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution quasirandom.SobolEngine The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance",
        "Y": "random permutation of integers",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution quasirandom.SobolEngine The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Random integers generated uniformly between low (inclusive) and high (exclusive) Returns a tensor with the same shape as input",
        "Y": "exclusive",
        "Z": "Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the in-place version of torch.bernoulli() torch?",
        "Y": "torch.Tensor.bernoulli_()",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).   Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from 0 to n - 1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_() - in-place version of torch.bernoulli() torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution torch.Tensor.exponential_() - numbers drawn from the exponential distribution torch.Tensor.geometric_() - elements drawn from the geometric distribution torch.Tensor.log_normal_() - samples from the log-normal distribution torch.Tensor.normal_() - in-place version of torch.normal() torch.Tensor.random_() - numbers sampled from the discrete uniform distribution torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution quasirandom.SobolEngine The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs what of tensor1 by tensor2 multiply the result by the scalar value and add it to input?",
        "Y": "element-wise multiplication",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the given input tensor's what?",
        "Y": "element-wise angle",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "The element-wise arctangent of inputi/otheritextinputi / textother_i",
        "Y": "quadrant",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the bitwise NOT of the given input tensor. Computes the bitwise OR of input and other. Computes",
        "Y": "Computes the bitwise NOT of the given input tensor",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the bitwise AND of input and other. Computes the bitwise XOR of input and other. Computes the bit",
        "Y": "Computes the bitwise AND of input and other",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the bitwise OR of input and other. Computes the bitwise XOR of input and other.",
        "Y": "Computes the bitwise OR of input and other",
        "Z": "Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the bitwise XOR of input and other. Computes the bitwise OR of input and other. Computes the what",
        "Y": "bitwise XOR",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does it do to all elements in input into the range?",
        "Y": "Clamps all elements in input into the range",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What code computes the element-wise conjugate of the given input tensor?",
        "Y": "torch.clamp()",
        "Z": "Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is created with the magnitude of input and the sign of other, elementwise?",
        "Y": "floating-point tensor",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a new tensor with the cosine of the elements of input. Returns a new tensor with what",
        "Y": "hyperbolic",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Adds what to each element of the input input and returns a new resulting tensor?",
        "Y": "scalar",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.   Alias for torch.acosh().   Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Alias for torch.asinh(). Returns a new tensor with the what of the elements of input?",
        "Y": "arctangent",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "With consideration of what is the element-wise arctangent of inputi/otheritextinputi / text",
        "Y": "quadrant",
        "Z": "Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the what of input and other?",
        "Y": "bitwise XOR",
        "Z": "Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Clamp all elements in input into the range of?",
        "Y": "Clamps all elements in input into the range",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What function computes the element-wise conjugate of the given input tensor?",
        "Y": "torch.clamp()",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Create a new what with the magnitude of input and the sign of other, elementwise?",
        "Y": "floating-point tensor",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a new tensor with the cosine of the elements of input. Returns a new tensor with the",
        "Y": "Returns a new tensor",
        "Z": "Adds the scalar other to each element of the input input and returns a new resulting tensor.   Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What function returns a new tensor with the arctangent of the elements of input?",
        "Y": "torch.asinh()",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "The arctangent of inputi/otheri/textinputi /otheri is considered with consideration of what?",
        "Y": "quadrant",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the bitwise OR of input and other. Computes the bitwise XOR of input and other. Computes the bit",
        "Y": "Computes the bitwise OR of input and other",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What are the elements of input converted from?",
        "Y": "angles in degrees to radians",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What divides each element of the input?",
        "Y": "Divides each element of the input",
        "Z": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Multiply the result by what?",
        "Y": "scalar value",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "With consideration of what is the arctangent of inputi/otheri / textother_iinputi",
        "Y": "quadrant",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the element-wise conjugate of the given input tensor. Create a new floating-point tensor with the",
        "Y": "torch.clamp()",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How does each element of the input input by the corresponding element of other?",
        "Y": "Divides",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the logarithmic derivative of the gamma?",
        "Y": "torch.div()",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes what derivative of the gamma function?",
        "Y": "logarithmic derivative",
        "Z": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.   Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does each element of the input input by the corresponding element of other do?",
        "Y": "Divides",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the logarithmic derivative of the gamma function on input?",
        "Y": "torch.div()",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the what of the gamma function on input?",
        "Y": "logarithmic derivative",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the function that computes the logarithmic derivative of the gamma function on input?",
        "Y": "torch.special.erf()",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does Alias for torch.special.erf(). Alias for what?",
        "Y": "torch.special.erfc()",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Alias for torch.special.erf(). Alias for torch.special.erfc(). What is the name of the",
        "Y": "Alias for torch.special.erf",
        "Z": "Computes the element-wise angle (in radians) of the given input tensor.   Returns a new tensor with the arcsine  of the elements of input.   Alias for torch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements of input.   Alias for torch.asinh().   Returns a new tensor with the arctangent  of the elements of input.   Alias for torch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements of input.   Alias for torch.atanh().   Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b with consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND of input and other.   Computes the bitwise OR of input and other.   Computes the bitwise XOR of input and other.   Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.   Clamps all elements in input into the range [ min, max ].   Alias for torch.clamp().   Computes the element-wise conjugate of the given input tensor.   Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.   Returns a new tensor with the cosine  of the elements of input.   Returns a new tensor with the hyperbolic cosine  of the elements of input.   Returns a new tensor with each of the elements of input converted from angles in degrees to radians.   Divides each element of the input input by the corresponding element of other.   Alias for torch.div().   Computes the logarithmic derivative of the gamma function on input.   Alias for torch.special.erf().   Alias for torch.special.erfc().   Alias for torch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensor input.   Alias for torch.special.exp2().   Alias for torch.special.expm1().   Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Broadcasts input to what?",
        "Y": "shape shape",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is similar to broadcast_tensors() but for shapes?",
        "Y": "broadcast_tensors()",
        "Z": "Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries. Do cartesian product of",
        "Y": "indices",
        "Z": "Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes batched the distance between each pair of the two collections of row vectors. Returns a copy of input. Compute combinations of",
        "Y": "p-norm",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Compute combinations of length rrr of the given tensor. Returns a copy of input. Returns a copy of",
        "Y": "Compute combinations of length rrr of the given tensor",
        "Z": "Returns a 1-dimensional view of each input tensor with zero dimensions.   Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a namedtuple where values is the cumulative minimum of elements of input in the dimension dim. Returns the cumulative product of elements of",
        "Y": "values",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a partial view of input with its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the",
        "Y": "2-D square tensor",
        "Z": "Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Sums the product of the elements of the input operands along dimensions specified using a notation based on what?",
        "Y": "Einstein",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does reshaping input into a one-dimensional tensor do?",
        "Y": "Flattens input",
        "Z": "Returns a 1-dimensional view of each input tensor with zero dimensions.   Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Compute combinations of length rrr of the given tensor?",
        "Y": "Compute combinations of length rrr of the given tensor",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes what difference along the given dimension?",
        "Y": "n-th forward",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Flattens input by reshaping it into what?",
        "Y": "one-dimensional tensor",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "In the left/right direction, return a new tensor. What does Flip tensor do?",
        "Y": "Flip tensor",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Flip tensor in the left/right direction, returning a new tensor?",
        "Y": "Flip tens",
        "Z": "Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Compute combinations of length rrr of the given tensor. Returns the cross product of vectors in dimension dim of input and",
        "Y": "Compute combinations of length rrr of the given tensor",
        "Z": "Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what (values, indices) where values is the cumulative maximum of elements of input in the dimension dim?",
        "Y": "a namedtuple",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns a namedtuple where values is the cumulative minimum of elements of input in the dimension dim. Returns a namedtuple",
        "Y": "values",
        "Z": "Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is similar to broadcasts input to the shape shape?",
        "Y": "broadcast_tensors()",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "In the left/right direction, return a new tensor. In the up/down direction, return a new tensor",
        "Y": "Flip tensor",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a new tensor. Computes the Kronecker product, denoted by otimes?",
        "Y": "Flip tensor in the up/down direction",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes what product, denoted by otimes, of input and other?",
        "Y": "Kronecker",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Rotate a n-D Rotate a n-D Rotate a n-D Rotate a n-",
        "Y": "Rotate a n-D",
        "Z": "Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Similar to broadcast_tensors() but for what?",
        "Y": "shapes",
        "Z": "Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the along the given dimension. Sums the product of the elements of the input operands along dimensions specified using a notation",
        "Y": "n-th forward difference",
        "Z": "Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What happens to input by reshaping it into a one-dimensional tensor?",
        "Y": "Flattens",
        "Z": "Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How do you rotate a n-D tensor?",
        "Y": "Rotate a n-D tensor by 90 degrees",
        "Z": "Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "In the left/right direction, returning a new tensor, what do you do?",
        "Y": "Flip tensor",
        "Z": "Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Compute what of the given tensor?",
        "Y": "combinations of length rrr",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the difference along the given dimension. Sums the product of the elements of the input operands along dimensions specified using a notation",
        "Y": "n-th forward",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the element-wise greatest common divisor of input and other. Computes the histogram of a tenss.",
        "Y": "GCD",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes batched the distance between each pair of the two collections of row vectors. Returns a copy of input. Computes what",
        "Y": "p-norm",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the input and other. Rotate a n-D tensor by 90 degrees in the plane specified by dims",
        "Y": "Kronecker",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does GCD stand for?",
        "Y": "element-wise greatest common divisor",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What are NNN tensors?",
        "Y": "Take NNN tensors",
        "Z": "Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Compute combinations of length rrr of the given tensor. Returns a copy of input.",
        "Y": "Compute combinations of length rrr of the given tensor",
        "Z": "Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the GCD of input and other. Computes the histogram of a tensor.",
        "Y": "element-wise greatest common divisor",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create what?",
        "Y": "NNN N-dimensional grids",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Compute what combinations of the given tensor?",
        "Y": "combinations of length rrr",
        "Z": "Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the difference along the given dimension?",
        "Y": "n-th forward",
        "Z": "Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does flip tensor in the up/down direction return a new tensor?",
        "Y": "Flip tensor in the up/down direction",
        "Z": "Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what of elements of input in the dimension dim?",
        "Y": "cumulative product",
        "Z": "Returns a 2-dimensional view of each input tensor with zero dimensions.   Returns a 3-dimensional view of each input tensor with zero dimensions.   Count the frequency of each value in an array of non-negative ints.   Create a block diagonal matrix from provided tensors.   Broadcasts the given tensors according to Broadcasting semantics.   Broadcasts input to the shape shape.   Similar to broadcast_tensors() but for shapes.   Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy of input.   Compute combinations of length rrr of the given tensor.   Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does NNN N-dimensional grids do?",
        "Y": "Compute",
        "Z": "Returns the cross product of vectors in dimension dim of input and other.   Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.   Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.   Returns the cumulative product of elements of input in the dimension dim.   Returns the cumulative sum of elements of input in the dimension dim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.   Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.   Flattens input by reshaping it into a one-dimensional tensor.   Reverse the order of a n-D tensor along given axis in dims.   Flip tensor in the left/right direction, returning a new tensor.   Flip tensor in the up/down direction, returning a new tensor.   Computes the Kronecker product, denoted by \u2297\\otimes\u2297, of input and other.   Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.   Computes the element-wise greatest common divisor (GCD) of input and other.   Computes the histogram of a tensor.   Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs.   Computes the element-wise least common multiple (LCM) of input and other.   Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs a matrix multiplication of the matrices what?",
        "Y": "mat1 and mat2",
        "Z": "Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Returns what matrix product of the NNN 2-D tensors?",
        "Y": "matrix product of the NNN 2-D tensors",
        "Z": "Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the inverse of a symmetric positive-definite matrix AAA using what?",
        "Y": "Cholesky factor uuu",
        "Z": "Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the factorization of a matrix or batches of matrices A. Returns the LU solve of the linear system Ax",
        "Y": "LU",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What function returns the LU solve of the linear system Ax=bAx = b using the partially pivoted LU factorization of A",
        "Y": "torch.lu()",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.   Computes the singular value decomposition of either a matrix or batch of matrices input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Unpacks the data?",
        "Y": "Unpacks the data",
        "Z": "Performs a matrix multiplication of the matrices mat1 and mat2.   Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs what product of the matrix mat and the vector vec?",
        "Y": "matrix-vector product",
        "Z": "Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Positive semidefinite matrix to be inverted given what?",
        "Y": "Cholesky factor matrix uuu",
        "Z": "Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the what factorization of a matrix or batches of matrices A?",
        "Y": "LU",
        "Z": "Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the LU factorization of a tensor do?",
        "Y": "Unpacks the data and pivots",
        "Z": "Performs a matrix-vector product of the matrix mat and the vector vec.   Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.   Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs a what product of matrices in batch1 and batch2?",
        "Y": "batch matrix-matrix",
        "Z": "Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the what of a matrix or batches of matrices A?",
        "Y": "LU factorization",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.   Computes the singular value decomposition of either a matrix or batch of matrices input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the LU factorization of a matrix or batches of matrices A?",
        "Y": "LU factorization of a tensor",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the matrix product of matrices stored in input and mat2?",
        "Y": "Mat",
        "Z": "Performs a batch matrix-matrix product of matrices in batch1 and batch2.   Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does a positive semidefinite matrix have to be inverted given?",
        "Y": "Cholesky factor matrix uuu",
        "Z": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does the matrix product of two tensors produce?",
        "Y": "Matrix product of two tensors",
        "Z": "Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the Alias for torch.linalg.matrix product of two tensors?",
        "Y": "torch.linalg.matrix",
        "Z": "Performs a batch matrix-matrix product of matrices stored in input and mat2.   Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the what decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices",
        "Y": "Cholesky",
        "Z": "Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does matrix_power return for a 2-D tensor?",
        "Y": "numerical rank",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the numerical rank of a 2-D tensor?",
        "Y": "Compute",
        "Z": "Returns the matrix product of the NNN 2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the what product of two 1D tensors?",
        "Y": "dot",
        "Z": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is computed by LU factorization of a tensor into tensors?",
        "Y": "Matrix product of two tensors",
        "Z": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs what of the matrices input and mat2?",
        "Y": "matrix multiplication",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.   Computes the singular value decomposition of either a matrix or batch of matrices input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs a what product of the matrix input and mat2. Performs a matrix multiplication of the matrices input and mat2. Perform",
        "Y": "matrix-vector",
        "Z": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu: returns matrix inv.   Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the result of LU factorization of a tensor?",
        "Y": "Matrix product of two tensors",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is returned by a 2-D tensor?",
        "Y": "numerical rank",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Performs a what product of the matrix input and the vector vec?",
        "Y": "matrix-vector",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.   Computes the singular value decomposition of either a matrix or batch of matrices input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the name of the function that computes the matrix-matrix?",
        "Y": "torch.linalg.householder_product()",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes what matrix-matrix multiplication?",
        "Y": "matrix-matrix multiplication",
        "Z": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu.   Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the what of two 1D tensors?",
        "Y": "dot product",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is computed by lu()?",
        "Y": "Matrix product of two tensors",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What product of input and vec2?",
        "Y": "Outer product",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Who is responsible for Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix",
        "Y": "Alias",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes what for 1D tensors?",
        "Y": "dot product",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does matrix_power() return the numerical rank of?",
        "Y": "2-D tensor",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Computes",
        "Y": "Outer product",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What decomposition does pinv compute?",
        "Y": "QR decomposition of a matrix",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the function that computes the dot product for 1D tensors?",
        "Y": "Computes the dot product for 1D tensors",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.   Computes the singular value decomposition of either a matrix or batch of matrices input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is the result of lu()?",
        "Y": "Matrix product of two tensors",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.   Computes the singular value decomposition of either a matrix or batch of matrices input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What does matrix_power return?",
        "Y": "numerical rank of a 2-D tensor",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.   Computes the singular value decomposition of either a matrix or batch of matrices input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "Outer product of input and what?",
        "Y": "vec2",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.   Computes the singular value decomposition of either a matrix or batch of matrices input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What returns a QR decomposition of a matrix or a batch of matrices input?",
        "Y": "namedtuple",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias of torch.outer().   Computes the dot product for 1D tensors.   Alias for torch.linalg.inv()   Alias for torch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias for torch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size (m\u00d7n)(m \\times n)(m\u00d7n) and a matrix BBB of size (m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matrices A.   Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that LU_data, LU_pivots = (P @ L @ U).lu().   Matrix product of two tensors.   Alias for torch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.   Computes the matrix exponential of a square matrix or of each square matrix in a batch.   Performs a matrix multiplication of the matrices input and mat2.   Performs a matrix-vector product of the matrix input and the vector vec.   Alias for torch.linalg.householder_product().   Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.   Outer product of input and vec2.   Alias for torch.linalg.pinv()   Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices.   This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of A, in order as a namedtuple solution, LU.   Computes the singular value decomposition of either a matrix or batch of matrices input.",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "What is used to create Python code matching the Graph's semantics?",
        "Y": "Graph IR",
        "Z": "FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can symbolic tracing be used in isolation for?",
        "Y": "to capture a form of the code for analysis (and not transformation) purposes",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How many uses does FX have?",
        "Y": "many uses",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can some examples of FX transformations be found?",
        "Y": "examples repository",
        "Z": "FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the intermediate representation of a GraphModule constitute?",
        "Y": "Python-to-Python transformation pipeline",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the symbolic tracer perform of the Python code?",
        "Y": "symbolic execution",
        "Z": "The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the intermediate representation of the GraphModule constitute?",
        "Y": "Python-to-Python transformation pipeline",
        "Z": "The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Why can symbolic tracing be used in isolation?",
        "Y": "to capture a form of the code for analysis (and not transformation) purposes",
        "Z": "The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can some examples of transformations be found?",
        "Y": "examples repository",
        "Z": "The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we create for each Graph IR?",
        "Y": "Python code matching the Graph\u2019s semantics",
        "Z": "The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be used for programmatically generating models?",
        "Y": "Code generation",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What should your FX transform return as identical to a regular torch?",
        "Y": "torch",
        "Z": "The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is it possible to do to an existing GraphModule instead of creating a new one?",
        "Y": "modify an existing GraphModule",
        "Z": "The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the FX transform that can be passed to another FX transform?",
        "Y": "TorchScript",
        "Z": "Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository. What is an FX transform? Essentially, it\u2019s a function that looks like this. Your transform will take in an torch.nn.Module, acquire a Graph\nfrom it, do some modifications, and return a new\ntorch.nn.Module. You should think of the torch.nn.Module that your FX\ntransform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\ntorch.nn.Module will allow for composability. Note It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: Note that you MUST call GraphModule.recompile() to bring the generated\nforward() method on the GraphModule in sync with the modified Graph. Given that you\u2019ve passed in a torch.nn.Module that has been traced into a\nGraph, there are now two primary approaches you can take to building a new\nGraph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "All three concepts are represented with what?",
        "Y": "Node instances",
        "Z": "All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the target of the placeholder node?",
        "Y": "x",
        "Z": "target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can a full treatment of the semantics of all three concepts be found?",
        "Y": "Node documentation",
        "Z": "All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What specifies the return value in a Graph?",
        "Y": "a special output node",
        "Z": "target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In what language is code represented?",
        "Y": "FX",
        "Z": "All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is one approach to building a new Graph?",
        "Y": "directly manipulate your old one",
        "Z": "opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can we do to aid in building a new Graph?",
        "Y": "modify it",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we want to replace with torch.mul() calls?",
        "Y": "torch.add() calls",
        "Z": "Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of rewrites can be done?",
        "Y": "Graph rewrites",
        "Z": "All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can a full treatment of the semantics of the operations in the method be found?",
        "Y": "Node documentation",
        "Z": "target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the return value of the method?",
        "Y": "return value",
        "Z": "Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In what programming language is code represented?",
        "Y": "FX",
        "Z": "name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does FX have to aid in transforming the graph?",
        "Y": "utility functions",
        "Z": "Here we define a module MyModule for demonstration purposes, instantiate it,\nsymbolically trace it, then call the Graph.print_tabular() method to print\nout a table showing the nodes of this Graph: opcode name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Name target args kwargs placeholder what ()  get_attr linear_weight linear.weight ()",
        "Y": "x x",
        "Z": "name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "To aid in building a new Graph, we can simply take the Graph we obtain from symbolic tracing and what?",
        "Y": "modify it",
        "Z": "target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of using APIs to append a call to a Graph?",
        "Y": "torch.relu()",
        "Z": "name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "For simple transformations that only consist of substitutions, you can make use of what?",
        "Y": "subgraph rewriter",
        "Z": "name target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the methods specified via special placeholder nodes?",
        "Y": "inputs",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we now know about how code is represented in FX?",
        "Y": "basics",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an example of a torch.mul() call?",
        "Y": "append",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Simple transformations that only consist of substitutions can also make use of what?",
        "Y": "subgraph rewriter",
        "Z": "target args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the return value specified by in a Graph?",
        "Y": "a special output node",
        "Z": "args kwargs placeholder x x () {} get_attr linear_weight linear.weight () {} call_function add_1 <built-in function add> (x, linear_weight) {} call_module linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What would you want to replace torch.add() calls with?",
        "Y": "torch.mul()",
        "Z": "relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "For each what can we create valid Python code matching the Graph's semantics?",
        "Y": "Graph IR",
        "Z": "This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: The symbolic tracer performs \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non theses Proxies are recorded. More information about symbolic tracing\ncan be found in the symbolic_trace() and Tracer\ndocumentation. The intermediate representation is the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nor torch.nn.Module instances), and return values. More information\nabout the IR can be found in the documentation for Graph. The\nIR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up in GraphModule, which is a\ntorch.nn.Module instance that holds a Graph as well as a\nforward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX! Several example transformations can be found at the\nexamples\nrepository.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the call_method relu_1 relu (linear_1)?",
        "Y": "linear_1",
        "Z": "linear_1 linear (add_1,) {} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the function that is built-in method sum?",
        "Y": "call_function sum_1",
        "Z": "{} call_method relu_1 relu (linear_1,) {} call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1} call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {} output output output (topk_1,) {} We can use this information to answer the questions we posed above. What are the inputs to the method? In FX, method inputs are specified\nvia special placeholder nodes. In this case, we have a single\nplaceholder node with a target of x, meaning we have\na single (non-self) argument named x. What are the operations within the method? The get_attr,\ncall_function, call_module, and call_method nodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in the Node\ndocumentation. What is the return value of the method? The return value in a\nGraph is specified by a special output node. Given that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a Graph. One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is another name for torch.add()?",
        "Y": "torch.mul()",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can tedious graph manipulation code become as the transformations get more complex?",
        "Y": "can get unwieldy",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the replace_pattern API used for?",
        "Y": "Basic usage Quantization Invert Transformation",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. For simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter. FX also provides another level of automation on top of direct graph manipulation.\nThe replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing\nGraphs. It allows you to specify a pattern and replacement function\nand it will trace through those functions, find instances of the group of operations\nin the pattern graph, and replace those instances with copies of the replacement\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex. Replace one\nop Conv/Batch Norm\nfusion replace_pattern: Basic usage Quantization Invert Transformation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What would we do with the graph rewriting?",
        "Y": "insert the comparison and multiplication after the F.relu",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be used to automate the rewriting process?",
        "Y": "Proxy objects",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Proxy objects are called what?",
        "Y": "arugments",
        "Z": "Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. In addition to avoiding explicit graph manipulation, using Proxys\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. A worked example of using Proxys for Graph manipulation\ncan be found\nhere.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we want to run and record the torch.Tensor shape and dtype properties on?",
        "Y": "GraphModule",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What class encompasses the above logic in a way that certain aspects of the interpreter\u2019s execution can be overridden via method overrides",
        "Y": "Interpreter",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the Transformer class called?",
        "Y": "Shape Propagation Performance Profiler",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: As you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe Interpreter class, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides. In addition to executing operations, we can also generate a new\nGraph by feeding Proxy values through an interpreter.\nSimilarly, we provide the Transformer class to encompass\nthis pattern. Transformer behaves similarly to\nInterpreter, but instead of calling the run method to\nget a concrete output value from the Module, you would call the\nTransformer.transform() method to return a new\nGraphModule which was subject to any transformation rules\nyou installed as overridden methods. Shape\nPropagation Performance Profiler",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What happens in the course of authoring transformations?",
        "Y": "our code will not be quite right",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers. Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In the course of authoring transformations, our code may not be quite right. In this case, we may need to do what?",
        "Y": "debugging",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers. Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the first tool in our toolbox to check if transformed modules are behaving as we expect?",
        "Y": "torch.allclose()",
        "Z": "Often in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers. Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you want to run the same code multiple times, it can be a bit tedious to step to the right code with what tool?",
        "Y": "pdb",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an easier way to examine modules and parameters?",
        "Y": "to_folder",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the first step in identifying that a transformation is creating incorrect code?",
        "Y": "debug the transformation itself",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What section in the documentation will we check first to see if a transformation is creating incorrect code?",
        "Y": "Limitations of Symbolic Tracing",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the goal of tracing once we verify that tracing is working as expected?",
        "Y": "figuring out what went wrong during our GraphModule transformation",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a quick answer to the question of what went wrong during a GraphModule transformation?",
        "Y": "Writing Transformations",
        "Z": "Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be a good next step if it's not clear what's going wrong?",
        "Y": "pdb",
        "Z": "This is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation. Because FX generates the forward() function on GraphModules, using\ntraditional debugging techniques like print statements or pdb is\nnot as straightfoward. Luckily, we have several techniques we can use\nfor debugging the generated code. Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When the forward pass is invoked, what is used to step into the Graph?",
        "Y": "pdb",
        "Z": "Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you\u2019d like to run the same code multiple times, it can be a bit tedious to step to the right code with what?",
        "Y": "pdb",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How do you examine modules and parameters?",
        "Y": "to_folder",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Now that we\u2019ve identified that a transformation is creating incorrect code, it\u2019s time to what?",
        "Y": "debug the transformation itself",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the goal of debugging if tracing is working as expected?",
        "Y": "GraphModule transformation",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Sometimes, a simple visual comparison is enough to trace down what?",
        "Y": "a bug",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What debugger can be a good next step if it\u2019s still not clear what\u2019s going wrong?",
        "Y": "pdb",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we start to find what goes wrong using a debugger?",
        "Y": "pdb session",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.) The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does a pdb session break on?",
        "Y": "transform_graph(traced)",
        "Z": "Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where can you find a quick answer to this question?",
        "Y": "Writing Transformations",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we start if we want to find what goes wrong using a debugger?",
        "Y": "pdb session",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does a debugger break on?",
        "Y": "transform_graph(traced)",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we want to see?",
        "Y": "Node\u2019s",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a quick answer to the question of what went wrong in GraphModule?",
        "Y": "Writing Transformations",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What debugger is used to find out what went wrong?",
        "Y": "pdb",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. After running the above example, we can then look at the code within\nfoo/module.py and modify it as desired (e.g. adding print\nstatements or using pdb) to debug the generated code. Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the most common Python debugger?",
        "Y": "pdb",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the debugger we want to break on?",
        "Y": "transform_graph(traced)",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.) The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do we want to see in a Node?",
        "Y": "input_nodes and users",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the file you want to debug?",
        "Y": "python -m pdb FILENAME.py",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How do pdb debugger commands move through your running program?",
        "Y": "stepwise",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a breakpoint set when you start pdb?",
        "Y": "b LINE-NUMBER",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.) The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do you need to use to get to the part of the code you want to examine?",
        "Y": "s or n",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can you write before the line you want to break at?",
        "Y": "import pdb; pdb.set_trace()",
        "Z": "Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.) The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you add what, your program will automatically start in debug mode when you run it?",
        "Y": "pdb.set_trace()",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can your program start in debug mode when you run it?",
        "Y": "type python FILENAME.py into the command line",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.) The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you add pdb.set_trace(), your program will automatically start in debug mode when you run it?",
        "Y": "Once",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step. Going off of the example above, consider the following code: Using the above example, let\u2019s say that the call to print(traced)\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a pdb session. We can see\nwhat\u2019s happening during the transform by breaking on\ntransform_graph(traced), then pressing s to \u201cstep into\u201d the call\nto transform_graph(traced). We may also have good luck by editing the print_tabular method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019s input_nodes and users.) The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can you do with the pdb debugger commands?",
        "Y": "move through your running program stepwise",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does b LINE-NUMBER do when you start pdb?",
        "Y": "set a breakpoint",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What commands do you need to use to get to the part of the code you want to examine?",
        "Y": "s or n",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does import pdb call before the line you want to break at?",
        "Y": "pdb.set_trace()",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can you type into the command line instead of python -m pdb FILENAME.py?",
        "Y": "python FILENAME.py",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can you do once you're running your file in debug mode?",
        "Y": "step through the code and examine your program\u2019s internal state using certain commands",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is an excellent tutorial on pdb?",
        "Y": "RealPython\u2019s \u201cPython Debugging With Pdb\u201d",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How can you use pdb in VSCode?",
        "Y": "a) use pdb by pulling up a terminal window in your IDE",
        "Z": "The most common Python debugger is\npdb. You can start\nyour program in \u201cdebug mode\u201d with pdb by typing\npython -m pdb FILENAME.py into the command line, where FILENAME\nis the name of the file you want to debug. After that, you can use the\npdb debugger commands\nto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (b LINE-NUMBER) when you start pdb, then call c to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using s or n) to get to the part\nof the code you want to examine. Alternatively, you can write\nimport pdb; pdb.set_trace() before the line you want to break at.\nIf you add pdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just type\npython FILENAME.py into the command line instead of\npython -m pdb FILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials on pdb online, including RealPython\u2019s\n\u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use pdb by pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around pdb).",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is dynamic control flow?",
        "Y": "x can change",
        "Z": "Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does symbolic tracing use to show you where a situation happens?",
        "Y": "The traceback",
        "Z": "Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Static control flow arises for code making decisions about a model's architecture based on what?",
        "Y": "hyper-parameters",
        "Z": "FX uses a system of symbolic tracing (a.k.a symbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is tracing in that it executes the program (really a\ntorch.nn.Module or function) to record operations. It is\nsymbolic in that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some\nlimitations. The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is supported by symbolic tracing?",
        "Y": "a valid pattern",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of control flow does symbolic tracing not currently support?",
        "Y": "loops or if statements",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a method that can be traced as calls to the Method?",
        "Y": "Customizing Tracing with the Tracer class",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does symbolic tracing use to show you where a situation can happen?",
        "Y": "The traceback",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Why does static control flow arise in PyTorch programs?",
        "Y": "for code making decisions about a model\u2019s architecture based on hyper-parameters",
        "Z": "The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program. For example, let\u2019s examine the following program: The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the method that shows you where a situation happens?",
        "Y": "The traceback",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a loops or if statements whose value cannot change across invocations?",
        "Y": "static control flow",
        "Z": "The condition to the if statement relies on the value of x.sum(),\nwhich relies on the value of x, a function input. Since\nx can change (i.e. if you pass a new input tensor to the traced\nfunction), this is dynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens. On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is not supported by __torch_function__?",
        "Y": "built-in function len",
        "Z": "The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does not depend on any function inputs?",
        "Y": "if-statement if self.do_activation",
        "Z": "The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Many instances of dynamic control flow are what?",
        "Y": "semantically static control flow",
        "Z": "The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What function is not supported by __torch_function__?",
        "Y": "built-in function len",
        "Z": "Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them. FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are some functions that are not covered by __torch_function__?",
        "Y": "builtin Python functions or those in the math module",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What function is not supported?",
        "Y": "built-in function len",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The default set of leaf modules is the set of standard what?",
        "Y": "torch.nn module instances",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What constructors are currently not traceable?",
        "Y": "Tensor constructors",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The value produced by deterministic constructors will be embedded in the trace as what?",
        "Y": "a constant",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What do arguments to constructors refer to?",
        "Y": "dynamic input sizes",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What may be a viable substitute?",
        "Y": "ones_like or zeros_like",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Nondeterministic constructors (rand, randn) will have a single what?",
        "Y": "random value embedded in the trace",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is wrapped in a torch.fx.wrap function?",
        "Y": "torch.randn",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Python 3-style type annotations are supported and will be preserved by what?",
        "Y": "symbolic tracing",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What Python 2-style comment type annotations are supported and will be preserved by symbolic tracing?",
        "Y": "Python 2-style comment type annotations",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: The set of leaf modules can be customized by overriding\nTracer.is_leaf_module(). Tensor constructors (e.g. torch.zeros, torch.ones,\ntorch.rand, torch.randn, torch.sparse_coo_tensor)\nare currently not traceable. The deterministic constructors (zeros, ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,\nones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wrap torch.randn in a torch.fx.wrap function and call that instead. This behavior may be fixed in a future release. Type annotations Python 3-style type annotations (e.g.\nfunc(x : torch.Tensor, y : int) -> torch.Tensor) are supported\nand will be preserved by symbolic tracing. Python 2-style comment type annotations\n# type: (torch.Tensor, int) -> torch.Tensor are not currently\nsupported. Annotations on local names within a function are not currently\nsupported.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a loop that cannot change across invocations?",
        "Y": "if statements",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: In the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\nCustomizing Tracing with the Tracer class) or function (see\nwrap()) rather than tracing through them.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of tracing is used?",
        "Y": "Symbolic tracing",
        "Z": "Symbolic tracing API Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can typically not trace through this due to the presence of control flow?",
        "Y": "FX",
        "Z": "Symbolic tracing API Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If you pass in different values of b, they will be what?",
        "Y": "ignored",
        "Z": "Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Concrete_args will use what to flatten your input?",
        "Y": "pytrees",
        "Z": "Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will a \u201cleaf function\u201d be preserved as?",
        "Y": "a CallFunction node in the FX trace",
        "Z": "This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will this function return if an nn.Module or function instance root is present?",
        "Y": "GraphModule",
        "Z": "Given an nn.Module or function instance root, this function will return a GraphModule\nconstructed by recording operations seen while tracing through root. concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures. For example: FX can typically not trace through this due to the presence of control\nflow. However, we can use concrete_args to specialize on the value of\nb to trace through this. f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the module that can be called at module-level scope to register fn_or_name as a \u201clea",
        "Y": "GraphModule",
        "Z": "f = fx.symbolic_trace(f, concrete_args={\u2018b\u2019: False})\nassert f(3, False)  == 6 Note that although you can still pass in different values of b, they will be ignored. We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the function that can be called at module-level scope to register fn_or_name as a \u201clea",
        "Y": "GraphModule",
        "Z": "We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a root (Union[torch.nn.Module, Callable]) a module or function to be trace",
        "Y": "Graph representation",
        "Z": "root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the global function to insert into the graph when it\u2019s called GraphModule?",
        "Y": "fn_or_name",
        "Z": "enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When graph is reassigned, what will be automatically regenerated?",
        "Y": "code and forward",
        "Z": "This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the function you must call if you edit the contents of the graph without reassigning the graph attribute itself?",
        "Y": "recompile()",
        "Z": "enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Root can either be an nn.Module instance or a Dict mapping strings to any attribute type?",
        "Y": "nn.Module instance or a Dict mapping strings to any attribute type",
        "Z": "enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In the case that root is a Module, any references to what will be copied over from the respective place within root\u2019s Module hierarchy into the",
        "Y": "Module-based objects",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In the case that root is a what?",
        "Y": "dict",
        "Z": "root (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation. concrete_args (Optional[Dict[str, any]]) \u2013 Inputs to be partially specialized enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the GraphModule created from the recorded operations from root?",
        "Y": "GraphModule",
        "Z": "enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In the case that root is a what, any references to Module-based objects will be copied over from the respective place within root\u2019s Module hierarchy",
        "Y": "Module",
        "Z": "enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "In the case that root is a what, the qualified name found in a Node\u2019s target will be looked up directly in the dict",
        "Y": "dict",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The object mapped to by the Dict will be copied over into the appropriate place within what Module\u2019s module hierarchy?",
        "Y": "Graph",
        "Z": "enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the graph that contains the nodes this GraphModule contains?",
        "Y": "graph (Graph) \u2013 graph contains the nodes this GraphModule",
        "Z": "enable_cpatching \u2013 Enables C-level patching of functions (captures things like torch.randn) a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the module created from the recorded operations from root?",
        "Y": "GraphModule",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of module does root belong to?",
        "Y": "Module",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If root is a Module, the qualified name found in a Node\u2019s target will be looked up directly in what?",
        "Y": "dict",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a graph that contains the nodes this GraphModule should use for code generation?",
        "Y": "graph",
        "Z": "a Module created from the recorded operations from root. GraphModule This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the function to update the generated code if you edit the contents of the graph without reassigning the graph attribute itself",
        "Y": "recompile()",
        "Z": "This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where will the object mapped to by the Dict be copied over?",
        "Y": "the appropriate place within the GraphModule\u2019s module hierarchy",
        "Z": "This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Name denotes the name of this GraphModule for what purpose?",
        "Y": "debugging purposes",
        "Z": "Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If it\u2019s unset, what happens to the name of this GraphModule for debugging purposes?",
        "Y": "If it\u2019s unset",
        "Z": "This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can a function equivalently be used as?",
        "Y": "decorator",
        "Z": "This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The object mapped to by the Dict will be copied over into the appropriate place within what?",
        "Y": "GraphModule\u2019s module hierarchy",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is class_name denoted the name of this GraphModule for?",
        "Y": "debugging purposes",
        "Z": "A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If it\u2019s unset, what will happen to all error messages?",
        "Y": "all error messages will report as originating from GraphModule",
        "Z": "This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What should the name of the GraphModule be set to?",
        "Y": "root\u2019s original name or a name that makes sense within the context of your transform",
        "Z": "A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Does this install empty Modules?",
        "Y": "Adds the given submodule to self",
        "Z": "This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Installs empty Modules where none exists?",
        "Y": "This installs empty Modules where none",
        "Z": "This function can also equivalently be used as a decorator: A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be thought of as a \u201cleaf function\u201d?",
        "Y": "A wrapped function",
        "Z": "A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What must you call to update the generated code if you edit the contents of the graph without reassigning the graph attribute itself?",
        "Y": "recompile()",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If root is a what, any references to Module-based objects will be copied over from the respective place within root\u2019s Module hierarchy into the",
        "Y": "Module",
        "Z": "A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will report as originating from GraphModule if unset?",
        "Y": "all error messages",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If unset, all error messages will report as originating from GraphModule. If it\u2019s unset, all error messages will report",
        "Y": "root\u2019s original name",
        "Z": "A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through. fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does this install?",
        "Y": "empty Modules",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What root can be an nn.Module instance or a Dict mapping strings to any attribute type?",
        "Y": "root",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "For what purpose does class_name denote the name of this GraphModule?",
        "Y": "debugging purposes",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If unset, all error messages will report as originating from GraphModule. It may be helpful to set this to what?",
        "Y": "root\u2019s original name",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Installs empty Modules where none exist yet if they are subpaths of target. what does this do?",
        "Y": "Adds the given submodule to self",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "This installs what where none exist yet if they are subpaths of target. target \u2013 The fully-qualified string name of the new sub",
        "Y": "empty Modules",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name of the submodule?",
        "Y": "m",
        "Z": "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\ngraph attribute, as well as code and forward attributes generated\nfrom that graph. Warning When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Which root can be an nn.Module instance or a Dict mapping strings to any attribute type?",
        "Y": "root",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Any references to what will be copied over from the respective place within root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy?",
        "Y": "Module-based objects",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will report as originating from GraphModule?",
        "Y": "all error messages",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does Delete do to the Python code generated from the Graph underlying this GraphModule?",
        "Y": "Delete",
        "Z": "When graph is reassigned, code and forward will be automatically\nregenerated. However, if you edit the contents of the graph without reassigning\nthe graph attribute itself, you must call recompile() to update the generated\ncode. Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Name denotes the name of this GraphModule for what?",
        "Y": "debugging purposes",
        "Z": "graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If it\u2019s unset, what will happen if it\u2019s unset?",
        "Y": "all error messages will report as originating from GraphModule",
        "Z": "graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the name that makes sense within the context of your transform?",
        "Y": "root\u2019s original name",
        "Z": "graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Does this install empty Modules where none exist yet if they are subpaths of target?",
        "Y": "Adds the given submodule to self",
        "Z": "graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Installs what when none exist yet if they are subpaths of target?",
        "Y": "empty Modules",
        "Z": "Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Target \u2013 The name of the new submodule (See example in nn.Module.get_submodule for how to",
        "Y": "fully-qualified string",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What happens to all unused submodules from self?",
        "Y": "Deletes all unused submodules from self",
        "Z": "m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A Module is considered \u201cused\u201d when which of the following is true?",
        "Y": "if any one of the following is true: 1.",
        "Z": "graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A Module is considered \u201cused\u201d if any of the following is true: 1. It has what children that are used 2?",
        "Y": "children that are used 2",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Deletes the given submodule from self. The module will not be deleted if target is not a valid target?",
        "Y": "delete_submodule",
        "Z": "A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Deletes the given submodule from what?",
        "Y": "self",
        "Z": "Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If target is not a valid target, the module will not be deleted?",
        "Y": "if target is not a valid target",
        "Z": "Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Target \u2013 What is the name of the new submodule we want to delete?",
        "Y": "The fully-qualified string name of the new submodule",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does bool Return?",
        "Y": "Graph",
        "Z": "Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What should be called when the GraphModule Recompile this GraphModule from its graph attribute?",
        "Y": "This should be called",
        "Z": "graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If unset, all error messages will report as originating from what?",
        "Y": "GraphModule",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is a name that makes sense within the context of your transform?",
        "Y": "root\u2019s original name",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "This installs what where none exist yet if they are subpaths of target?",
        "Y": "empty Modules",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A Module is considered \u201cused\u201d when what is true?",
        "Y": "if any one of the following is true: 1.",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "bool Return the what underlying this GraphModule Recompile this GraphModule from its graph attribute?",
        "Y": "Graph",
        "Z": "class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When should the GraphModule Recompile this GraphModule from its graph attribute be called?",
        "Y": "after editing the contained graph",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Installs what where none exist yet if they are subpaths of target?",
        "Y": "empty Modules",
        "Z": "Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Target \u2013 What is the name of the new submodule?",
        "Y": "fully-qualified string name of the new submodule",
        "Z": "Construct a GraphModule. root (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case that root is a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place\nwithin root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case that root is a dict, the qualified name found in a Node\u2019s target will be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy. graph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation class_name (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating from GraphModule. It may be helpful to set this\nto root\u2019s original name or a name that makes sense within the context of your transform. Adds the given submodule to self. This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How is a Module considered \u201cused\u201d?",
        "Y": "if any one of the following is true: 1.",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Dumps out module to folder with what name?",
        "Y": "module_name",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A Module is considered \u201cused\u201d if any one of the following is true: 1. It has what that are used?",
        "Y": "children",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the Python code generate from the GraphModule do?",
        "Y": "Deletes all unused submodules from self",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What underlying the GraphModule Recompile this GraphModule from its graph attribute?",
        "Y": "Graph",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph:",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Dumps out module to folder with what?",
        "Y": "module_name",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The submodule itself; the actual object we want to install in the current Module which method to return True, each object in the chain denoted",
        "Y": "m",
        "Z": "m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Deletes the given submodule from self. The module will not be deleted what if target is not a valid target?",
        "Y": "if target is not a valid target",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Target \u2013 what is the name of the new submodule we want to delete?",
        "Y": "The fully-qualified string name of the new submodule",
        "Z": "The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What underlying this GraphModule?",
        "Y": "Graph",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the return value of each object in the chain denoted by target must either a) not exist yet, or b) reference an",
        "Y": "return True",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "If any one of the following is true: 1. it has children that are used 2. Its forward is called directly via a call_module no",
        "Y": "if any one of the following is true: 1.",
        "Z": "Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What is the main data structure used in the FX Intermediate Representation?",
        "Y": "a series of Node s",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What will produce the following Graph?",
        "Y": "the following code",
        "Z": "this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Return what Python code generated from the Graph underlying this GraphModule?",
        "Y": "Python code generated from the Graph underlying this GraphModule",
        "Z": "Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Deletes the given submodule from self. The module will not be deleted if target is not a valid target. target \u2013 The",
        "Y": "delete_submodule",
        "Z": "Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be called after",
        "Y": "bool",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When should the GraphModule be recompiled?",
        "Y": "after editing the contained graph",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Construct an what?",
        "Y": "empty Graph",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Insert what into the Graph?",
        "Y": "a call_function Node into the Graph",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does a call_function node represent?",
        "Y": "a call to a Python callable",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "A call_function node represents a call to a Python callable, specified by what?",
        "Y": "the_function",
        "Z": "Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Where does delete the given submodule from?",
        "Y": "self",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "When should this be called?",
        "Y": "after editing the contained graph",
        "Z": "This installs empty Modules where none exist yet if they are\nsubpaths of target. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.) m \u2013 The submodule itself; the actual object we want to\ninstall in the current Module  this method to return True, each object in the chain\ndenoted by target must either a) not exist yet,\nor b) reference an nn.Module (not a parameter or\nother attribute) bool Return the Python code generated from the Graph underlying this\nGraphModule. Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What does the Graph consist of?",
        "Y": "a series of Node s",
        "Z": "Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be the_function (Callable[.., Any])?",
        "Y": "the_function",
        "Z": "Deletes all unused submodules from self. A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can be the_function (Callable[.., Any]) \u2013 The function to be called?",
        "Y": "the_function",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What type of PyTorch can the function to be called be?",
        "Y": "PyTorch",
        "Z": "A Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via a call_module node\n3. It has a non-Module attribute that is used from a\nget_attr node This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What can the function to be called be?",
        "Y": "PyTorch operator, Python function, or member of the builtins or operator namespaces",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "args (Optional[Tuple[Argument,..]]) \u2013 The what?",
        "Y": "positional arguments to be passed to the called function",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What are the positional arguments to be passed to the called function?",
        "Y": "kwargs",
        "Z": "This method can be called to clean up an nn.Module without\nmanually calling delete_submodule on each unused submodule. Deletes the given submodule from self. The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the",
        "Y": "type_expr",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What module will not be deleted if target is not a valid target?",
        "Y": "module",
        "Z": "The module will not be deleted if target is not a valid\ntarget. target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Return value of what means that the target was not a valid reference to a submodule?",
        "Y": "False",
        "Z": "target \u2013 The fully-qualified string name of the new submodule\n(See example in nn.Module.get_submodule for how to\nspecify a fully-qualified string.)  submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node().",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Insert a what Node into the Graph. A call_function node represents a call to a Python callable, specified by the",
        "Y": "call_function",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "kwargs (Optional[Any]) \u2013 An optional type annotation representing the Python type the output of this node will have.",
        "Y": "type_expr",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Returns The what?",
        "Y": "newly created and inserted call_function node",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "The same insertion point and type expression rules apply for this method as what?",
        "Y": "Graph.create_node()",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "Insert a Which Node into the Graph?",
        "Y": "call_method",
        "Z": "submodule we want to delete. A return value of False\nmeans that the target was not a valid reference to\na submodule. bool Return the Graph underlying this GraphModule Recompile this GraphModule from its graph attribute. This should be\ncalled after editing the contained graph, otherwise the generated\ncode of this GraphModule will be out of date. Dumps out module to folder with module_name so that it can be\nimported with from <folder> import <module_name> folder (Union[str, os.PathLike]) \u2013 The folder to write the code out to module_name (str) \u2013 Top-level name to use for the Module while\nwriting out the code Graph is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of Node s, each representing callsites (or other\nsyntactic constructs). The list of Node s, taken together, constitute a\nvalid Python function. For example, the following code Will produce the following Graph: For the semantics of operations represented in the Graph, please see Node. Construct an empty Graph. Insert a call_function Node into the Graph. A call_function node\nrepresents a call to a Python callable, specified by the_function. the_function\ncan be the_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of the builtins or operator\nnamespaces. args (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed\nto the called function. kwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed\nto the called function type_expr (Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. Returns The newly created and inserted call_function node. Note The same insertion point and type expression rules apply for this method\nas Graph.create_node(). Insert a call_method Node into the Graph. A call_method node\nrepresents a call to a given method on the 0th element of args.",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "What may cause the extension to need to be recompiled?",
        "Y": "a new card is installed",
        "Z": "Creates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the setuptools.Extension for?",
        "Y": "CUDA/C++",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities:",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Convenience method that creates what with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension?",
        "Y": "setuptools.Extension",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If a new card is installed, the extension may need to be recompiled.",
        "Y": "extension may need to be recompiled",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does CC stand for?",
        "Y": "compute capability",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "You can override the default behavior using what to explicitly specify which CCs you want the extension to support?",
        "Y": "TORCH_CUDA_ARCH_LIST",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "This improves your binary\u2019s what?",
        "Y": "forward compatibility",
        "Z": "Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What do you want to target?",
        "Y": "GPUs",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What creates a CUDA/C++ extension?",
        "Y": "setuptools",
        "Z": "Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What are the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension?",
        "Y": "CUDA include path, library path and runtime library",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What happens if a new card is installed?",
        "Y": "extension may need to be recompiled",
        "Z": "Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How can you override the default behavior?",
        "Y": "TORCH_CUDA_ARCH_LIST",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What can modestly reduce performance on those newer CCs?",
        "Y": "relying on older PTX to provide forward compat",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off what?",
        "Y": "specifying them individually",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What GPUs do you want your extension to run on?",
        "Y": "8.0 and 8.6",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvc",
        "Y": "nvcc",
        "Z": "Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary. All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How can you override the default behavior of Pytorch?",
        "Y": "TORCH_CUDA_ARCH_LIST",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What happens when more archs are included?",
        "Y": "slower the building process",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. Example Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension .",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX.",
        "Y": "Compute capabilities",
        "Z": "Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support",
        "Y": "Pytorch",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "The slower the building process will be, because it will build a separate kernel image for each arch.",
        "Y": "the more archs get included",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Along with archs of the cards visible during the building process of the extension, what else will the extension be compiled to run on?",
        "Y": "PTX",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What does PTX improve your binary's?",
        "Y": "forward compatibility",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "The slower the building process will be, because it will build a separate kernel image for each arch?",
        "Y": "the more archs get included",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is used to build a separate kernel image for each arch?",
        "Y": "custom setuptools",
        "Z": "By default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general).",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is used to specify which CCs you want the extension to support?",
        "Y": "TORCH_CUDA_ARCH_LIST",
        "Z": "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off specifying them what?",
        "Y": "individually",
        "Z": "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What will the building process be if more archs are included?",
        "Y": "slower",
        "Z": "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    }
]