[
    {
        "X": "What is a forward pre-hook common to all modules?",
        "Y": "Global Hooks For Module Registers",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does Hold submodules in a dictionary do?",
        "Y": "Holds submodules in a dictionary",
        "Z": "A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is held in a list?",
        "Y": "parameters",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does a module hold in a list?",
        "Y": "Holds parameters in a dictionary",
        "Z": "Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the function that holds submodules in a list?",
        "Y": "Holds submodules in a list",
        "Z": "DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does the Global Hooks For Module register for all the modules?",
        "Y": "global forward hook",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does a submodule hold in a list?",
        "Y": "Holds submodules in a dictionary",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is registered for all the modules?",
        "Y": "global forward hook",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a kind of Tensor that is to be considered?",
        "Y": "module parameter",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a module parameter?",
        "Y": "A parameter that is not initialized",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another term for a dictionary?",
        "Y": "Holds parameters in a dictionary",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.Conv1d do?",
        "Y": "Holds parameters in a dictionary",
        "Z": "A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is common to all the modules?",
        "Y": "a backward hook",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the class that registers a global forward hook for all the modules?",
        "Y": "nn.Conv1d",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "A sequential container. Holds submodules in a list. Holds parameters in a dictionary. Holds parameters in a dictionary.",
        "Y": "Base class for all neural network modules",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is held in a dictionary?",
        "Y": "parameters",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a global forward hook common to all modules?",
        "Y": "nn.Conv1d",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is registered for all modules?",
        "Y": "global forward hook",
        "Z": "Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of convolution does nn.Conv1d apply over an input signal composed of several input planes?",
        "Y": "1D",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies a 1D convolution over an input signal composed of several input planes?",
        "Y": "nn.Conv2d",
        "Z": "Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv1d Applies what convolution over an input signal composed of several input planes?",
        "Y": "1D",
        "Z": "Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where are submodules held?",
        "Y": "a dictionary",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of hook is registered for all the modules?",
        "Y": "a global forward hook",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does a sequential container hold in a dictionary?",
        "Y": "parameters",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Registers a forward pre-hook common to all modules?",
        "Y": "Global Hooks For Module",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does Holds submodules in a dictionary?",
        "Y": "Holds submodules in a dictionary",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of convolution does nn.Conv2d apply over an input signal composed of several input planes?",
        "Y": "2D",
        "Z": "nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies a 2D convolution over an input signal composed of several input planes?",
        "Y": "nn.Conv3d",
        "Z": "Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does Holds submodules in a dictionary do?",
        "Y": "Holds submodules in a dictionary",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another term for holding submodules in a list?",
        "Y": "Holds submodules in a dictionary",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does a dictionary hold?",
        "Y": "parameters",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where do parameters in a neural network belong?",
        "Y": "a dictionary",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "A sequential container. Holds submodules in a list. Holds parameters in a dictionary. Holds what class for all neural network",
        "Y": "Base class",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What hook is common to all modules?",
        "Y": "global forward hook",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv1d Applies a 1D convolution over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of convolution does nn.Conv3d apply over an input signal composed of several input planes?",
        "Y": "3D convolution",
        "Z": "nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of what?",
        "Y": "several input planes",
        "Z": "Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of what?",
        "Y": "several input planes",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies a 1D transposed convolution operator over an input image composed of several input planes?",
        "Y": "nn.ConvTranspose1d",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Applies a 1D convolution over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies a 3D convolution over an input signal composed of several input planes?",
        "Y": "nn.ConvTranspose3d",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ConvTranspose1d apply over an input image composed of several input planes?",
        "Y": "1D transposed convolution operator",
        "Z": "Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ConvTranspose2d apply over an input image composed of several input planes?",
        "Y": "2D transposed convolution operator",
        "Z": "nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConvTranspose1d Applies what transposed convolution operator over an input image composed of several input planes?",
        "Y": "1D",
        "Z": "nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for nn.Conv2d?",
        "Y": "nn.ConvTranspose3d",
        "Z": "nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv3d Applies what type of convolution over an input signal composed of several input planes?",
        "Y": "3D",
        "Z": "Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConvTranspose1d Applies what type of transposed convolution operator over an input image composed of several input planes",
        "Y": "1D",
        "Z": "Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the program that applies a 2D transposed convolution over an input signal?",
        "Y": "nn.ConvTranspose3d",
        "Z": "Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv3d Applies a 3D convolution over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of what?",
        "Y": "several input planes",
        "Z": "Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transposed convolution operator does nn.ConvTranspose2d apply over an input image composed of several input planes",
        "Y": "2D",
        "Z": "Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for nn.LazyConv1d?",
        "Y": "nn.LazyConv1d",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Applies a 3D convolution over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the program that applies a 3D transposed convolution over an input signal?",
        "Y": "nn.LazyConv1d",
        "Z": "Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ConvTranspose1d Apply over an input image composed of several input planes?",
        "Y": "1D transposed convolution operator",
        "Z": "nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ConvTranspose3d apply over an input image composed of several input planes?",
        "Y": "3D transposed convolution operator",
        "Z": "nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.Conv1dmodule with lazy initialization of what of theConv1d?",
        "Y": "thein_channelsargument",
        "Z": "nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which a torch.nn.Conv1dmodule has lazy initialization of thein_channelsargument of theConv",
        "Y": "nn.LazyConv2d",
        "Z": "nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConvTranspose2d Applies what transposed convolution operator over an input image composed of several input planes?",
        "Y": "2D",
        "Z": "Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConvTranspose3d Applies what transposed convolution operator over an input image composed of several input planes?",
        "Y": "3D",
        "Z": "nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is inferred from theinput.size(1)?",
        "Y": "thein_channelsargument",
        "Z": "nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the module with lazy initialization of thein_channelsargument of theConv1d?",
        "Y": "nn.LazyConv2d",
        "Z": "nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv",
        "Y": "nn.LazyConv2d",
        "Z": "Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What transposed convolution operator does nn.ConvTranspose2d Applies over an input image composed of several input planes?",
        "Y": "1D",
        "Z": "Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConvTranspose3d Applies what type of transposed convolution operator over an input image composed of several input planes",
        "Y": "3D",
        "Z": "These are the basic building blocks for graphs: torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transposed convolution operator does nn.ConvTranspose3d apply?",
        "Y": "3D",
        "Z": "nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is lazy initialization of of theConv1d that is inferred from theinput.size(1)?",
        "Y": "thein_channelsargument",
        "Z": "nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transposed convolution operator does nn.ConvTranspose2d apply?",
        "Y": "2D",
        "Z": "nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies over an input image composed of several input planes?",
        "Y": "2D transposed convolution operator",
        "Z": "Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.Conv2dmodule with lazy initialization of what of theConv2d?",
        "Y": "thein_channelsargument",
        "Z": "nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transposed convolution operator does nn.ConvTranspose3d Applies over an input image composed of several input plane",
        "Y": "3D",
        "Z": "Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transposed convolution operator does nn.ConvTranspose3d Applies?",
        "Y": "3D",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the 3D transposed convolution operator?",
        "Y": "nn.LazyConvTranspose3d",
        "Z": "nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is applied over an input image composed of several input planes?",
        "Y": "3D transposed convolution operator",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does a torch.nn.Conv3dmodule use to initialize thein_channelsargument of theConv",
        "Y": "nn.LazyConvTranspose1d",
        "Z": "nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of thein_channelsargument of theConv3d that is inferred from theinput.",
        "Y": "nn.LazyConv3d a torch.nn.Conv3dmodule",
        "Z": "nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.Conv1dmodule with lazy initialization of what?",
        "Y": "thein_channelsargument of theConv1d",
        "Z": "a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.Conv2dmodule with what initialization of thein_channelsargument of theConv2",
        "Y": "lazy",
        "Z": "a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theCon",
        "Y": "nn.LazyConv3d",
        "Z": "Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is lazy initialized of of theConvTranspose1d that is inferred from theinput.size(1)?",
        "Y": "thein_channelsargument",
        "Z": "Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which a torch.nn.ConvTranspose2dmodule has lazy initialization of thein_channelsargument of",
        "Y": "nn.LazyConvTranspose2d",
        "Z": "nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which a torch.nn.ConvTranspose1dmodule has lazy initialization of thein_channelsargument of",
        "Y": "nn.LazyConvTranspose1d",
        "Z": "nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the module that has lazy initialization of thein_channelsargument of theConv2d?",
        "Y": "nn.LazyConvTranspose2d",
        "Z": "nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.Conv2dmodule with lazy initialization of what?",
        "Y": "thein_channelsargument of theConv2d",
        "Z": "a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.Conv3dmodule with what initialization of thein_channelsargument of theConv3",
        "Y": "lazy",
        "Z": "a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of thein_channelsargument of theConvTranspose1d that is inferred from the",
        "Y": "nn.LazyConvTranspose1d",
        "Z": "a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of thein_channelsargument of theConv2d that is inferred from theinput.",
        "Y": "nn.LazyConv2d a torch.nn.Conv2dmodule",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule",
        "Y": "thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1)",
        "Z": "Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which a torch.nn.Conv3dmodule has lazy initialization of thein_channelsargument of theConv",
        "Y": "nn.LazyConvTranspose3d",
        "Z": "nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the module that has lazy initialization of thein_channelsargument of theConv3d?",
        "Y": "nn.LazyConvTranspose3d",
        "Z": "nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.Conv3dmodule with lazy initialization of what?",
        "Y": "thein_channelsargument of theConv3d",
        "Z": "a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of initialization of thein_channelsargument of theConvTranspose1d that is inferred from thein",
        "Y": "lazy",
        "Z": "a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of thein_channelsargument of theConvTranspose2d that is inferred from the",
        "Y": "nn.LazyConvTranspose2d",
        "Z": "a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule",
        "Y": "nn.Unfold",
        "Z": "nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.ConvTranspose1dmodule with lazy initialization of what?",
        "Y": "thein_channelsargument of theConvTranspose1d",
        "Z": "a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of initialization of thein_channelsargument of theConvTranspose2d that is inferred from thein",
        "Y": "lazy",
        "Z": "a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of thein_channelsargument of theConvTranspose3d that is inferred from the",
        "Y": "nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule",
        "Z": "nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for lazy initialization of thein_channelsargument of theConvTranspose3d?",
        "Y": "nn.Unfold",
        "Z": "a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.Unfold extract sliding local blocks from?",
        "Y": "batched input tensor",
        "Z": "nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What extracts sliding local blocks from a batched input tensor?",
        "Y": "nn.Fold",
        "Z": "nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Combines an array of sliding local blocks into a large containing tensor?",
        "Y": "nn.Fold",
        "Z": "nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.MaxPool1d Applies what type of max pooling over an input signal composed of several input planes?",
        "Y": "1D",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of max pooling does nn.MaxPool2d apply over an input signal composed of several input planes?",
        "Y": "2D",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of max pooling does nn.MaxPool3d apply over an input signal composed of several input planes?",
        "Y": "3D",
        "Z": "nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.MaxPool1d apply over an input signal composed of several input planes?",
        "Y": "1D max pooling",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.MaxPool2d Applies over an input signal composed of several input planes?",
        "Y": "2D max pooling",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.MaxPool3d apply over an input signal composed of several input planes?",
        "Y": "3D max pooling",
        "Z": "nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Computes a partial inverse of MaxPool1d?",
        "Y": "nn.MaxUnpool1d",
        "Z": "nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Computes a partial inverse of MaxPool2d?",
        "Y": "nn.MaxUnpool2d",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Computes a partial inverse of MaxPool3d?",
        "Y": "nn.MaxUnpool3d",
        "Z": "Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "MaxPool2d Applies what type of max pooling over an input signal composed of several input planes?",
        "Y": "2D",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pooling does nn.MaxPool2d apply over an input signal composed of several input planes?",
        "Y": "1D max pooling",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.MaxPool3d Applies over an input signal composed of several input planes?",
        "Y": "3D max pooling",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Computes a partial inverse ofMaxPool2d?",
        "Y": "nn.MaxUnpool2d",
        "Z": "Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Computes a partial inverse ofMaxPool3d?",
        "Y": "nn.MaxUnpool3d",
        "Z": "Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the pooling algorithm that computes a partial inverse of MaxPool1d?",
        "Y": "nn.AvgPool1d",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.MaxPool2d Applies what type of max pooling over an input signal composed of several input planes?",
        "Y": "2D",
        "Z": "nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.AvgPool1d apply over an input signal composed of several input planes?",
        "Y": "1D average pooling",
        "Z": "Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies a 1D average pooling over an input signal composed of several input planes?",
        "Y": "nn.AvgPool1d",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.MaxPool2d apply over an input signal composed of several input planes?",
        "Y": "2D max pooling",
        "Z": "nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "MaxPool3d Applies what type of max pooling over an input signal composed of several input planes?",
        "Y": "3D",
        "Z": "nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pooling does nn.MaxPool3d apply over an input signal composed of several input planes?",
        "Y": "2D max pooling",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "MaxPool3d Applies what over an input signal composed of several input planes?",
        "Y": "3D max pooling",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.AvgPool1d Applies over an input signal composed of several input planes?",
        "Y": "1D average pooling",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pooling does nn.AvgPool1d apply over an input signal composed of several input planes?",
        "Y": "1D average pooling",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pooling does nn.AvgPool2d apply over an input signal composed of several input planes?",
        "Y": "2D average pooling",
        "Z": "nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies a 2D average pooling over an input signal composed of several input planes?",
        "Y": "nn.AvgPool2d",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies a 3D pooling over an input signal composed of several input planes?",
        "Y": "nn.AvgPool3d",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.AvgPool1d Applies what type of average pooling over an input signal composed of several input planes",
        "Y": "1D",
        "Z": "nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pooling does nn.AvgPool3d apply over an input signal composed of several input planes?",
        "Y": "2D average pooling",
        "Z": "Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.FractionalMaxPool2d do?",
        "Y": "nn.FractionalMaxPool2d",
        "Z": "nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What computes a partial inverse ofMaxPool1d?",
        "Y": "nn.AdaptiveMaxPool2d",
        "Z": "Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.MaxUnpool2d do?",
        "Y": "Computes a partial inverse ofMaxPool1d",
        "Z": "Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a partial inverse of MaxPool1d?",
        "Y": "nn.FractionalMaxPool2d",
        "Z": "Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What computes a partial inverse ofMaxPool3d?",
        "Y": "nn.AdaptiveMaxPool3d",
        "Z": "Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the pooling algorithm that computes a partial inverse of MaxPool2d?",
        "Y": "nn.FractionalMaxPool2d",
        "Z": "nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.AvgPool3d apply over an input signal composed of several input planes?",
        "Y": "3D average pooling",
        "Z": "nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.FractionalMaxPool2d apply over an input signal composed of several input planes?",
        "Y": "2D fractional max pooling",
        "Z": "nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Computes a partial what of MaxPool2d?",
        "Y": "inverse",
        "Z": "Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.MaxUnpool3d compute?",
        "Y": "partial inverse",
        "Z": "nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.AvgPool2d Apply over an input signal composed of several input planes?",
        "Y": "2D average pooling",
        "Z": "nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.AvgPool2d apply over an input signal composed of several input planes?",
        "Y": "2D average pooling",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.FractionalMaxPool3d apply over an input signal composed of several input planes?",
        "Y": "3D fractional max pooling",
        "Z": "nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the fractional max pooling over an input signal composed of several input planes?",
        "Y": "2D",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.LPPool1d stand for?",
        "Y": "nn.LPPool1d",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.LPPool1d apply over an input signal composed of several input planes?",
        "Y": "1D power-average pooling",
        "Z": "nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.AvgPool3d Applies over an input signal composed of several input planes?",
        "Y": "2D average pooling",
        "Z": "Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies a 3D average pooling over an input signal composed of several input planes?",
        "Y": "nn.AdaptiveMaxPool3d",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.FractionalMaxPool3d Applies over an input signal composed of several input planes?",
        "Y": "3D fractional max pooling",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.FractionalMaxPool2d Applies over an input signal composed of several input planes?",
        "Y": "2D fractional max pooling",
        "Z": "Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed",
        "Y": "several input planes",
        "Z": "Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies over an input signal composed of several input planes?",
        "Y": "3D average pooling",
        "Z": "Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.LPPool1d Apply over an input signal composed of several input planes?",
        "Y": "1D power-average pooling",
        "Z": "Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.LPPool2d Apply over an input signal composed of several input planes?",
        "Y": "2D power-average pooling",
        "Z": "nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for AdaptiveMaxPool1d?",
        "Y": "AdaptiveMaxPool1d",
        "Z": "nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.FractionalMaxPool3d Applies a fractional max pooling over an input signal composed of several",
        "Y": "3D",
        "Z": "Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of fractional max pooling does nn.FractionalMaxPool3d Applies?",
        "Y": "3D",
        "Z": "Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pooling does nn.LPPool1d apply?",
        "Y": "1D power-average",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes?",
        "Y": "nn",
        "Z": "Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.LPPool1d Applies what power-average pooling over an input signal composed of several input planes?",
        "Y": "1D",
        "Z": "nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for AdaptiveMaxPool3d?",
        "Y": "AdaptiveAvgPool1d",
        "Z": "nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of adaptive max pooling does AdaptiveMaxPool1d apply?",
        "Y": "1D",
        "Z": "Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of adaptive max pooling does AdaptiveMaxPool3d apply?",
        "Y": "3D",
        "Z": "Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveAvgPool1d Applies a 1D adaptive max pooling over an input signal composed of several input plane",
        "Y": "nn",
        "Z": "nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveMaxPool3d Applies what type of adaptive max pooling over an input signal composed of several input planes?",
        "Y": "3D",
        "Z": "Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How many input planes does AdaptiveAvgPool1d have?",
        "Y": "nn",
        "Z": "Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of power-average pooling is applied over an input signal composed of several input planes?",
        "Y": "2D",
        "Z": "Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveMaxPool2d Applies what type of adaptive max pooling over an input signal composed of several input planes?",
        "Y": "2D",
        "Z": "Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveAvgPool2d Applies a 2D adaptive max pooling over an input signal composed of several input plane",
        "Y": "nn",
        "Z": "nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input plane",
        "Y": "nn",
        "Z": "Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pooling does AdaptiveAvgPool1d apply?",
        "Y": "1D",
        "Z": "nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveAvgPool3d Applies a 3D adaptive max pooling over an input signal composed of several input plane",
        "Y": "nn",
        "Z": "nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does AdaptiveAvgPool1d Applies?",
        "Y": "1D adaptive average pooling over an input signal",
        "Z": "nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of adaptive average pooling does AdaptiveAvgPool1d apply?",
        "Y": "1D",
        "Z": "Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How many input planes does AdaptiveAvgPool2d have?",
        "Y": "nn",
        "Z": "Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does AdaptiveAvgPool1d Applies over an input signal composed of several input planes?",
        "Y": "1D adaptive average pooling",
        "Z": "Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pooling does AdaptiveAvgPool2d apply?",
        "Y": "2D",
        "Z": "nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does AdaptiveMaxPool3d apply over an input signal composed of several input planes?",
        "Y": "3D adaptive max pooling",
        "Z": "nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does AdaptiveAvgPool1d apply over an input signal composed of several input planes?",
        "Y": "1D adaptive average pooling",
        "Z": "nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "AdaptiveAvgPool2d Applies what type of adaptive average pooling over an input signal composed of several input planes",
        "Y": "2D",
        "Z": "nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ReplicationPad1d Pads the input tensor using?",
        "Y": "replication of the input boundary",
        "Z": "nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ReplicationPad3d do?",
        "Y": "replication of the input boundary",
        "Z": "nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ZeroPad2d Pads the input tensor boundaries with what value?",
        "Y": "zero",
        "Z": "nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Pads the input tensor boundaries with zero?",
        "Y": "nn.ConstantPad1d",
        "Z": "Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ReflectionPad1d Pads the input tensor using?",
        "Y": "reflection of the input boundary",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "ReflectionPad2d Pads the input tensor using what?",
        "Y": "reflection of the input boundary",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Pads the input tensor using replication of the input boundary?",
        "Y": "nn.ReplicationPad3d Pads",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is used to pad the input tensor?",
        "Y": "reflection of the input boundary",
        "Z": "Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does Pads the input tensor use?",
        "Y": "reflection of the input boundary",
        "Z": "Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "ReplicationPad1d Pads the input tensor using what?",
        "Y": "replication of the input boundary",
        "Z": "Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "ReplicationPad2d Pads the input tensor using what?",
        "Y": "replication of the input boundary",
        "Z": "nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "ReplicationPad3d Pads the input tensor using what?",
        "Y": "replication of the input boundary",
        "Z": "Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does ZeroPad2d pad the input tensor boundaries with?",
        "Y": "zero",
        "Z": "Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ReplicationPad2d Pads the input tensor use?",
        "Y": "replication of the input boundary",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConstantPad1d Pads the input tensor boundaries with what value?",
        "Y": "constant value",
        "Z": "nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ZeroPad2d pad the input tensor boundaries with?",
        "Y": "zero",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What value does ZeroPad2d pad the input tensor boundaries with?",
        "Y": "zero",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What value does constantPad1d Pads the input tensor boundaries with?",
        "Y": "constant",
        "Z": "Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which Pads the input tensor boundaries with a constant value?",
        "Y": "nn.ConstantPad3d",
        "Z": "nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What do constantPad1d Pads the input tensor boundaries with?",
        "Y": "constant value",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What do constantPad2d Pads the input tensor boundaries with?",
        "Y": "constant value",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Pads the input tensor using what?",
        "Y": "replication of the input boundary",
        "Z": "Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ReplicationPad2d use?",
        "Y": "replication of the input boundary",
        "Z": "Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the input tensor using replication of the input boundary?",
        "Y": "Pads",
        "Z": "Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What do constantPad3d Pads the input tensor boundaries with?",
        "Y": "constant value",
        "Z": "Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies the hard shrinkage function element-wise?",
        "Y": "nn.Hardshrink",
        "Z": "nn.Hardshrink Applies the hard shrinkage function element-wise: nn.Hardsigmoid Applies the element-wise function: nn.Hardtanh Applies the HardTanh function element-wise nn.Hardswish Applies the hardswish function, element-wise, as described in the paper: nn.LeakyReLU Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What function does Hardsigmoid apply element-wise?",
        "Y": "hard shrinkage",
        "Z": "Applies the hard shrinkage function element-wise: nn.Hardsigmoid Applies the element-wise function: nn.Hardtanh Applies the HardTanh function element-wise nn.Hardswish Applies the hardswish function, element-wise, as described in the paper: nn.LeakyReLU Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the hard shrinkage function element-wise?",
        "Y": "nn",
        "Z": "Applies the hard shrinkage function element-wise: nn.Hardsigmoid Applies the element-wise function: nn.Hardtanh Applies the HardTanh function element-wise nn.Hardswish Applies the hardswish function, element-wise, as described in the paper: nn.LeakyReLU Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend information from different representation subspace",
        "Y": "nn.LogSigmoid",
        "Z": "nn.Hardsigmoid Applies the element-wise function: nn.Hardtanh Applies the HardTanh function element-wise nn.Hardswish Applies the hardswish function, element-wise, as described in the paper: nn.LeakyReLU Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does ReLU Apply element-wise?",
        "Y": "rectified linear unit function",
        "Z": "nn.Hardsigmoid Applies the element-wise function: nn.Hardtanh Applies the HardTanh function element-wise nn.Hardswish Applies the hardswish function, element-wise, as described in the paper: nn.LeakyReLU Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "ReLU Applies what element-wise function?",
        "Y": "rectified linear unit function",
        "Z": "nn.Hardsigmoid Applies the element-wise function: nn.Hardtanh Applies the HardTanh function element-wise nn.Hardswish Applies the hardswish function, element-wise, as described in the paper: nn.LeakyReLU Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What element-wise function does nn.LeakyReLU Applies: nn.LogSigmoid Applies: n",
        "Y": "hardswish function",
        "Z": "Applies the hardswish function, element-wise, as described in the paper: nn.LeakyReLU Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation sub",
        "Y": "nn.LogSigmoid",
        "Z": "nn.LeakyReLU Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the randomized leaky rectified liner unit function?",
        "Y": "nn.SELU",
        "Z": "Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation sub",
        "Y": "nn.LogSigmoid",
        "Z": "Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the randomized leaky rectified liner unit function, element-wise, as described in the paper?",
        "Y": "nn.CELU",
        "Z": "Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What function does GELU apply?",
        "Y": "Gaussian Error Linear Units",
        "Z": "nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh Applies the element-wise function: nn.Tanhshrink Applies the element-wise function: nn.Threshold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does GELU apply?",
        "Y": "Gaussian Error Linear Units function",
        "Z": "nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How does the model attend to information from different representation subspaces?",
        "Y": "jointly",
        "Z": "Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Allows the model to jointly attend to what from different representation subspaces?",
        "Y": "information",
        "Z": "Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Who applies the randomized leaky rectified liner unit function, element-wise?",
        "Y": "nn.Mish",
        "Z": "Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Who applies the Sigmoid Linear Unit (SiLU) function, element-wise?",
        "Y": "nn.Mish",
        "Z": "Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies the randomized leaky rectified liner unit function, element-wise?",
        "Y": "nn.RReLU",
        "Z": "nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.RReLU Applies the randomized leaky rectified liner unit function, what?",
        "Y": "element-wise",
        "Z": "nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applied element-wise, as: nn.CELU Applies the element-wise function: nn.Sigmoid App",
        "Y": "nn.SELU",
        "Z": "nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh Applies the element-wise function:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Who Applies the element-wise function?",
        "Y": "nn.Sigmoid",
        "Z": "Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh Applies the element-wise function: nn.Tanhshrink Applies the element-wise function:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Applied element-wise, as: what?",
        "Y": "nn",
        "Z": "Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh Applies the element-wise function: nn.Tanhshrink",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the Gaussian Error Linear Units function?",
        "Y": "nn.Sigmoid",
        "Z": "Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh Applies the element-wise function: nn.Tanhshrink",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies the Gaussian Error Linear Units function?",
        "Y": "nn.GELU",
        "Z": "nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh Applies the element-wise function: nn.Tanhshrink Applies the element-wise function: nn.Threshold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies the Mish function, element-wise?",
        "Y": "Mish",
        "Z": "nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh Applies the element-wise function: nn.Tanhshrink Applies the element-wise function: nn.Threshold Thresholds each element of the input Tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What function does nn.Sigmoid Applies?",
        "Y": "Gaussian Error Linear Units",
        "Z": "Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh Applies the element-wise function: nn.Tanhshrink Applies the element-wise function: nn.Threshold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which Gaussian Error Linear Units function Applies the element-wise function?",
        "Y": "nn.Sigmoid",
        "Z": "Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh Applies the element-wise function: nn.Tanhshrink Applies the element-wise function: nn.Threshold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies Softmax over features to each spatial location?",
        "Y": "nn.Softmax2d",
        "Z": "nn.Softmax Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. nn.Softmax2d Applies SoftMax over features to each spatial location. nn.LogSoftmax Applies thelog\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor. nn.AdaptiveLogSoftmaxWithLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies SoftMax over features to each spatial location?",
        "Y": "Softmax2d",
        "Z": "nn.Softmin Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range[0, 1]and sum to 1. nn.Softmax Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. nn.Softmax2d Applies SoftMax over features to each spatial location. nn.LogSoftmax Applies thelog\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor. nn.AdaptiveLogSoftmaxWithLoss Efficient softmax approximation as described inEfficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies Softmax over features to each spatial location?",
        "Y": "nn.LogSoftmax",
        "Z": "nn.Softmin Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range[0, 1]and sum to 1. nn.Softmax Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. nn.Softmax2d Applies SoftMax over features to each spatial location. nn.LogSoftmax",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What function is applied to an n-dimensional input Tensor?",
        "Y": "Softmin",
        "Z": "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range[0, 1]and sum to 1. nn.Softmax Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. nn.Softmax2d Applies SoftMax over features to each spatial location. nn.LogSoftmax",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Applies what function to an n-dimensional input Tensor?",
        "Y": "Softmin",
        "Z": "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range[0, 1]and sum to 1. nn.Softmax Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. nn.Softmax2d Applies SoftMax over features to each spatial location. nn.LogSoftmax",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for Softmax2d?",
        "Y": "nn.LogSoftmax",
        "Z": "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range[0, 1]and sum to 1. nn.Softmax Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. nn.Softmax2d Applies SoftMax over features to each spatial location. nn.LogSoftmax",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Softmax2d Applies SoftMax to each spatial location?",
        "Y": "over features",
        "Z": "nn.Softmax2d Applies SoftMax over features to each spatial location. nn.LogSoftmax Applies thelog\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor. nn.AdaptiveLogSoftmaxWithLoss Efficient softmax approximation as described inEfficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the input of LogSoftmax?",
        "Y": "Tensor",
        "Z": "nn.Softmax2d Applies SoftMax over features to each spatial location. nn.LogSoftmax Applies thelog\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor. nn.AdaptiveLogSoftmaxWithLoss Efficient softmax approximation as described inEfficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Who wrote Efficient softmax approximation for GPUs?",
        "Y": "Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou",
        "Z": "nn.Softmin Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range[0, 1]and sum to 1. nn.Softmax Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. nn.Softmax2d Applies SoftMax over features to each spatial location. nn.LogSoftmax Applies thelog\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor. nn.AdaptiveLogSoftmaxWithLoss Efficient softmax approximation as described inEfficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What dimension is the input Tensor?",
        "Y": "n-dimensional",
        "Z": "nn.Softmax2d Applies SoftMax over features to each spatial location. nn.LogSoftmax Applies thelog\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor. nn.AdaptiveLogSoftmaxWithLoss Efficient softmax approximation as described inEfficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Along with Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and what other person, what",
        "Y": "Herv\u00e9 J\u00e9gou",
        "Z": "nn.Softmin Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range[0, 1]and sum to 1. nn.Softmax Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. nn.Softmax2d Applies SoftMax over features to each spatial location. nn.LogSoftmax Applies thelog\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor. nn.AdaptiveLogSoftmaxWithLoss Efficient softmax approximation as described inEfficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies Batch Normalization over a 4D input?",
        "Y": "nn.BatchNorm3d",
        "Z": "nn.BatchNorm1d Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a mini-batch of?",
        "Y": "1D inputs with optional additional channel dimension",
        "Z": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a 2D or 3D input?",
        "Y": "a mini-batch of 1D inputs with optional additional channel dimension",
        "Z": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies Batch Normalization over a 2D or 3D input?",
        "Y": "nn.BatchNorm3d",
        "Z": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What input does BatchNorm2d apply Batch Normalization over?",
        "Y": "4D",
        "Z": "nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What input does nn.BatchNorm3d apply Batch Normalization over?",
        "Y": "5D input",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies Batch Normalization over a 5D input?",
        "Y": "nn.GroupNorm",
        "Z": "nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.BatchNorm3d Applies Batch Normalization over what input?",
        "Y": "5D",
        "Z": "nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How many inputs does Batch Normalization apply over?",
        "Y": "4D",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How many inputs does nn.BatchNorm3d apply Batch Normalization over?",
        "Y": "5D",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the mini-batch of 2D inputs with additional channel dimension?",
        "Y": "4D input",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.BatchNorm3d Applies Batch Normalization over a what input?",
        "Y": "5D",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies Batch Normalization over a 5D input as described in the paperBatch Normalization: Accelerating Deep Network Training by Reduc",
        "Y": "nn.LazyBatchNorm1d",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of thenum_featuresargument of theBatchNorm1d inferred from?",
        "Y": "theinput.size(1)",
        "Z": "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the module that uses lazy initialization of thenum_featuresargument of theBatchNorm1d?",
        "Y": "nn.LazyBatchNorm2d",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for nn.LazyBatchNorm2d?",
        "Y": "nn.LazyBatchNorm2d",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What input is used for Batch Normalization?",
        "Y": "5D",
        "Z": "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is inferred from the number_featuresargument of theBatchNorm1d?",
        "Y": "theinput.size",
        "Z": "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the module with lazy initialization of thenum_featuresargument of theBatchNorm1d?",
        "Y": "nn.LazyBatchNorm2d",
        "Z": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What input does Batch Normalization apply over?",
        "Y": "5D",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of theBatchNorm1d inferred from?",
        "Y": "theinput.size(1)",
        "Z": "nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of theBatchNorm2d inferred from?",
        "Y": "theinput.size(1)",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where is the number_featuresargument of theBatchNorm3d inferred from?",
        "Y": "theinput.size",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which module has lazy initialization of thenum_featuresargument of theBatchNorm3d that is inferred from theinput",
        "Y": "nn.GroupNorm",
        "Z": "nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the module that has lazy initialization of thenum_featuresargument of theBatchNorm3d?",
        "Y": "nn.GroupNorm",
        "Z": "a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.BatchNorm1dmodule with lazy initialization of what?",
        "Y": "thenum_featuresargument of theBatchNorm1d",
        "Z": "a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of thenum_featuresargument of theBatchNorm2d that is inferred from theinput",
        "Y": "nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargu",
        "Y": "nn.LazyBatchNorm3d",
        "Z": "a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of thenum_featuresargument of theBatchNorm2d inferred from?",
        "Y": "theinput.size(1)",
        "Z": "nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the number of featuresargument of theBatchNorm3d inferred from?",
        "Y": "theinput.size",
        "Z": "nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization?",
        "Y": "nn.GroupNorm",
        "Z": "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does GroupNorm Applies over a mini-batch of inputs as described in the paperGroup Normalization?",
        "Y": "Group Normalization",
        "Z": "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.BatchNorm2dmodule with lazy initialization of what?",
        "Y": "thenum_featuresargument of theBatchNorm2d",
        "Z": "a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.BatchNorm3dmodule with lazy initialization of what?",
        "Y": "thenum_featuresargument of theBatchNorm3d",
        "Z": "a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is inferred from theBatchNorm3d?",
        "Y": "theinput.size",
        "Z": "nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does GroupNorm apply Group Normalization over?",
        "Y": "a mini-batch of inputs",
        "Z": "a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the paperBatch Normalization?",
        "Y": "Accelerating Deep Network Training",
        "Z": "nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the class that applies Batch Normalization over a N-Dimensional input?",
        "Y": "nn.InstanceNorm1d",
        "Z": "Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the paperInstance Normalization?",
        "Y": "The Missing Ingredient for Fast Stylization",
        "Z": "Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies Instance Normalization over a 3D input?",
        "Y": "nn.InstanceNorm1d",
        "Z": "nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "InstanceNorm1d Applies Instance Normalization over what input?",
        "Y": "3D",
        "Z": "Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies Instance Normalization over a 3D input?",
        "Y": "nn.InstanceNorm2d",
        "Z": "nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What input does nn.InstanceNorm1d apply Instance Normalization over?",
        "Y": "3D input",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies Instance Normalization over a 3D input as described in the paperInstance Normalization: The Missing Ingredient for Fast Sty",
        "Y": "nn.InstanceNorm2d",
        "Z": "Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.InstanceNorm1d apply Instance Normalization over?",
        "Y": "a 3D input",
        "Z": "nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies Instance Normalization over a 4D input?",
        "Y": "nn.InstanceNorm3d",
        "Z": "Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "InstanceNorm2d Applies Instance Normalization over what input?",
        "Y": "4D",
        "Z": "nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies Instance Normalization over a 4D input?",
        "Y": "nn.InstanceNorm3d",
        "Z": "nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of input does nn.InstanceNorm2d apply Instance Normalization over?",
        "Y": "4D",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is an example of a mini-batch of 1D inputs with optional additional channel dimension?",
        "Y": "3D input",
        "Z": "nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies Instance Normalization over a 5D input?",
        "Y": "nn.LayerNorm",
        "Z": "Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "InstanceNorm3d Applies Instance Normalization over what input?",
        "Y": "5D",
        "Z": "nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.LayerNorm Applies Instance Normalization over a 5D input?",
        "Y": "nn.LayerNorm",
        "Z": "nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How many inputs does nn.LayerNorm apply Instance Normalization over?",
        "Y": "4D",
        "Z": "Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.LayerNorm Apply over a mini-batch of inputs?",
        "Y": "Layer Normalization",
        "Z": "nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "InstanceNorm3d Applies Instance Normalization over a 5D input as described in what paper?",
        "Y": "Instance Normalization: The Missing Ingredient for Fast Stylization",
        "Z": "nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does LayerNorm apply over a mini-batch of inputs?",
        "Y": "Layer Normalization",
        "Z": "nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Applies a multi-layer Elman RNN withtanhtanhtanhorReLUtextReLU",
        "Y": "nn.RNNBase",
        "Z": "nn.RNNBase  nn.RNN Applies a multi-layer Elman RNN withtanh\u2061\\tanhtanhorReLU\\text{ReLU}ReLUnon-linearity to an input sequence. nn.LSTM Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence. nn.GRU Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. nn.RNNCell An Elman RNN cell with tanh or ReLU non-linearity. nn.LSTMCell A long short-term memory (LSTM) cell. nn.GRUCell A gated recurrent unit (GRU) cell",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies a multi-layer gated recurrent unit to an input sequence?",
        "Y": "nn.GRU",
        "Z": "nn.RNNBase  nn.RNN Applies a multi-layer Elman RNN withtanh\u2061\\tanhtanhorReLU\\text{ReLU}ReLUnon-linearity to an input sequence. nn.LSTM Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence. nn.GRU Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. nn.RNNCell An Elman RNN cell with tanh or ReLU non-linearity. nn.LSTMCell A long short-term memory (LSTM) cell. nn.GRUCell A gated recurrent unit (GRU) cell",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does an Elman RNN cell have?",
        "Y": "tanh or ReLU non-linearity",
        "Z": "nn.RNNBase  nn.RNN Applies a multi-layer Elman RNN withtanh\u2061\\tanhtanhorReLU\\text{ReLU}ReLUnon-linearity to an input sequence. nn.LSTM Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence. nn.GRU Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. nn.RNNCell An Elman RNN cell with tanh or ReLU non-linearity. nn.LSTMCell A long short-term memory (LSTM) cell. nn.GRUCell A gated recurrent unit (GRU) cell",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does GRU stand for?",
        "Y": "gated recurrent unit",
        "Z": "nn.RNNBase  nn.RNN Applies a multi-layer Elman RNN withtanh\u2061\\tanhtanhorReLU\\text{ReLU}ReLUnon-linearity to an input sequence. nn.LSTM Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence. nn.GRU Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. nn.RNNCell An Elman RNN cell with tanh or ReLU non-linearity. nn.LSTMCell A long short-term memory (LSTM) cell. nn.GRUCell A gated recurrent unit (GRU) cell",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a long short-term memory cell?",
        "Y": "nn.LSTMCell",
        "Z": "nn.RNNBase  nn.RNN Applies a multi-layer Elman RNN withtanh\u2061\\tanhtanhorReLU\\text{ReLU}ReLUnon-linearity to an input sequence. nn.LSTM Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence. nn.GRU Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. nn.RNNCell An Elman RNN cell with tanh or ReLU non-linearity. nn.LSTMCell A long short-term memory (LSTM) cell. nn.GRUCell A gated recurrent unit (GRU) cell",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of model is nn.Transformer?",
        "Y": "transformer",
        "Z": "nn.Transformer A transformer model. nn.TransformerEncoder TransformerEncoder is a stack of N encoder layers nn.TransformerDecoder TransformerDecoder is a stack of N decoder layers nn.TransformerEncoderLayer TransformerEncoderLayer is made up of self-attn and feedforward network. nn.TransformerDecoderLayer TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Is the placeholder identity operator argument sensitive or argument sensitive?",
        "Y": "argument-insensitive",
        "Z": "nn.Identity A placeholder identity operator that is argument-insensitive. nn.Linear Applies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b nn.Bilinear Applies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b nn.LazyLinear a torch.nn.Linearmodule wherein_featuresis inferred.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transformation does nn.Linear apply to the incoming data?",
        "Y": "linear",
        "Z": "nn.Identity A placeholder identity operator that is argument-insensitive. nn.Linear Applies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b nn.Bilinear Applies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b nn.LazyLinear a torch.nn.Linearmodule wherein_featuresis inferred.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Identity A placeholder identity operator that is what?",
        "Y": "argument-insensitive",
        "Z": "nn.Identity A placeholder identity operator that is argument-insensitive. nn.Linear Applies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b nn.Bilinear Applies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b nn.LazyLinear a torch.nn.Linearmodule wherein_featuresis inferred.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the value of xAT + by=xAT+b nn.Linear Applies a linear transformation to the",
        "Y": "xAT+by",
        "Z": "nn.Identity A placeholder identity operator that is argument-insensitive. nn.Linear Applies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b nn.Bilinear Applies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b nn.LazyLinear a torch.nn.Linearmodule wherein_featuresis inferred.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "When does nn.Dropout randomly zeroes some of the elements of the input tensor?",
        "Y": "During training",
        "Z": "nn.Dropout During training, randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution. nn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 2D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.Dropout3d Randomly zero out entire channels (a channel is a 3D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 3D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.AlphaDropout Applies Alpha Dropout over the input.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.Dropout2d do?",
        "Y": "nn.Dropout3d",
        "Z": "During training, randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution. nn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 2D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.Dropout3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution?",
        "Y": "nn.Dropout",
        "Z": "nn.Dropout During training, randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution. nn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 2D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.Dropout3d Randomly zero out entire channels (a channel is a 3D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 3D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.AlphaDropout Applies Alpha Dropout over the input.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Randomly zero out entire channels?",
        "Y": "nn.Dropout3d",
        "Z": "nn.Dropout During training, randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution. nn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 2D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.Dropout3d Randomly zero out entire channels (a channel is a 3D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 3D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.AlphaDropout Applies Alpha Dropout over the input.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for nn.Dropout2d?",
        "Y": "nn.Dropout3d",
        "Z": "nn.Dropout During training, randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution. nn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 2D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.Dropout3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What program randomly zeros out entire channels?",
        "Y": "nn.Dropout2d",
        "Z": "During training, randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution. nn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 2D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.Dropout3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the program that randomly zeros out entire channels?",
        "Y": "nn.Dropout3d",
        "Z": "During training, randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution. nn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 2D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.Dropout3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a channel in Dropout3d?",
        "Y": "3D feature map",
        "Z": "nn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 2D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.Dropout3d Randomly zero out entire channels (a channel is a 3D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 3D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.AlphaDropout Applies Alpha Dropout over the input.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Dropout3d Randomly zero out entire channels (a channel is a what?",
        "Y": "3D feature map",
        "Z": "nn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 2D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.Dropout3d Randomly zero out entire channels (a channel is a 3D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 3D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.AlphaDropout Applies Alpha Dropout over the input.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does AlphaDropout Applies over the input?",
        "Y": "Alpha Dropout",
        "Z": "nn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 2D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.Dropout3d Randomly zero out entire channels (a channel is a 3D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 3D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.AlphaDropout Applies Alpha Dropout over the input.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the simple lookup table that stores embeddings of a fixed dictionary and size?",
        "Y": "nn",
        "Z": "nn.Embedding A simple lookup table that stores embeddings of a fixed dictionary and size. nn.EmbeddingBag Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.EmbeddingBag Computes sums or means of \u2018bags\u2019 of embeddings, without what?",
        "Y": "instantiating the intermediate embeddings",
        "Z": "nn.Embedding A simple lookup table that stores embeddings of a fixed dictionary and size. nn.EmbeddingBag Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.EmbeddingBag compute?",
        "Y": "means of \u2018bags\u2019",
        "Z": "nn.Embedding A simple lookup table that stores embeddings of a fixed dictionary and size. nn.EmbeddingBag Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.CosineSimilarity Returns cosine similarity betweenx1x_1x1 andx2x",
        "Y": "dim",
        "Z": "nn.CosineSimilarity Returns cosine similarity betweenx1x_1x1\u200bandx2x_2x2\u200b, computed along dim. nn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is used to calculate the pairwise distance between vectorsv1v_1v1,v2v_2v2?",
        "Y": "p-norm",
        "Z": "nn.CosineSimilarity Returns cosine similarity betweenx1x_1x1\u200bandx2x_2x2\u200b, computed along dim. nn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the cosine similarity betweenx1x_1x1 andx2x_2x2?",
        "Y": "nn",
        "Z": "nn.CosineSimilarity Returns cosine similarity betweenx1x_1x1\u200bandx2x_2x2\u200b, computed along dim. nn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is used to compute the batchwise pairwise distance between vectorsv1v_1v1,v2v_2v2?",
        "Y": "p-norm",
        "Z": "nn.CosineSimilarity Returns cosine similarity betweenx1x_1x1\u200bandx2x_2x2\u200b, computed along dim. nn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the mean squared error?",
        "Y": "squared L2 norm",
        "Z": "Creates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy. nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is NLLLoss?",
        "Y": "negative log likelihood loss",
        "Z": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the criterion that measures the mean absolute error between each element in the inputxxxand targetyyy?",
        "Y": "MAE",
        "Z": "Creates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy. nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What criterion measures the mean absolute error between each element in the inputxxxand targetyyy?",
        "Y": "nn.PoissonNLLLoss",
        "Z": "Creates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy. nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is NLLLoss Negative log likelihood loss with Poisson distribution of target?",
        "Y": "Poisson",
        "Z": "The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of NLLLoss Gaussian negative log likelihood loss?",
        "Y": "Gaussian",
        "Z": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the criterion that measures the mean squared error between each element in the inputxxxand targetyyy?",
        "Y": "nn.KLDivLoss",
        "Z": "nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a criterion that measures the mean squared error?",
        "Y": "squared L2 norm",
        "Z": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What criterion measures the mean squared error between each element in the inputxxxand targetyyy?",
        "Y": "nn.KLDivLoss",
        "Z": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is NLLLoss a criterion that measures the Binary Cross Entropy between the target and the output?",
        "Y": "negative log likelihood loss",
        "Z": "nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output:",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does BCELoss measure between the target and the output?",
        "Y": "Binary Cross Entropy",
        "Z": "nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of loss is nn.NLLLoss?",
        "Y": "negative log likelihood loss",
        "Z": "nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does PoissonNLLLoss Negative log likelihood loss have?",
        "Y": "Poisson distribution of target",
        "Z": "The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the Kullback-Leibler divergence loss measure?",
        "Y": "nn.KLDivLoss",
        "Z": "Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the loss?",
        "Y": "Connectionist Temporal Classification loss",
        "Z": "The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the loss that combines aSigmoidlayer and theBCELossin?",
        "Y": "nn.MarginRankingLoss",
        "Z": "The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What loss measures the Binary Cross Entropy between the target and the output?",
        "Y": "MarginRankingLoss",
        "Z": "Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the loss that measures the Binary Cross Entropy between the target and the output?",
        "Y": "nn.MarginRankingLoss",
        "Z": "The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the negative log likelihood loss with Poisson distribution of target?",
        "Y": "Poisson",
        "Z": "nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of loss is NLLLoss?",
        "Y": "Gaussian",
        "Z": "nn.L1Loss Creates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy. nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the loss with Poisson distribution of target?",
        "Y": "negative log likelihood loss",
        "Z": "The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the loss measure that measures the Binary Cross Entropy between the target and the output?",
        "Y": "nn.MarginRankingLoss",
        "Z": "Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the Negative log likelihood loss with?",
        "Y": "Poisson distribution of target",
        "Z": "nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What loss is associated with Poisson distribution of target?",
        "Y": "Negative log likelihood loss",
        "Z": "Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What measure measures the Binary Cross Entropy between the target and the output?",
        "Y": "nn.KLDivLoss",
        "Z": "Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What label contains 1 or - 1?",
        "Y": "1D mini-batch tensoryy",
        "Z": "nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the criterion that measures the loss given inputs?",
        "Y": "nn.HingeEmbeddingLoss",
        "Z": "nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is Gaussian negative log likelihood loss?",
        "Y": "Gaussian negative log likelihood loss",
        "Z": "Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion that measures the loss given inputsx1x1x1,x2x2x2?",
        "Y": "nn.MarginRankingLoss",
        "Z": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the criterion that measures the loss given inputsx1x1x1,x2x2x2?",
        "Y": "nn.HingeEmbeddingLoss",
        "Z": "nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of negative log likelihood loss is used?",
        "Y": "Gaussian",
        "Z": "Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What two classes does BCEWithLogitsLoss combine?",
        "Y": "aSigmoidlayer and theBCELossin",
        "Z": "nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the criterion that measures the Binary Cross Entropy between the target and the output?",
        "Y": "nn.BCEWithLogitsLoss",
        "Z": "The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the criterion that measures the loss given inputs?",
        "Y": "nn.HingeEmbeddingLoss",
        "Z": "The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What class measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensor",
        "Y": "MultiLabelMarginLoss",
        "Z": "This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a label?",
        "Y": "1D mini-batch tensoryy",
        "Z": "nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels",
        "Y": "tensoryyy",
        "Z": "nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What class measures the loss given an input tensorxxxand a labels tensoryyy?",
        "Y": "MultiLabelMarginLoss",
        "Z": "nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does a label 1D mini-batch tensoryyy contain?",
        "Y": "1 or -1)",
        "Z": "Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -",
        "Y": "nn.HingeEmbeddingLoss",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the criterion that measures the Binary Cross Entropy between the target and the output?",
        "Y": "nn.BCEWithLogitsLoss",
        "Z": "nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the loss given an input tensorxxxand a labels tensoryyy?",
        "Y": "MultiLabelMarginLoss",
        "Z": "Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the class that measures the loss given inputsx1x1x1,x2x2x2, and a labels",
        "Y": "MultiLabelMarginLoss",
        "Z": "nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the loss given an input tensorxxand a labels tensoryyy?",
        "Y": "MultiLabelMarginLoss",
        "Z": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What two classes does this loss combine?",
        "Y": "aSigmoidlayer and theBCELossin",
        "Z": "This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a label that measures the loss given inputsx1x1x1,x2x2x2, and two 1D mini-b",
        "Y": "1D mini-batch tensoryy",
        "Z": "This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What measure the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1)",
        "Y": "nn.HingeEmbeddingLoss",
        "Z": "nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does MultiLabelMarginLoss optimize?",
        "Y": "multi-class multi-classification hinge loss",
        "Z": "Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is inputxxx?",
        "Y": "a 2D mini-batchTensor",
        "Z": "nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the hinge loss that uses a squared term if the absolute element-wise error falls below delta?",
        "Y": "SmoothL1Loss",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a criterion that uses a squared term if the absolute element-wise error falls below delta?",
        "Y": "SmoothL1Loss",
        "Z": "Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion that optimizes a multi-class multi-classification hinge loss?",
        "Y": "nn.MultiLabelMarginLoss",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What criterion uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1",
        "Y": "SmoothL1Loss",
        "Z": "nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does SmoothL1Loss use if the absolute element-wise error falls below beta?",
        "Y": "a squared term",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the criterion that optimizes a multi-class multi-classification hinge loss?",
        "Y": "nn.SoftMarginLoss",
        "Z": "Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What optimizes a multi-class multi-classification hinge loss?",
        "Y": "criterion",
        "Z": "Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.HuberLoss create a criterion that uses a squared term?",
        "Y": "if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise",
        "Z": "Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1",
        "Y": "nn.SmoothL1Loss",
        "Z": "Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.SoftMarginLoss optimize between input tensorxxxand target tensoryy",
        "Y": "a two-class classification logistic loss",
        "Z": "nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the criterion that optimizes a two-class classification logistic loss between input tensorxxxand target",
        "Y": "nn",
        "Z": "nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "If the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise, what is a squared term?",
        "Y": "if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise",
        "Z": "nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "When does SmoothL1Loss use a squared term?",
        "Y": "if the absolute element-wise error falls below beta and an L1 term otherwise",
        "Z": "nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target",
        "Y": "nn.MultiLabelSoftMarginLoss",
        "Z": "nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target ten",
        "Y": "MultiLabelSoftMarginLoss",
        "Z": "Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "When does SmoothL1Loss create a criterion that uses a squared term?",
        "Y": "if the absolute element-wise error falls below beta and an L1 term otherwise",
        "Z": "This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does SoftMarginLoss optimize?",
        "Y": "a two-class classification logistic loss",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does MultiLabelSoftMarginLoss optimize?",
        "Y": "multi-label one-versus-all loss based on max-entropy",
        "Z": "nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the criterion that optimizes a multi-label one-versus-all loss?",
        "Y": "nn.CosineEmbeddingLoss",
        "Z": "nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What term does SmoothL1Loss use if the absolute element-wise error falls below beta?",
        "Y": "squared",
        "Z": "nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of loss does MultiLabelSoftMarginLoss optimize?",
        "Y": "multi-label one-versus-all loss based on max-entropy",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the criterion that optimizes a multi-label one-versus-all loss based on max-en",
        "Y": "nn.CosineEmbeddingLoss",
        "Z": "Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "When does a criterion use a squared term?",
        "Y": "if the absolute element-wise error falls below beta",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which criterion optimizes a multi-label one-versus-all loss based on max-entropy?",
        "Y": "nn.CosineEmbeddingLoss",
        "Z": "Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What term is used if the absolute element-wise error falls below beta?",
        "Y": "squared term",
        "Z": "Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the criterion that uses a squared term if the absolute element-wise error falls below beta?",
        "Y": "nn",
        "Z": "Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion that measures the loss given input tensorsx1x_1x1,x2",
        "Y": "nn.CosineEmbeddingLoss",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What criterion measures the loss given input tensorsx1x_1x1,x2x_2x2",
        "Y": "MultiMarginLoss",
        "Z": "Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the criterion that optimizes between input tensorxxxand target tensoryyy?",
        "Y": "a two-class classification logistic loss",
        "Z": "Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy?",
        "Y": "criterion",
        "Z": "Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion that optimizes a multi-label one-versus-all loss based on max-entrop",
        "Y": "nn.MultiLabelSoftMarginLoss",
        "Z": "This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the criterion that optimizes a multi-label one-versus-all loss based on max-entropy?",
        "Y": "nn",
        "Z": "nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a criterion that measures the loss given input tensorsx1x_1x1,x2x",
        "Y": "aTensorlabelyyy",
        "Z": "nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the criterion that optimizes a multi-class classification hinge loss?",
        "Y": "TripletMarginLoss",
        "Z": "nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion that optimizes a multi-class classification hinge loss?",
        "Y": "MultiMarginLoss",
        "Z": "Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the criterion that measures the triplet loss given an input tensor with a value greater than000?",
        "Y": "TripletMarginWithDistanceLoss",
        "Z": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does MultiMarginLoss optimize?",
        "Y": "multi-class classification hinge loss",
        "Z": "nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a criterion that optimizes between inputxxx and outputyyy?",
        "Y": "a multi-class classification hinge loss",
        "Z": "Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What criterion measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x",
        "Y": "TripletMarginWithDistanceLoss",
        "Z": "Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.TripletMarginLoss create?",
        "Y": "a criterion",
        "Z": "Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does the criterion measure?",
        "Y": "triplet loss",
        "Z": "Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the criterion that measures the triplet loss?",
        "Y": "TripletMarginWithDistanceLoss",
        "Z": "Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the criterion that measures the triplet loss given an input tensor?",
        "Y": "nn.TripletMarginWithDistanceLoss",
        "Z": "Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a downscale factor?",
        "Y": "Upsample",
        "Z": "nn.PixelShuffle Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is an upscale factor. nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.PixelUnshuffle do to reverse thePixelShuffleoperation?",
        "Y": "rearranging elements in a tensor of shape",
        "Z": "nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does UpsamplingNearest2d apply to an input signal composed of several input channels?",
        "Y": "2D nearest neighbor",
        "Z": "nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies a 2D nearest neighbor upsampling to an input signal composed of several input channels?",
        "Y": "nn.UpsamplingBilinear2d",
        "Z": "Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.PixelUnshuffle Reve what ses thePixelShuffleoperation?",
        "Y": "r",
        "Z": "nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric",
        "Y": "nn.Upsample",
        "Z": "nn.PixelShuffle Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is an upscale factor. nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d Applies a 2D bilinear upsampling to an input signal composed of several input channels.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "UpsamplingNearest2d Applies a 2D what to an input signal composed of several input channels?",
        "Y": "nearest neighbor",
        "Z": "nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of upsampling does nn.UpsamplingNearest2d do?",
        "Y": "Bilinear2d",
        "Z": "nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.UpsamplingNearest2d Applies what type of upsampling to an input signal composed of several input channels",
        "Y": "2D nearest neighbor",
        "Z": "Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the tensor of shape(,C,Hr,Wr)(*, C, H time",
        "Y": "r",
        "Z": "Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "UpsamplingNearest2d Applies a 2D upsampling to an input signal composed of several input channels?",
        "Y": "nearest neighbor",
        "Z": "Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of upsampling does UpsamplingNearest2d do?",
        "Y": "Bilinear2d",
        "Z": "Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.UpsamplingNearest2d apply to an input signal composed of several input channels?",
        "Y": "2D nearest neighbor",
        "Z": "nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d Applies a 2D bilinear upsampling to an input signal composed of several input channels.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of upsampling does nn.UpsamplingBilinear2d apply to an input signal composed of several input",
        "Y": "bilinear",
        "Z": "nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d Applies a 2D bilinear upsampling to an input signal composed of several input channels.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "UpsamplingNearest2d Applies a 2D upsampling to an input signal composed of several input channels.",
        "Y": "nearest neighbor",
        "Z": "nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d Applies a 2D bilinear upsampling to an input signal composed of several input channels.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "UpsamplingBilinear2d Applies a 2D what upsampling to an input signal composed of several input channels?",
        "Y": "bilinear",
        "Z": "nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d Applies a 2D bilinear upsampling to an input signal composed of several input channels.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.DataParallel Implements data parallelism at what level?",
        "Y": "module level",
        "Z": "nn.DataParallel Implements data parallelism at the module level. nn.parallel.DistributedDataParallel Implements distributed data parallelism that is based ontorch.distributedpackage at the module level.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.parallel.DistributedDataParallel Implements distributed data parallelism that is based on what?",
        "Y": "ontorch.distributedpackage",
        "Z": "nn.DataParallel Implements data parallelism at the module level. nn.parallel.DistributedDataParallel Implements distributed data parallelism that is based ontorch.distributedpackage at the module level.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What implements data parallelism at the module level?",
        "Y": "nn.DataParallel",
        "Z": "nn.DataParallel Implements data parallelism at the module level. nn.parallel.DistributedDataParallel Implements distributed data parallelism that is based ontorch.distributedpackage at the module level.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the norm of an iterable of parameters?",
        "Y": "Clips gradient",
        "Z": "Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Clips gradient of an iterable of parameters at what value?",
        "Y": "specified value",
        "Z": "Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How many vectors are prune.BasePruningMethod Abstract base class for creation of new pruning techniques?",
        "Y": "one vector",
        "Z": "From thetorch.nn.utilsmodule   Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.PruningContainer Container holding a sequence of pruning methods for?",
        "Y": "iterative pruning",
        "Z": "prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the container holding a sequence of pruning methods for iterative pruning?",
        "Y": "prune.remove",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of an iterable of parameters?",
        "Y": "Clips gradient norm",
        "Z": "Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the function that converts parameters to one vector?",
        "Y": "Convert parameters to one vector",
        "Z": "Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pruning is prune.PruningContainer Container holding a sequence of pruning methods for?",
        "Y": "iterative pruning",
        "Z": "Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.Identity generate the pruning parametrization with?",
        "Y": "mask of ones",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How many vectors can parameters be converted to?",
        "Y": "one",
        "Z": "Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What utility pruning method does not prune any units but generates the pruning parametrization with a mask of ones?",
        "Y": "prune.Identity",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the gradient of an iterable of parameters at specified value?",
        "Y": "Clips gradient",
        "Z": "From thetorch.nn.utilsmodule   Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the utility pruning method that does not prune any units?",
        "Y": "does not prune any units",
        "Z": "Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the pruning method that does not prune any units but generates the pruning parametrization with a mask of ones?",
        "Y": "Utility",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is currently unpruned units in a tensor at random?",
        "Y": "RandomUnstructured Prune",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pruning method prunes units in a tensor at random?",
        "Y": "L1Unstructured",
        "Z": "Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How many vectors does prune.BasePruningMethod convert to the parameters?",
        "Y": "one",
        "Z": "Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does the Utility pruning method do?",
        "Y": "does not prune any units",
        "Z": "Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the pruning method that prunes units in a tensor at random?",
        "Y": "L1Unstructured",
        "Z": "Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.BasePruningMethod Abstract?",
        "Y": "base class",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pruning is prune.PruningContainer Container used for?",
        "Y": "iterative pruning",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does the prune.Identity Utility pruning method generate the pruning parametrization with?",
        "Y": "mask of ones",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "In what direction are randomUnstructured Prune units located?",
        "Y": "tensor",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How do prune.L1Unstructured Prune (currently unpruned) units in a tensor?",
        "Y": "zeroing out the ones with the lowest L1-norm",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a base class for creation of?",
        "Y": "pruning techniques",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.Identity Utility pruning method do?",
        "Y": "does not prune any units",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How do prune.L1Unstructured Prune (currently unpruned) units in a tensor at random?",
        "Y": "by zeroing out the ones with the lowest L1-norm",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Abstract base class for creation of new what?",
        "Y": "pruning techniques",
        "Z": "Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lowest L1-norm in a tensor?",
        "Y": "L1",
        "Z": "prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the pruning method that prunes units in a tensor at random?",
        "Y": "RandomStructured",
        "Z": "Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where are the unpruned units in the prune.RandomUnstructured Prune located?",
        "Y": "a tensor at random",
        "Z": "prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where are the channels currently unpruned in a prune.RandomStructured Prune?",
        "Y": "tensor at random",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the term for channels in a tensor at random?",
        "Y": "RandomStructured Prune",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does the identification utility pruning method generate the pruning parametrization with?",
        "Y": "mask of ones",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where are RandomUnstructured Prune currently unpruned units located?",
        "Y": "a tensor at random",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How do you prune L1Unstructured Prune units in a tensor?",
        "Y": "zeroing out the ones with the lowest L1-norm",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a random structure in a tensor?",
        "Y": "entire (currently unpruned) channels",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the pruning method that does not prune any units but generates the pruning parametrization with a mask of ones?",
        "Y": "CustomFromMask prune.identity",
        "Z": "Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pruning is a container holding a sequence of pruning methods for?",
        "Y": "iterative pruning",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a Utility pruning method that generates the pruning parametrization with a mask of ones?",
        "Y": "does not prune any units",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is currently unpruned units in a tensor by zeroing out the ones with the lowest L1-norm",
        "Y": "prune.L1Unstructured Prune",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the term for RandomStructured Prune entire (currently unpruned) channels in a tensor",
        "Y": "LnStructured",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where are the unpruned units in prune.RandomUnstructured Prune located?",
        "Y": "a tensor at random",
        "Z": "Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.RandomStructured Prune?",
        "Y": "entire (currently unpruned) channels",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pruning method does prune?",
        "Y": "random",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does the identification utility pruning method generate with a mask of ones?",
        "Y": "pruning parametrization",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the term for units in a tensor that is currently unpruned?",
        "Y": "L1Unstructured Prune",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pruning method is used to prune entire channels in a tensor at random?",
        "Y": "LnStructured",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What method does not prune any units but generates the pruning parametrization with a mask of ones?",
        "Y": "pruning",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of unit is RandomUnstructured Prune currently?",
        "Y": "unpruned",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "In what tensor does RandomStructured Prune channels?",
        "Y": "tensor at random",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the LnStructured Prune based on?",
        "Y": "Ln-norm",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the term for unpruned units in a tensor?",
        "Y": "L1Unstructured Prune",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the pruning parametrization with a mask of ones?",
        "Y": "RandomStructured Prune",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the structure that prunes channels in a tensor based on their Ln-norm?",
        "Y": "Ln",
        "Z": "Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the current status of RandomUnstructured Prune units in a tensor?",
        "Y": "unpruned",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How can prune.L1Unstructured Prune units in a tensor be unpruned?",
        "Y": "zeroing out the ones with the lowest L1-norm",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where are the channels in a prune.RandomStructured Prune located?",
        "Y": "tensor at random",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are the channels in a tensor based on?",
        "Y": "Ln-norm",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the program that prune uses?",
        "Y": "CustomFromMask prune.identity",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are currently unpruned units in a tensor at random?",
        "Y": "Prune",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How do prune.L1Unstructured Prune units in a tensor?",
        "Y": "by zeroing out the ones with the lowest L1-norm",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where are unpruned channels in a tensor?",
        "Y": "a tensor at random",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What do LnStructured Prune channels in a tensor based on?",
        "Y": "Ln-norm",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the command that prune uses?",
        "Y": "CustomFromMask prune.identity",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "At what rate do prune.RandomStructured Prune channels in a tensor?",
        "Y": "random",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.LnStructured Prune channels in a tensor based on?",
        "Y": "Ln-norm",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units?",
        "Y": "prune.CustomFromMask prune.identity",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is _unstructured?",
        "Y": "l1",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "At what rate are channels in a tensor prune.RandomStructured Prune?",
        "Y": "random",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of Structured Prune channels in a tensor based on their Ln-norm?",
        "Y": "Ln",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "CustomFromMask prune.identity Applies what to the tensor corresponding to the parameter callednameinmodule?",
        "Y": "pruning reparametrization",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pruning reparametrization does prune.identity Applies pruning reparametrization to the tensor corresponding to",
        "Y": "random_unstructured",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How do you prune unpruned units in a tensor?",
        "Y": "zeroing out the ones with the lowest L1-norm",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "At what rate do prune.RandomStructured prune entire channels in a tensor?",
        "Y": "random",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the LnStructured Prune channels in a tensor based on?",
        "Y": "Ln-norm",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodule without actually pruning any units?",
        "Y": "CustomFromMask prune.identity",
        "Z": "Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prune units in a tensor by zeroing out the ones with the lowest what?",
        "Y": "L1-norm",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prune entire channels in a tensor at random. prune.LnStructured Prune entire (currently unpru",
        "Y": "RandomStructured",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is Structured Prune entire (currently unpruned) channels in a tensor based on their Ln-",
        "Y": "Ln",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of structure is used to prune units in a tensor?",
        "Y": "random_unstructured",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.RandomStructured Prune entire (currently unpruned) channels in?",
        "Y": "a tensor at random",
        "Z": "prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What do prune.LnStructured Prune channels in a tensor based on?",
        "Y": "Ln-norm",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of structure does prune use?",
        "Y": "random",
        "Z": "Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "At what rate do prune.RandomStructured Prune entire channels in a tensor?",
        "Y": "random",
        "Z": "prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.LnStructured Prune entire (currently unpruned) channels in a tensor",
        "Y": "Ln-norm",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of structure does prune.identity apply pruning reparametrization to the tensor corresponding to the parameter callednameinmodul",
        "Y": "random_unstructured",
        "Z": "prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.LnStructured do?",
        "Y": "Prune entire (currently unpruned) channels in a tensor at random",
        "Z": "Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.LnStructured Prune channels in a tensor based on?",
        "Y": "Ln-norm",
        "Z": "Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pruning does _unstructured Prunes tensor corresponding to parameter callednameinmodule?",
        "Y": "random",
        "Z": "Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.LnStructured?",
        "Y": "Prune entire (currently unpruned) channels in a tensor at random",
        "Z": "Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What do prune.LnStructured Prune entire (currently unpruned) channels in a tensor",
        "Y": "Ln-norm",
        "Z": "prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.customFromMask apply to the tensor corresponding to the parameter callednameinmodule?",
        "Y": "pruning reparametrization",
        "Z": "Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "_unstructured Prunes tensor corresponding to parameter callednameinmodule by removing the specifiedamountof (currently",
        "Y": "random",
        "Z": "Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the tensor based on?",
        "Y": "Ln-norm",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of unstructured Prunes tensor corresponding to parameter callednameinmodule?",
        "Y": "random",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Prune entire channels in a tensor based on their Ln-norm?",
        "Y": "prune.LnStructured",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of _unstructured Prunes tensor corresponding to parameter callednameinmodule?",
        "Y": "random",
        "Z": "prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What prune.random_unstructured Prunes tensor corresponding to parameter callednameinmodule?",
        "Y": "prune.ln_structured",
        "Z": "prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prune entire channels in a tensor based on what?",
        "Y": "Ln-norm",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.random_unstructured do?",
        "Y": "prune.l1_unstructured",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.random_unstructured?",
        "Y": "prune.l1_unstructured",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the unstructured Prunes tensor?",
        "Y": "random",
        "Z": "prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lowest value of the prune.l1_unstructured Prunes tensor?",
        "Y": "L1-norm",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What tensor corresponding to parameter callednameinmodule?",
        "Y": "prune.l1_unstructured Prunes tensor",
        "Z": "prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the value of the prune.l1_unstructured Prunes tensor?",
        "Y": "lowest L1-norm",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of _structured Prunes tensor is prune?",
        "Y": "random",
        "Z": "prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.identity apply to the tensor corresponding to the parameter callednameinmodule?",
        "Y": "reparametrization",
        "Z": "prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unprune",
        "Y": "prune.l1_unstructured",
        "Z": "Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the Prunes tensor that removes units with the lowest L1-norm?",
        "Y": "prune.random_structured",
        "Z": "prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Applies what to the tensor corresponding to the parameter callednameinmodule without actually pruning any units?",
        "Y": "pruning reparametrization",
        "Z": "Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which Prunes tensor corresponding to parameter callednameinmodule removes the specifiedamountof (currently unpruned",
        "Y": "prune.random_structured",
        "Z": "Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of prune's unstructured Prunes tensor?",
        "Y": "random",
        "Z": "prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the Prunes tensor?",
        "Y": "prune.ln_structured",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which Prunes tensor corresponding to parameter callednameinmodule removing the specifiedamountof (currently unpruned",
        "Y": "prune.global_unstructured",
        "Z": "prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are currently unpruned units selected at random?",
        "Y": "specifiedamountof",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "The prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing what (currently un",
        "Y": "specifiedamountof",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What corresponding to parameter callednameinmodule by removing the specifiedamountof (currently unpruned) units selected at random",
        "Y": "Prunes tensor",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.l1_unstructured Prunes tensor do?",
        "Y": "removing the specifiedamountof (currently unpruned) units with the lowest L1-norm",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of _structured Prunes tensor corresponding to parameter callednameinmodule?",
        "Y": "random",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the prune.random_structured Prunes tensor?",
        "Y": "prune.ln_structured",
        "Z": "prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lowest unit in the prune tensor?",
        "Y": "L1-norm",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is used to apply the custom_from_mask Prunes tensor?",
        "Y": "pre-computed mask inmask",
        "Z": "prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the pre-computed mask inmask?",
        "Y": "prune.remove",
        "Z": "prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Globally prunes tensors corresponding to what?",
        "Y": "all parameters inparameters",
        "Z": "prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is applied to the prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmodule?",
        "Y": "pre-computed mask inmask",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.custom_from_mask do?",
        "Y": "prune.remove",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is used to apply the custom_from_mask?",
        "Y": "pre-computed mask inmask",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What prunes tensors corresponding to all parameters inparameters by applying the specifiedpruning_method?",
        "Y": "global_unstructured",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.custom_from_mask use to prune tensor corresponding to parameter callednameinmodule?",
        "Y": "pre-computed mask inmask",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What removes the pruning reparameterization from a module?",
        "Y": "Removes",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How does prune.is_pruned check if a module is pruned?",
        "Y": "by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What global prunes tensors corresponding to all parameters inparameters?",
        "Y": "global_unstructured",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.custom_from_mask apply?",
        "Y": "pre-computed mask inmask",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What module does prune.is_pruned inherit from?",
        "Y": "theBasePruningMethod",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision?",
        "Y": "Quantization",
        "Z": "Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than\nfloating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to theQuantizationdocumentation.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What types of quantization does PyTorch support?",
        "Y": "per tensor and per channel asymmetric linear quantization",
        "Z": "Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than\nfloating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to theQuantizationdocumentation.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where can you find information on how to use quantized functions in PyTorch?",
        "Y": "theQuantizationdocumentation",
        "Z": "Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than\nfloating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to theQuantizationdocumentation.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than what?",
        "Y": "floating point precision",
        "Z": "Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than\nfloating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to theQuantizationdocumentation.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What document explains how to use quantized functions in PyTorch?",
        "Y": "theQuantizationdocumentation",
        "Z": "Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than\nfloating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to theQuantizationdocumentation.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for modules that lazily initialize parameters?",
        "Y": "lazy modules",
        "Z": "nn.modules.lazy.LazyModuleMixin A mixin for modules that lazily initialize parameters, also known as \u201clazy modules.\u201d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is Pytorch Hub designed to facilitate?",
        "Y": "research reproducibility",
        "Z": "Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility. Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish).",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What repository does Pytorch Hub support publishing pre-trained models to?",
        "Y": "github repository",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is each entrypoint defined as?",
        "Y": "python",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a pre-trained model repository designed to facilitate research reproducibility?",
        "Y": "Pytorch Hub",
        "Z": "Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility. Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What supports publishing pre-trained models to a github repository?",
        "Y": "Pytorch Hub",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish).",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Pytorch Hub supports publishing pre-trained models to what repository?",
        "Y": "github",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can have multiple entrypoints?",
        "Y": "hubconf.pycan",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Each entrypoint is defined as what type of function?",
        "Y": "python",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What specifies an entrypoint forresnet18model if we expand the implementation inpytorch/vision/hubconf.p",
        "Y": "snippet",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "In most cases importing the right function is what?",
        "Y": "inhubconf.pyis sufficient",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is used as an example to show how hubconf.py works?",
        "Y": "expanded version",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where can you see the full script inpytorch/vision repo?",
        "Y": "full script inpytorch/vision repo",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can hubconf.py have?",
        "Y": "multiple entrypoints",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does the code snippet specify if we expand the implementation inpytorch/vision/hubconf.py?",
        "Y": "an entrypoint forresnet18model",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "In most cases importing the right function is sufficient.",
        "Y": "inhubconf.py",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What do we want to use the expanded version as to show how it works?",
        "Y": "an example",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where can you see the full script?",
        "Y": "inpytorch/vision repo",
        "Z": "hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a listof package names required toload the model?",
        "Y": "inpytorch/vision repo dependencies variable",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What might be slightly different from inpytorch/vision repo dependencies variable?",
        "Y": "dependencies required for training a model",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is alistof package names required toloadthemodel?",
        "Y": "dependencies variable",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How different might dependencies variable be from dependencies required for training a model?",
        "Y": "slightly different",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is passed along to the real callable function?",
        "Y": "argsandkwargsare",
        "Z": "argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What part of the function works as a help message?",
        "Y": "Docstring",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does the Docstring of the function explain?",
        "Y": "what does the model do and what are the allowed positional/keyword arguments",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is highly recommended to do with the Docstring?",
        "Y": "add a few examples",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a list of package names required to load the model?",
        "Y": "dependencies variable",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is dependencies variable slightly different from?",
        "Y": "dependencies",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What are passed along to the real callable function?",
        "Y": "argsandkwargs",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What are the allowed arguments for the model?",
        "Y": "positional/keyword",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list().",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is highly recommended to add here?",
        "Y": "a few examples",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is highly recommended to add to the Docstring of the function?",
        "Y": "a few examples",
        "Z": "argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can an entrypoint function return to make the user workflow smoother?",
        "Y": "auxiliary tools",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is an example of an entrypoint function that can make the user workflow smoother?",
        "Y": "tokenizers",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What do callables prefixed with are considered as helper functions?",
        "Y": "underscore",
        "Z": "argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list().",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where can pretrained weights be stored locally?",
        "Y": "github repo",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How big is the size of the pretrained weights?",
        "Y": "less than 2GB",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where can you put the following logic?",
        "Y": "entrypoint definition",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "The published models should be at least in what?",
        "Y": "branch/tag",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can't be a published model?",
        "Y": "random commit",
        "Z": "Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How can pretrained weights be loaded?",
        "Y": "stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url()",
        "Z": "Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What should you do if the weight is less than 2GB?",
        "Y": "attach it to aproject release",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where can you put the following logic in the example abovetorchvision.models.resnet.resnet18handlespret",
        "Y": "entrypoint definition",
        "Z": "Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What should the published models be at least in?",
        "Y": "branch/tag",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can't be published models in a branch/tag?",
        "Y": "a random commit",
        "Z": "Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What should the published models be in?",
        "Y": "branch/tag",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What provides convenient APIs to explore all available models in hub throughtorch.hub.list()?",
        "Y": "Pytorch Hub",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does Pytorch Hub list?",
        "Y": "all entrypoints available in github hubconf",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a string with format \u201crepo_owner/repo_name[:tag_name]?",
        "Y": "github(string)",
        "Z": "Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default branch of github?",
        "Y": "local",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is an example of a github branch?",
        "Y": "\u2018pytorch/vision[:hub]\u2019",
        "Z": "argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of all entrypoints available in hub?",
        "Y": "in github hubconf",
        "Z": "Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional tag/",
        "Y": "github(string)",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does github do?",
        "Y": "List all entrypoints available in github hubconf",
        "Z": "List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a string with format \"repo_owner/repo_name[:tag_name] with an optional tag/bra",
        "Y": "github",
        "Z": "List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the command to force a fresh download?",
        "Y": "force_reload",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value of force_reload?",
        "Y": "True",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the entrypointmodel?",
        "Y": "docstring",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is available in github hubconf?",
        "Y": "all entrypoints",
        "Z": "List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for a list of available entrypoint names?",
        "Y": "Default is False",
        "Z": "List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of entrypointmodel?",
        "Y": "docstring",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a string with format \"repo_owner/repo_name[:tag_name]?",
        "Y": "github",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default branch?",
        "Y": "ismasterif not specified",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for force_reload?",
        "Y": "Default is False",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the docstring of?",
        "Y": "entrypointmodel",
        "Z": "Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the command to discard the cache and force a fresh download?",
        "Y": "force_reload",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a string with format repo_owner/repo_name[:tag_name] with an optional tag/",
        "Y": "github(string)",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where is the entrypoint name defined in the hubconf.py?",
        "Y": "repo",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default is False. a list of available entrypoint names entrypoints?",
        "Y": "force_reload",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a string with format repo_owner/repo_name[:tag_name]> with an optional tag",
        "Y": "github(string)",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default if not specified?",
        "Y": "branch",
        "Z": "entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a string of entrypoint name defined in repo\u2019s hubconf.py?",
        "Y": "model(string)",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of a model defined in a repo's hubconf.py?",
        "Y": "pytorch/vision[:hub]",
        "Z": "github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Load a model from a github repo or what?",
        "Y": "a local directory",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What default value does force_reload(bool,optional) return?",
        "Y": "False",
        "Z": "Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the entrypoint model defined in the repo's hubconf.py force_reload(bool,optional",
        "Y": "pytorch/vision[:hub]",
        "Z": "Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for force_reload(bool,optional)?",
        "Y": "False",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a string of entrypoint name defined in repo's hubconf.py force_reload(bool,optional)",
        "Y": "model(string)",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for model(string)?",
        "Y": "False",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What repo can a model be loaded from?",
        "Y": "github",
        "Z": "Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the typical use case?",
        "Y": "Loading a model",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "if sourceis'github',repo_or_diris expected to be of what?",
        "Y": "formrepo_owner/repo_name[:tag_name]with an optional tag/branch",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where can a model be loaded from?",
        "Y": "github repo or a local directory",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Loading a model is the typical use case, but can also be used to load other objects such as what?",
        "Y": "tokenizers, loss functions, etc",
        "Z": "Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is expected to be of the formrepo_owner/repo_name[:tag_name]with an optional tag/bra",
        "Y": "if sourceis'github'",
        "Z": "a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the function that can be used to force a fresh download?",
        "Y": "force_reload",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "if sourceis'local',repo_or_diris expected to be what?",
        "Y": "path to a local directory",
        "Z": "if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "if sourceis'local',repo_or_diris expected to be what to a local directory?",
        "Y": "path",
        "Z": "Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where can you load a model from?",
        "Y": "a local directory",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Loading a model can also be used to load other objects such as what?",
        "Y": "tokenizers, loss functions",
        "Z": "Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Loading a model is the typical use case but can also be used to load other objects such as what?",
        "Y": "tokenizers, loss functions, etc",
        "Z": "Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is expected to be a path to a local directory?",
        "Y": "if sourceis'local',repo_or_diris",
        "Z": "if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is repo_or_dir(string)?",
        "Y": "repo name",
        "Z": "Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "if sourceis'github',repo_or_diris expected to be of the formrepo_owner/repo",
        "Y": "tag/branch",
        "Z": "Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the repo?",
        "Y": "repo_or_dir",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does repo_or_dir(string) mean?",
        "Y": "repo name",
        "Z": "if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of a callable defined in the repo/dir'shubconf.py?",
        "Y": "model(string)",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does *args(optional) provide for callablemodel?",
        "Y": "corresponding args",
        "Z": "if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "if sourceis',repo_or_diris expected to be of the formrepo_owner/repo_name[",
        "Y": "github",
        "Z": "if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the corresponding args for callablemodel?",
        "Y": "source(string,optional)",
        "Z": "List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does repo_or_diris expect to be a path to a local directory?",
        "Y": "if sourceis'local'",
        "Z": "if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "*args(optional) \u2013 the corresponding args for what?",
        "Y": "callablemodel",
        "Z": "*args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What specifies how repo_or_dir is to be interpreted?",
        "Y": "source(string,optional)",
        "Z": "*args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is specified to be interpreted?",
        "Y": "how repo_or_dir is",
        "Z": "if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default name for how repo_or_dir is to be interpreted?",
        "Y": "github",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is repo_or_diris expected to be?",
        "Y": "a path to a local directory",
        "Z": "if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of a callable defined in the repo/dir\u2019shubconf.py?",
        "Y": "model(string)",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does source(string,optional) specify?",
        "Y": "how repo_or_dir is to be interpreted",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default name for repo_or_diris?",
        "Y": "github",
        "Z": "if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does repo_or_dir(string) contain?",
        "Y": "repo name",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does if source='local' mean?",
        "Y": "a path to a local directory",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does github specify to be interpreted?",
        "Y": "how repo_or_dir is",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default for how repo_or_dir is to be interpreted?",
        "Y": "github",
        "Z": "Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does repo_or_dir(string) refer to?",
        "Y": "repo name",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where is the name of a callable defined?",
        "Y": "repo/dir\u2019shubconf.py",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the source of the github repo?",
        "Y": "source",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default name of the github repo?",
        "Y": "github",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a way to force a fresh download of the github repo?",
        "Y": "force_reload",
        "Z": "Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does force_reload(bool,optional) not have any effect on?",
        "Y": "if source='local'",
        "Z": "Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the corresponding args for callablemodel?",
        "Y": "source(string,optional)",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the command to force a fresh download of the github repo?",
        "Y": "force_reload",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Does force_reload have any effect?",
        "Y": "Does not have any effect",
        "Z": "*args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for force_reload?",
        "Y": "is False",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default name of the git repo?",
        "Y": "Default is'github'",
        "Z": "*args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Does not have any effect if source='local'?",
        "Y": "if source='local'",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What are the corresponding args for callablemodel?",
        "Y": "*args",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is 'github'|'local'?",
        "Y": "source",
        "Z": "*args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does source(string,optional) specify to be interpreted?",
        "Y": "how repo_or_dir is",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default location for how repo_or_dir is to be interpreted?",
        "Y": "github",
        "Z": "Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the option to force a fresh download of the github repo?",
        "Y": "force_reload",
        "Z": "*args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the string that specifies how repo_or_dir is to be interpreted?",
        "Y": "source",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does verbose(bool,optional) do to mute messages about hitting local caches?",
        "Y": "If False",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "The message about first download cannot be what?",
        "Y": "muted",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is 'github'?",
        "Y": "Default",
        "Z": "Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Does force_reload(bool,optional) have any effect if source='local'?",
        "Y": "Does not have any effect",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does If False do?",
        "Y": "mute messages about hitting local caches",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What message cannot be muted if source='local'?",
        "Y": "message about first download cannot be muted",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Does the message about first download have any effect if source='local'?",
        "Y": "Does not have any effect",
        "Z": "*args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default if source='local'?",
        "Y": "True",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Does force_reload(bool,optional) \u2013 whether to force a fresh download of the github repo unconditionally?",
        "Y": "Does not have any effect if source='local'",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does force_reload(bool,optional) do?",
        "Y": "Default is False",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Does not have any effect if source='local'. Default is True. verbose(bool,optional)",
        "Y": "Does not have any effect",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the corresponding kwargs for callablemodel?",
        "Y": "kwargs",
        "Z": "if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the output of themodelcallable when called with the given*argsand**kwargs?",
        "Y": "a local path",
        "Z": "The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Does not have any effect what?",
        "Y": "if source='local'",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for muting messages about hitting local caches?",
        "Y": "True",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the corresponding kwargs for?",
        "Y": "callablemodel",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the output of when called with the given*argsand**kwargs?",
        "Y": "themodelcallable",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the download object at the given URL to?",
        "Y": "a local path",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the url(string) of the object to download?",
        "Y": "URL",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for muting messages about hitting local caches?",
        "Y": "Default is True",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where is the Example Download object?",
        "Y": "given URL to a local path",
        "Z": "**kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the URL of the object to download?",
        "Y": "a local path",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does the object at the given URL download to?",
        "Y": "a local path",
        "Z": "**kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What type of file should start withhash_prefix?",
        "Y": "SHA256",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for the SHA256 downloaded file?",
        "Y": "None",
        "Z": "**kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What are the corresponding kwargs for callablemodel?",
        "Y": "**kwargs",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "url(string) \u2013 URL of the object to download?",
        "Y": "dst",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for the SHA256 downloaded file to start withhash_prefix?",
        "Y": "None",
        "Z": "**kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "The output of themodelcallable when called with what?",
        "Y": "given*argsand**kwargs",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default to display a progress bar to stderr?",
        "Y": "None progress(bool,optional)",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What happens when called with the given*argsand**kwargs?",
        "Y": "output of themodelcallable",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for the progress bar to stderr?",
        "Y": "Default: None",
        "Z": "Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Example Download object at the given URL to what?",
        "Y": "a local path",
        "Z": "Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does dst(string) mean?",
        "Y": "Full path where object will be saved",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does the Torch serialized object load at the given URL to a local path?",
        "Y": "Example Download object",
        "Z": "Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If None, the SHA256 downloaded file should start what?",
        "Y": "withhash_prefix",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Download object at the given URL to what?",
        "Y": "local path",
        "Z": "Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If downloaded file is what, it will be automatically decompressed?",
        "Y": "a zip file",
        "Z": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the path where the object will be saved?",
        "Y": "Full path",
        "Z": "url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If downloaded file is a zip file, it will be what?",
        "Y": "automatically decompressed",
        "Z": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the SHA256 downloaded file?",
        "Y": "hash_prefix",
        "Z": "hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for displaying a progress bar to stderr?",
        "Y": "True Example Loads the Torch serialized object at the given URL",
        "Z": "hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What should the SHA256 downloaded file start?",
        "Y": "withhash_prefix",
        "Z": "hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What type of commit can\u2019t a published model be?",
        "Y": "random commit",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can\u2019t be published models?",
        "Y": "a random commit",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "To what does progress(bool,optional) display a progress bar?",
        "Y": "stderr",
        "Z": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If the object is already present, it's deserialized and returned?",
        "Y": "inmodel_dir",
        "Z": "url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value of model_diris?",
        "Y": "byget_dir()",
        "Z": "**kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Default: True Example Loads the Torch serialized object at the given URL?",
        "Y": "Default: True Example Loads the Torch serialized object at the given URL",
        "Z": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If the object is already present, it\u2019s deserialized and returned.",
        "Y": "inmodel_dir",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value ofmodel_dirishub_dir>/checkpointswherehub_diris?",
        "Y": "byget_dir()",
        "Z": "If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where is the Torch serialized object loaded?",
        "Y": "the given URL",
        "Z": "Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What serialized object does the example load at the given URL?",
        "Y": "Torch",
        "Z": "Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If the downloaded file is what, it will be automatically decompressed?",
        "Y": "a zip file",
        "Z": "Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If the object is already present, what is it deserialized and returned?",
        "Y": "inmodel_dir",
        "Z": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "url(string) \u2013 URL of the object to download what?",
        "Y": "model_dir",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the URL of the object to download model_dir(string,optional)?",
        "Y": "url(string) \u2013 URL of the object to download model_dir(string,optional)",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If the object is already present, what is deserialized and returned?",
        "Y": "inmodel_dir",
        "Z": "If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value ofmodel_diris?",
        "Y": "the directory returned byget_dir()",
        "Z": "The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value of model_dir?",
        "Y": "True",
        "Z": "If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is used to specify how to remap storage locations?",
        "Y": "a function or a dict",
        "Z": "model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the first eight or more digits of the SHA256 hash of the contents of a file?",
        "Y": "first eight or more digits",
        "Z": "If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value of the hash?",
        "Y": "False",
        "Z": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is progress(bool,optional)?",
        "Y": "whether or not to display a progress bar to stderr",
        "Z": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the SHA256 hash of the contents of a file?",
        "Y": "first eight or more digits",
        "Z": "If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Why is the hash used?",
        "Y": "to ensure unique names and to verify the contents of the file",
        "Z": "check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is extwheresha256>?",
        "Y": "first eight or more digits",
        "Z": "check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for file_name?",
        "Y": "False file_name(string,optional) \u2013 name for the downloaded file",
        "Z": "map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir().",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is an example of a filename fromurl?",
        "Y": "Example",
        "Z": "check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the downloaded file?",
        "Y": "file_name",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the purpose of argsand**kwargsintorch.hub.load()?",
        "Y": "toinstantiatea model",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What should you do after you have loaded a model?",
        "Y": "how can you find out what you can do with the model",
        "Z": "Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a suggested workflow to see all available methods of the model?",
        "Y": "dir(model)",
        "Z": "Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does help(model.foo) use to check what argumentsmodel.footakes to run?",
        "Y": "model.foo",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the purpose of *argsand**kwargsintorch.hub.load()?",
        "Y": "toinstantiatea model",
        "Z": "Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can you do after you have loaded a model?",
        "Y": "how can you find out what you can do with the model",
        "Z": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a suggested workflow to see all available methods of a model?",
        "Y": "dir(model)",
        "Z": "Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What arguments do you need to run help(model.foo)?",
        "Y": "model.footakes",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is used to instantiate a model?",
        "Y": "*argsand**kwargsintorch.hub.load()",
        "Z": "Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Who makes function help messages clear and succinct?",
        "Y": "repo owners",
        "Z": "Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What type of example would be helpful to include?",
        "Y": "minimal working",
        "Z": "Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What should repo owners make clear and succinct?",
        "Y": "help messages",
        "Z": "Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What should repo owners include to help users explore without referring to documentation back and forth?",
        "Y": "a minimal working example",
        "Z": "Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What are the locations used in the order of?",
        "Y": "Callinghub.set_dir",
        "Z": "The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does $XDG_CACHE_HOME/torch/hub do?",
        "Y": "if environment variableXDG_CACHE_HOMEis set",
        "Z": "file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is used for storing downloaded models & weights?",
        "Y": "Torch Hub cache directory",
        "Z": "The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How are the locations used?",
        "Y": "in the order of Callinghub.set_dir",
        "Z": "The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the Torch Hub cache directory used for storing downloaded models & weights?",
        "Y": "$XDG_CACHE_HOME/torch/hub",
        "Z": "The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the directory that is set by callinghub.set_dir(PATH_TO_HUB_DIR>)",
        "Y": "$TORCH_HOME/hub",
        "Z": "Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default path if set_dir()is not called?",
        "Y": "$TORCH_HOME",
        "Z": "check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir().",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default path for the Torch Hub cache directory?",
        "Y": "$XDG_CACHE_HOME/torch",
        "Z": "Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is used to save downloaded models & weights?",
        "Y": "Optionally set the Torch Hub directory",
        "Z": "~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the Torch Hub directory used for storing downloaded models & weights?",
        "Y": "Torch Hub cache directory",
        "Z": "Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default path to the Torch Hub cache directory?",
        "Y": "Ifset_dir()is not called",
        "Z": "The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What directory is used to save downloaded models & weights?",
        "Y": "Torch Hub",
        "Z": "check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights. By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir().",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the Torch Hub directory used to save downloaded models & weights?",
        "Y": "Optionally set the Torch Hub directory",
        "Z": "Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Why is the default path $TORCH_HOME/hub?",
        "Y": "Ifset_dir()is not called",
        "Z": "Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What directory can be optionally set to save downloaded models & weights?",
        "Y": "Torch Hub",
        "Z": "Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the path to a local folder to save downloaded models & weights?",
        "Y": "d(string)",
        "Z": "Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default path for environment variable$TORCH_HOME?",
        "Y": "$XDG_CACHE_HOME/torch",
        "Z": "Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the path to a local folder to save downloaded models & weights?",
        "Y": "d(string)",
        "Z": "The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default behavior of Hub?",
        "Y": "we don\u2019t clean up files after loading it",
        "Z": "By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Hub uses the cache by default if it already exists in the directory returned what?",
        "Y": "byget_dir()",
        "Z": "By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How can users force a reload?",
        "Y": "callinghub.load",
        "Z": "By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does the force a reload delete?",
        "Y": "github folder",
        "Z": "By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does Hub not do after loading it?",
        "Y": "clean up files",
        "Z": "By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does Hub use if it already exists in the directory returned byget_dir()?",
        "Y": "the cache",
        "Z": "By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does force a reload do?",
        "Y": "delete the existing github folder and downloaded weights",
        "Z": "By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir(). Users can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing github folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What works by importing the package as if it was installed?",
        "Y": "Torch hub",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior. A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What causes some side effects?",
        "Y": "importing in Python",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is cachessys.modulesandsys.path_importer_cache?",
        "Y": "normal Python behavior",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior. A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How does Torch hub work?",
        "Y": "importing the package as if it was installed",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior. A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "In what language are some side effects introduced by importing?",
        "Y": "Python",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior. A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where can you see new items in Python?",
        "Y": "cachessys.modulesandsys.path_importer_cache",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a known limitation that is worth mentioning?",
        "Y": "userCANNOTload two different branches of the same repo in thesame python process",
        "Z": "A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "In what language are two packages with the same name installed?",
        "Y": "Python",
        "Z": "A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What might join the party and give you surprises if you try to load two different branches of the same repo in the same python",
        "Y": "Cache",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior. A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Is it okay to load two different branches of the same repo in separate processes?",
        "Y": "totally fine",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior. A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a known limitation?",
        "Y": "userCANNOTload two different branches of the same repo in thesame python process",
        "Z": "A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How many packages with the same name are installed in the same Python process?",
        "Y": "two",
        "Z": "A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What might join the party and give you surprises if you try it?",
        "Y": "Cache",
        "Z": "A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Is it okay to install two different branches of the same repo in the same process?",
        "Y": "load them in separate processes",
        "Z": "A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Returns what with the logarithm to the base 10 of the elements of input?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the logarithm to the base 10 of the elements\nofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10"
    },
    {
        "X": "What does liketorch.minimum() do?",
        "Y": "Computes the element-wise minimum ofinputandother",
        "Z": "Computes the element-wise minimum ofinputandother. This is liketorch.minimum()except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019sstd::fminand is similar to NumPy\u2019sfminfunction. Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "What is the name of the function that computes the element-wise minimum of inputandother?",
        "Y": "liketorch.minimum()",
        "Z": "Computes the element-wise minimum ofinputandother. This is liketorch.minimum()except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019sstd::fminand is similar to NumPy\u2019sfminfunction. Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "If both elements are what is the minimum, then the non-NaN element is taken as the minimum?",
        "Y": "NaN",
        "Z": "Computes the element-wise minimum ofinputandother. This is liketorch.minimum()except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019sstd::fminand is similar to NumPy\u2019sfminfunction. Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "What language is this function a wrapper around?",
        "Y": "C++",
        "Z": "This is liketorch.minimum()except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019sstd::fminand is similar to NumPy\u2019sfminfunction. Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "What types of inputs does fminand support?",
        "Y": "integer and floating-point inputs",
        "Z": "This is liketorch.minimum()except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019sstd::fminand is similar to NumPy\u2019sfminfunction. Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "What does liketorch.minimum() compute?",
        "Y": "the element-wise minimum ofinputandother",
        "Z": "Computes the element-wise minimum ofinputandother. This is liketorch.minimum()except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019sstd::fminand is similar to NumPy\u2019sfminfunction. Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "What is liketorch.minimum() a wrapper around?",
        "Y": "C++\u2019sstd::fminand",
        "Z": "Computes the element-wise minimum ofinputandother. This is liketorch.minimum()except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019sstd::fminand is similar to NumPy\u2019sfminfunction. Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "What is taken as the minimum if exactly one of the two elements being compared is a NaN?",
        "Y": "the non-NaN element",
        "Z": "This is liketorch.minimum()except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019sstd::fminand is similar to NumPy\u2019sfminfunction. Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "What is this function a wrapper around?",
        "Y": "C++\u2019sstd::fminand",
        "Z": "This function is a wrapper around C++\u2019sstd::fminand is similar to NumPy\u2019sfminfunction. Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "What function is similar to:fminand?",
        "Y": "NumPy\u2019sfminfunction",
        "Z": "This function is a wrapper around C++\u2019sstd::fminand is similar to NumPy\u2019sfminfunction. Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "What types of inputs does the function support?",
        "Y": "float, double, cfloat and cdouble dtypes",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What is the output of inputandvec2?",
        "Y": "Outer product ofinputandvec2",
        "Z": "Outer product ofinputandvec2.\nIf input is a vector of sizennnandvec2is a vector of\nsizemmm, thenoutmust be a matrix of size(n\u00d7m)(n \\times m)(n\u00d7m). Note This function does notbroadcast. input(Tensor) \u2013 1-D input vector vec2(Tensor) \u2013 1-D input vector out(Tensor,optional) \u2013 optional output matrix Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer"
    },
    {
        "X": "If input is a vector of sizennandvec2is a vector of what, thenoutmust be a matrix of",
        "Y": "sizemmm",
        "Z": "Outer product ofinputandvec2.\nIf input is a vector of sizennnandvec2is a vector of\nsizemmm, thenoutmust be a matrix of size(n\u00d7m)(n \\times m)(n\u00d7m). Note This function does notbroadcast. input(Tensor) \u2013 1-D input vector vec2(Tensor) \u2013 1-D input vector out(Tensor,optional) \u2013 optional output matrix Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer"
    },
    {
        "X": "What does input(Tensor) vec2(Tensor) vec2(Tensor) ve",
        "Y": "1-D input vector out(Tensor,optional) \u2013 optional output matrix",
        "Z": "Outer product ofinputandvec2.\nIf input is a vector of sizennnandvec2is a vector of\nsizemmm, thenoutmust be a matrix of size(n\u00d7m)(n \\times m)(n\u00d7m). Note This function does notbroadcast. input(Tensor) \u2013 1-D input vector vec2(Tensor) \u2013 1-D input vector out(Tensor,optional) \u2013 optional output matrix Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer"
    },
    {
        "X": "Returns what with the tangent of the elements of input?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the tangent of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan"
    },
    {
        "X": "What is the matrix to be added vec1(Tensor)?",
        "Y": "input(Tensor)",
        "Z": "Ifvec1is a vector of sizenandvec2is a vector\nof sizem, theninputmust bebroadcastablewith a matrix of size(n\u00d7m)(n \\times m)(n\u00d7m)andoutwill be a matrix of size(n\u00d7m)(n \\times m)(n\u00d7m). input(Tensor) \u2013 matrix to be added vec1(Tensor) \u2013 the first vector of the outer product vec2(Tensor) \u2013 the second vector of the outer product beta(Number,optional) \u2013 multiplier forinput(\u03b2\\beta\u03b2) alpha(Number,optional) \u2013 multiplier forvec1\u2297vec2\\text{vec1} \\otimes \\text{vec2}vec1\u2297vec2(\u03b1\\alpha\u03b1)",
        "source": "https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr"
    },
    {
        "X": "Input(Tensor) \u2013 matrix to be added what?",
        "Y": "vec1",
        "Z": "input(Tensor) \u2013 matrix to be added vec1(Tensor) \u2013 the first vector of the outer product vec2(Tensor) \u2013 the second vector of the outer product beta(Number,optional) \u2013 multiplier forinput(\u03b2\\beta\u03b2) alpha(Number,optional) \u2013 multiplier forvec1\u2297vec2\\text{vec1} \\otimes \\text{vec2}vec1\u2297vec2(\u03b1\\alpha\u03b1) out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr"
    },
    {
        "X": "What returns the cumulative product of elements of inputin the dimensiondim?",
        "Y": "If input is",
        "Z": "Returns the cumulative product of elements ofinputin the dimensiondim. For example, If input is a vector of size N, the result will also be\na vector of size N, with elements. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod"
    },
    {
        "X": "What is the default value for the input tensor?",
        "Y": "None",
        "Z": "Returns the cumulative product of elements ofinputin the dimensiondim. For example, If input is a vector of size N, the result will also be\na vector of size N, with elements. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod"
    },
    {
        "X": "What does inputin the dimensiondim return?",
        "Y": "the cumulative product of elements",
        "Z": "Returns the cumulative product of elements ofinputin the dimensiondim. For example, If input is a vector of size N, the result will also be\na vector of size N, with elements. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod"
    },
    {
        "X": "What is the dimension to do the operation over?",
        "Y": "dim(int)",
        "Z": "Returns the cumulative product of elements ofinputin the dimensiondim. For example, If input is a vector of size N, the result will also be\na vector of size N, with elements. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod"
    },
    {
        "X": "What does the dot product do for higher dimensions?",
        "Y": "sums the product of elements frominputandotheralong their last dimension",
        "Z": "Computes the dot product for 1D tensors. For higher dimensions, sums the product\nof elements frominputandotheralong their last dimension. Note If eitherinputorotheris a scalar, the result is equivalent\ntotorch.mul(input, other). If bothinputandotherare non-scalars, the size of their last\ndimension must match and the result is equivalent totorch.tensordot(input,\nother, dims=([-1], [-1])) input(Tensor) \u2013 First input tensor other(Tensor) \u2013 Second input tensor out(Tensor,optional) \u2013 Optional output tensor to write result into. The output\nshape isinput.shape[:-1] + other.shape[:-1]. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner"
    },
    {
        "X": "If eitherinputorotheris a what, the result is equivalent totorch.mul(input, other)?",
        "Y": "scalar",
        "Z": "Computes the dot product for 1D tensors. For higher dimensions, sums the product\nof elements frominputandotheralong their last dimension. Note If eitherinputorotheris a scalar, the result is equivalent\ntotorch.mul(input, other). If bothinputandotherare non-scalars, the size of their last\ndimension must match and the result is equivalent totorch.tensordot(input,\nother, dims=([-1], [-1])) input(Tensor) \u2013 First input tensor other(Tensor) \u2013 Second input tensor out(Tensor,optional) \u2013 Optional output tensor to write result into. The output\nshape isinput.shape[:-1] + other.shape[:-1]. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner"
    },
    {
        "X": "What must happen if bothinputandotherare non-scalars?",
        "Y": "the size of their last dimension must match",
        "Z": "Computes the dot product for 1D tensors. For higher dimensions, sums the product\nof elements frominputandotheralong their last dimension. Note If eitherinputorotheris a scalar, the result is equivalent\ntotorch.mul(input, other). If bothinputandotherare non-scalars, the size of their last\ndimension must match and the result is equivalent totorch.tensordot(input,\nother, dims=([-1], [-1])) input(Tensor) \u2013 First input tensor other(Tensor) \u2013 Second input tensor out(Tensor,optional) \u2013 Optional output tensor to write result into. The output\nshape isinput.shape[:-1] + other.shape[:-1]. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner"
    },
    {
        "X": "What is the output shape?",
        "Y": "input.shape",
        "Z": "Computes the dot product for 1D tensors. For higher dimensions, sums the product\nof elements frominputandotheralong their last dimension. Note If eitherinputorotheris a scalar, the result is equivalent\ntotorch.mul(input, other). If bothinputandotherare non-scalars, the size of their last\ndimension must match and the result is equivalent totorch.tensordot(input,\nother, dims=([-1], [-1])) input(Tensor) \u2013 First input tensor other(Tensor) \u2013 Second input tensor out(Tensor,optional) \u2013 Optional output tensor to write result into. The output\nshape isinput.shape[:-1] + other.shape[:-1]. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner"
    },
    {
        "X": "What element represents if each element ofinputis \"close\" to the corresponding element ofother?",
        "Y": "boolean elements",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.\nCloseness is defined as: whereinputandotherare finite. Whereinputand/orotherare nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other whenequal_nanis True. input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08",
        "source": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"
    },
    {
        "X": "What is Closeness defined as?",
        "Y": "whereinputandotherare finite",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.\nCloseness is defined as: whereinputandotherare finite. Whereinputand/orotherare nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other whenequal_nanis True. input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08 rtol(float,optional) \u2013 relative tolerance. Default: 1e-05 equal_nan(bool,optional) \u2013 If True, then twoNaNs will be considered equal. Default:False Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"
    },
    {
        "X": "What is considered equal to each other whenequal_nanis True?",
        "Y": "NaNs",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.\nCloseness is defined as: whereinputandotherare finite. Whereinputand/orotherare nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other whenequal_nanis True. input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08",
        "source": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"
    },
    {
        "X": "What does the second tensor to compare atol(float,optional) represent?",
        "Y": "absolute tolerance",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.\nCloseness is defined as: whereinputandotherare finite. Whereinputand/orotherare nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other whenequal_nanis True. input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08",
        "source": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"
    },
    {
        "X": "What do boolean elements represent in a new tensor?",
        "Y": "if each element ofinputis \u201cclose\u201d to the corresponding element ofother",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.\nCloseness is defined as: whereinputandotherare finite. Whereinputand/orotherare nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other whenequal_nanis True. input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08",
        "source": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"
    },
    {
        "X": "When are NaNs considered equal to each other?",
        "Y": "whenequal_nanis True",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.\nCloseness is defined as: whereinputandotherare finite. Whereinputand/orotherare nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other whenequal_nanis True. input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08 rtol(float,optional) \u2013 relative tolerance. Default: 1e-05 equal_nan(bool,optional) \u2013 If True, then twoNaNs will be considered equal. Default:False Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"
    },
    {
        "X": "Atol(float,optional) \u2013 second tensor to compare atol(float,optional) \u2013 what is",
        "Y": "absolute tolerance",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.\nCloseness is defined as: whereinputandotherare finite. Whereinputand/orotherare nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other whenequal_nanis True. input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08",
        "source": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"
    },
    {
        "X": "What is the default value of input(Tensor) to compare atol(float,optional) \u2013 absolute tolerance?",
        "Y": "1e-08",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.\nCloseness is defined as: whereinputandotherare finite. Whereinputand/orotherare nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other whenequal_nanis True. input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08",
        "source": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"
    },
    {
        "X": "Atol(float,optional) \u2013 what does input(Tensor) compare atol(float,optional) compare",
        "Y": "absolute tolerance",
        "Z": "whereinputandotherare finite. Whereinputand/orotherare nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other whenequal_nanis True. input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08 rtol(float,optional) \u2013 relative tolerance. Default: 1e-05 equal_nan(bool,optional) \u2013 If True, then twoNaNs will be considered equal. Default:False Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"
    },
    {
        "X": "Returns what with the reciprocal of the square-root of each of the elements of input?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the reciprocal of the square-root of each of\nthe elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt"
    },
    {
        "X": "What is the size of the new index?",
        "Y": "tensor",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If the left boundary ofsorted_sequenceis closed, what is the default?",
        "Y": "Ifrightis False",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What does the sorted_sequence right returned index satisfies?",
        "Y": "1-D False",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Return what with the same size asvalues?",
        "Y": "a new tensor",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note If your use case is always 1-D sorted sequence,torch.bucketize()is preferred,\nbecause it has fewer dimension checks resulting in slightly better performance. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is the default value for the left boundary ofsorted_sequence?",
        "Y": "Ifrightis False",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "sorted_sequence right returned index satisfies what rule?",
        "Y": "1-D False",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "sorted_sequence right returned index satisfies what?",
        "Y": "1-D False",
        "Z": "sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Where does a sorted_sequence(Tensor) contain monotonically increasing sequence?",
        "Y": "the innermost dimension",
        "Z": "returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "On what dimension is a sorted_sequence(Tensor) containing monotonically increasing sequence?",
        "Y": "the innermost dimension",
        "Z": "False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What satisfies 1-D False sorted_sequence[i-1]values[m][n]",
        "Y": "right returned index",
        "Z": "right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What satisfies sorted_sequence[i-1]values[m][n]?",
        "Y": "1-D False",
        "Z": "returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "On what dimension does a tensor contain a monotonically increasing sequence?",
        "Y": "the innermost dimension",
        "Z": "False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Which sorted_sequence[i-1]=values[m][n]?",
        "Y": "1-D True",
        "Z": "1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s).",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is a values(TensorScalar) containing the search value(s)?",
        "Y": "N-D tensor or a Scalar",
        "Z": "sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s).",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "True sorted_sequence[i-1]=values[m][n]...[l][x]sorted",
        "Y": "1-D",
        "Z": "1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Values(TensororScalar) \u2013 N-D tensor or a Scalar containing what?",
        "Y": "search value(s)",
        "Z": "sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "On what dimension does a sorted_sequence(Tensor) contain monotonically increasing sequence?",
        "Y": "the innermost dimension",
        "Z": "True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s).",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is sorted_sequence(Tensor)?",
        "Y": "N-D or 1-D tensor",
        "Z": "sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What does sorted_sequence(Tensor) contain?",
        "Y": "monotonically increasing sequence",
        "Z": "sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s).",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "On what dimension is the sequence sorted_sequence(Tensor) containing monotonically increasing sequence?",
        "Y": "the innermost dimension",
        "Z": "sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s).",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is a sorted_sequence(Tensor) containing on the innermost dimension?",
        "Y": "monotonically increasing sequence",
        "Z": "sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What are values(TensororScalar) containing the search value(s)?",
        "Y": "N-D tensor or a Scalar",
        "Z": "sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "sorted_sequence(Tensor) \u2013 N-D or what tensor?",
        "Y": "1-D",
        "Z": "sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Values(TensororScalar) \u2013 N-D tensor or Scalar containing what?",
        "Y": "search value",
        "Z": "sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Where is the monotonically increasing sequence located?",
        "Y": "the innermost dimension",
        "Z": "sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What does values(TensororScalar) contain?",
        "Y": "search value",
        "Z": "sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What type of tensor is a values(TensorScalar)?",
        "Y": "N-D tensor or a Scalar",
        "Z": "values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What are values(TensororScalar)?",
        "Y": "N-D tensor or a Scalar",
        "Z": "values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If False, return the first suitable location that is found. If True, return the last such index?",
        "Y": "if False",
        "Z": "right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is returned if False?",
        "Y": "If True, return the last such index",
        "Z": "sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If no suitable index is found, return what value for non-numerical value?",
        "Y": "0",
        "Z": "right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What are examples of non-numerical values?",
        "Y": "nan, inf",
        "Z": "sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What does False return for each value invalueson the correspondinginnermostdimension of thesorted_sequence?",
        "Y": "lower bound index",
        "Z": "right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What does True get instead of the lower bound index?",
        "Y": "upper bound index",
        "Z": "True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is the default value of the lower bound index?",
        "Y": "False",
        "Z": "sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If no suitable index found, return 0 for non-numerical value (eg. nan, inf) or what?",
        "Y": "the size ofinnermostdimension withinsorted_sequence",
        "Z": "right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If False, gets what for each value invalueson the correspondinginnermostdimension of thesorted_sequence?",
        "Y": "lower bound index",
        "Z": "sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If False, gets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence",
        "Y": "upper bound index",
        "Z": "right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If your use case is always what?",
        "Y": "1-D sorted sequence",
        "Z": "out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note If your use case is always 1-D sorted sequence,torch.bucketize()is preferred,\nbecause it has fewer dimension checks resulting in slightly better performance. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What type of matrix can be created from provided tensors?",
        "Y": "block diagonal matrix",
        "Z": "Create a block diagonal matrix from provided tensors. *tensors\u2013 One or more tensors with 0, 1, or 2 dimensions. A 2 dimensional tensor with all the input tensors arranged in\norder such that their upper left and lower right corners are\ndiagonally adjacent. All other elements are set to 0. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag"
    },
    {
        "X": "How are the input tensors arranged in order?",
        "Y": "their upper left and lower right corners are diagonally adjacent",
        "Z": "Create a block diagonal matrix from provided tensors. *tensors\u2013 One or more tensors with 0, 1, or 2 dimensions. A 2 dimensional tensor with all the input tensors arranged in\norder such that their upper left and lower right corners are\ndiagonally adjacent. All other elements are set to 0. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag"
    },
    {
        "X": "All other elements are set to what?",
        "Y": "0.",
        "Z": "Create a block diagonal matrix from provided tensors. *tensors\u2013 One or more tensors with 0, 1, or 2 dimensions. A 2 dimensional tensor with all the input tensors arranged in\norder such that their upper left and lower right corners are\ndiagonally adjacent. All other elements are set to 0. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag"
    },
    {
        "X": "What is an example of a block diagonal matrix?",
        "Y": "Tensor Example:",
        "Z": "Create a block diagonal matrix from provided tensors. *tensors\u2013 One or more tensors with 0, 1, or 2 dimensions. A 2 dimensional tensor with all the input tensors arranged in\norder such that their upper left and lower right corners are\ndiagonally adjacent. All other elements are set to 0. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag"
    },
    {
        "X": "What can be created from provided tensors?",
        "Y": "a block diagonal matrix",
        "Z": "Create a block diagonal matrix from provided tensors. *tensors\u2013 One or more tensors with 0, 1, or 2 dimensions. A 2 dimensional tensor with all the input tensors arranged in\norder such that their upper left and lower right corners are\ndiagonally adjacent. All other elements are set to 0. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag"
    },
    {
        "X": "What are diagonally adjacent in a 2 dimensional tensor?",
        "Y": "their upper left and lower right corners",
        "Z": "Create a block diagonal matrix from provided tensors. *tensors\u2013 One or more tensors with 0, 1, or 2 dimensions. A 2 dimensional tensor with all the input tensors arranged in\norder such that their upper left and lower right corners are\ndiagonally adjacent. All other elements are set to 0. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag"
    },
    {
        "X": "What does aRuntimeError throw if the matrix is not invertible?",
        "Y": "Computes the inverse of a square matrix if it exists",
        "Z": "Computes the inverse of a square matrix if it exists.\nThrows aRuntimeErrorif the matrix is not invertible. LettingK\\mathbb{K}KbeR\\mathbb{R}RorC\\mathbb{C}C,\nfor a matrixA\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n}A\u2208Kn\u00d7n,\nitsinverse matrixA\u22121\u2208Kn\u00d7nA^{-1} \\in \\mathbb{K}^{n \\times n}A\u22121\u2208Kn\u00d7n(if it exists) is defined as whereIn\\mathrm{I}_nIn\u200bis then-dimensional identity matrix. The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What happens if the matrix is not invertible?",
        "Y": "Throws aRuntimeError",
        "Z": "Computes the inverse of a square matrix if it exists.\nThrows aRuntimeErrorif the matrix is not invertible. LettingK\\mathbb{K}KbeR\\mathbb{R}RorC\\mathbb{C}C,\nfor a matrixA\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n}A\u2208Kn\u00d7n,\nitsinverse matrixA\u22121\u2208Kn\u00d7nA^{-1} \\in \\mathbb{K}^{n \\times n}A\u22121\u2208Kn\u00d7n(if it exists) is defined as whereIn\\mathrm{I}_nIn\u200bis then-dimensional identity matrix. The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "The inverse matrix exists if and only what?",
        "Y": "ifAAAisinvertible",
        "Z": "The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique. Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What is the inverse of a square matrix if it exists?",
        "Y": "unique",
        "Z": "Computes the inverse of a square matrix if it exists.\nThrows aRuntimeErrorif the matrix is not invertible. LettingK\\mathbb{K}KbeR\\mathbb{R}RorC\\mathbb{C}C,\nfor a matrixA\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n}A\u2208Kn\u00d7n,\nitsinverse matrixA\u22121\u2208Kn\u00d7nA^{-1} \\in \\mathbb{K}^{n \\times n}A\u22121\u2208Kn\u00d7n(if it exists) is defined as whereIn\\mathrm{I}_nIn\u200bis then-dimensional identity matrix. The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "In this case, the inverse matrix exists if and only ifAAAisinvertible.",
        "Y": "unique",
        "Z": "LettingK\\mathbb{K}KbeR\\mathbb{R}RorC\\mathbb{C}C,\nfor a matrixA\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n}A\u2208Kn\u00d7n,\nitsinverse matrixA\u22121\u2208Kn\u00d7nA^{-1} \\in \\mathbb{K}^{n \\times n}A\u22121\u2208Kn\u00d7n(if it exists) is defined as whereIn\\mathrm{I}_nIn\u200bis then-dimensional identity matrix. The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "The inverse matrix exists if and only ifAAAisinvertible. What is the inverse matrix?",
        "Y": "unique",
        "Z": "The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique. Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What input types does the inverse matrix support?",
        "Y": "float, double, cfloat and cdouble dtypes",
        "Z": "The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique. Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What does the inverse matrix support?",
        "Y": "batches of matrices",
        "Z": "The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique. Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What is a possible way to multiply a matrix on the left by the inverse?",
        "Y": "usingtorch.linalg.solve()",
        "Z": "The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique. Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "The inverse matrix exists if and only ifAAAisinvertible. In this case, the inverse is what?",
        "Y": "unique",
        "Z": "The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique. Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "The inverse matrix supports input of float, double, cfloat and what other dtype?",
        "Y": "cdouble",
        "Z": "The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique. Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What type of matrices does the function support?",
        "Y": "batches of matrices",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "Why is it always preferable to usesolve() for multiplying a matrix on the left by the inverse?",
        "Y": "faster and more numerically stable",
        "Z": "The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique. Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What is a faster and more numerically stable way to multiply a matrix on the left by the inverse?",
        "Y": "usingtorch.linalg.solve()",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "When inputs are on what device, this function synchronizes that device with the CPU?",
        "Y": "CUDA",
        "Z": "When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What does torch.linalg.pinv() compute?",
        "Y": "pseudoinverse",
        "Z": "The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique. Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What does torch.linalg.solve()computesA.inv() @Bwith?",
        "Y": "a numerically stable algorithm",
        "Z": "The inverse matrix exists if and only ifAAAisinvertible. In this case,\nthe inverse is unique. Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and ifAis a batch of matrices\nthen the output has the same batch dimensions. Note When inputs are on a CUDA device, this function synchronizes that device with the CPU. Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\nthe inverse, as: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What is faster and more numerically stable than computing the inverse explicitly?",
        "Y": "usesolve()",
        "Z": "It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What is the default value of out(Tensor,optional)?",
        "Y": "if None",
        "Z": "It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What does torch.linalg.solve()compute?",
        "Y": "A.inv()",
        "Z": "It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What is the tensor of shape(*, n, n)where*?",
        "Y": "zero or more batch dimensions",
        "Z": "See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None. RuntimeError\u2013 if the matrixAor any matrix in the batch of matricesAis not invertible. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What is the default setting for output tensor?",
        "Y": "Default:None",
        "Z": "It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What is the default value for output tensor?",
        "Y": "Default:None",
        "Z": "See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None. RuntimeError\u2013 if the matrixAor any matrix in the batch of matricesAis not invertible. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What if the matrixAor any matrix in the batch of matricesAis not invertible?",
        "Y": "RuntimeError",
        "Z": "See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None. RuntimeError\u2013 if the matrixAor any matrix in the batch of matricesAis not invertible. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What does RuntimeError stand for?",
        "Y": "Examples",
        "Z": "See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None. RuntimeError\u2013 if the matrixAor any matrix in the batch of matricesAis not invertible. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What does torch.backends control?",
        "Y": "backends that PyTorch supports",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is PyTorch built with support for?",
        "Y": "CUDA",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is the point of using CUDA in a PyTorch binary?",
        "Y": "if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What controls the behavior of various backends that PyTorch supports?",
        "Y": "torch.backends",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does not mean that PyTorch is built with CUDA support?",
        "Y": "CUDA",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What support does PyTorch have?",
        "Y": "OpenMP",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Does this mean CUDA support is available?",
        "Y": "if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it",
        "Z": "torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does this mean?",
        "Y": "if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What controls whether TensorFloat-32 tensor cores may be used in matrix multiplication on Ampere or newer GPU",
        "Y": "Aboolthat",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does Abool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplication on Amper",
        "Y": "SeeTensorFloat-32(TF32) on Ampere devices",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch",
        "Y": "whether PyTorch is built with CUDA support",
        "Z": "torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does not mean PyTorch is built with CUDA support?",
        "Y": "CUDA",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What controls whether TensorFloat-32 tensor cores may be used in matrix multiplication?",
        "Y": "Abool",
        "Z": "torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is used on Ampere devices?",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Why is CUDA not supported by PyTorch?",
        "Y": "if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it",
        "Z": "torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does abool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplication on Am",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does cufft_plan_cache do?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer",
        "Y": "Abool",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is the name of the tensor cores used on Ampere devices?",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does cufft_plan_cache cache?",
        "Y": "cuFFT plans",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What support does torch.backends.openmp return?",
        "Y": "CUDA",
        "Z": "torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does PyTorch use on Ampere devices?",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is built with CUDA support?",
        "Y": "PyTorch",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does the PyTorch binary do?",
        "Y": "SeeTensorFloat-32(TF32) on Ampere devices",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is the name of the cuFFT plans?",
        "Y": "cufft_plan_cachecaches",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "If PyTorch was run on a machine with working what would we be able to use it?",
        "Y": "CUDA drivers and devices",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does abool that controls if TensorFloat-32 tensor cores may be used in matrix multiplication on",
        "Y": "SeeTensorFloat-32(TF32) on Ampere devices",
        "Z": "torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aintthat controls what of a cuFFT plan?",
        "Y": "cache capacity",
        "Z": "cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does cufft_plan_cachecachecache do?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What indicates if CUDNN is currently available?",
        "Y": "bool",
        "Z": "Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aboolthat controls what?",
        "Y": "whether cuDNN is enabled",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What can TensorFloat-32 tensor cores be used in?",
        "Y": "matrix multiplications",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does cufft_plan_cachecache cache?",
        "Y": "cuFFT plans",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aintthat controls what of a cuFFT plan. Clears the cuFFT plan cache.",
        "Y": "cache capacity",
        "Z": "cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does it do to the cuFFT plan cache?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is returned when a bool indicating if CUDNN is currently available?",
        "Y": "version of cuDNN",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aboolthat controls whether what is enabled. Returns a bool indicating if CUDNN is currently available.",
        "Y": "cuDNN",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Abool that controls what?",
        "Y": "whether cuDNN is enabled",
        "Z": "Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What may be used in cuDNN convolutions on Ampere or newer GPUs?",
        "Y": "TensorFloat-32 tensor cores",
        "Z": "Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does abool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolution",
        "Y": "SeeTensorFloat-32(TF32) on Ampere devices",
        "Z": "Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is returned when the cuFFT plan cache is cleared?",
        "Y": "version of cuDNN",
        "Z": "cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aboolthat controls where tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs",
        "Y": "TensorFloat-32",
        "Z": "Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is the name of the tensor cores that can be used in cuDNN convolutions on Ampere devices?",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What shows the number of plans currently in the cuFFT plan cache?",
        "Y": "readonly int that",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does aint that controls cache capacity of cuFFT plan do?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere devices",
        "Y": "SeeTensorFloat-32(TF32) on Ampere devices",
        "Z": "torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What type of intthat shows the number of plans currently in the cuFFT plan cache?",
        "Y": "readonly",
        "Z": "A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does Aintthat controls cache capacity of cuFFT plan cache do?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is returned when a cuFFT plan cache is cleared?",
        "Y": "version of cuDNN",
        "Z": "A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aintthat controls what of cuFFT plan?",
        "Y": "cache capacity",
        "Z": "Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does Aint do?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aboolthat controls where what may be used in cuDNN convolutions on Ampere or newer GPUs?",
        "Y": "TensorFloat-32 tensor cores",
        "Z": "Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does Aint that controls cache capacity of cuFFT plan do?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does Aint that controls cache capacity of cuFFT plan return?",
        "Y": "version of cuDNN",
        "Z": "Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does the bool that returns the version of cuDNN control?",
        "Y": "whether cuDNN is enabled",
        "Z": "Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions?",
        "Y": "SeeTensorFloat-32(TF32) on Ampere devices",
        "Z": "Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aboolthat, if True, causes cuDNN to only use what?",
        "Y": "deterministic convolution algorithms",
        "Z": "Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does cuFFT do?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What type of algorithms does _algorithms_enabled() enable?",
        "Y": "deterministic",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does Returns a bool indicating if CUDNN is currently available?",
        "Y": "version of cuDNN",
        "Z": "Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does the bool that returns the version of cuDNN do?",
        "Y": "controls whether cuDNN is enabled",
        "Z": "Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is _enabled()?",
        "Y": "deterministic_algorithms",
        "Z": "Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What returns a indicating if CUDNN is currently available?",
        "Y": "bool",
        "Z": "Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does the Abool that returns a bool that controls?",
        "Y": "whether cuDNN is enabled",
        "Z": "Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does the Abool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolution",
        "Y": "SeeTensorFloat-32(TF32) on Ampere devices",
        "Z": "Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aboolthat causes cuDNN to only use deterministic convolution algorithms if what?",
        "Y": "True",
        "Z": "Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Returns what indicating if CUDNN is currently available?",
        "Y": "a bool",
        "Z": "Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aboolthat controls whether what is enabled?",
        "Y": "cuDNN",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Which bool controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Am",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does Abool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions",
        "Y": "SeeTensorFloat-32(TF32) on Ampere devices",
        "Z": "Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What type of algorithm is enabled in _algorithms_enabled()?",
        "Y": "deterministic",
        "Z": "Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does Aboolthat, if True, cause cuDNN to do?",
        "Y": "benchmark multiple convolution algorithms and select the fastest",
        "Z": "Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What type of algorithms are enabled by _algorithms_enabled()?",
        "Y": "deterministic",
        "Z": "Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What controls where TensorFloat-32 tensor cores may be used in cuDNN convolution?",
        "Y": "Aboolthat",
        "Z": "Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is built with MKL support?",
        "Y": "PyTorch",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aboolthat, if True, causes cuDNN to only use what convolution algorithms?",
        "Y": "deterministic",
        "Z": "Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does Aboolthat return?",
        "Y": "whether PyTorch is built with MKL support",
        "Z": "Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does aboolthat, if True, cause cuDNN to only use?",
        "Y": "deterministic convolution algorithms",
        "Z": "Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What type of support does PyTorch have?",
        "Y": "OpenMP",
        "Z": "Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What controls whether TensorFloat-32 tensor cores may be used in matrix multiplication on newer GPUs?",
        "Y": "Aboolthat",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does aint that controls the cache capacity of a cuFFT plan do?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in what?",
        "Y": "matrix multiplications",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is the name of the tensor cores used in matrix multiplications on Ampere devices?",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does Aintthat controls cache capacity of cuFFT plan do?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is built with OpenMP support?",
        "Y": "PyTorch",
        "Z": "Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "For what type of tensor does it compute the logical OR?",
        "Y": "bool tensors",
        "Z": "Computes the bitwise OR ofinputandother. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical OR. input\u2013 the first input tensor other\u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or"
    },
    {
        "X": "What is the second input tensor out(Tensor,optional) called?",
        "Y": "output tensor",
        "Z": "Computes the bitwise OR ofinputandother. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical OR. input\u2013 the first input tensor other\u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or"
    },
    {
        "X": "What does the one dimensional discrete Fourier transform ofinput. Compute the one dimensional inverse discrete Fourier transform ofinput",
        "Y": "Discrete Fourier transforms and related functions",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the one dimensional inverse discrete Fourier transform ofinput?",
        "Y": "2 dimensional inverse discrete Fourier transform ofinput",
        "Z": "Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the one dimensional discrete Fourier transform ofinput?",
        "Y": "N dimensional discrete Fourier transform ofinput",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does the N dimensional discrete Fourier transform ofinput compute?",
        "Y": "2 dimensional inverse discrete Fourier transform ofinput",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does the 2 dimensional inverse discrete Fourier transform ofinput compute?",
        "Y": "N dimensional discrete Fourier transform ofinput",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the N dimensional discrete Fourier transform ofinput?",
        "Y": "Computes the N dimensional discrete Fourier transform ofinput",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is one dimensional discrete Fourier transform ofinput?",
        "Y": "Discrete Fourier transforms",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes what ofinput?",
        "Y": "one dimensional inverse discrete Fourier transform",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the 2 dimensional discrete Fourier transform ofinput?",
        "Y": "Computes the 2 dimensional discrete Fourier transform ofinput",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the N dimensional inverse discrete Fourier transform ofinput?",
        "Y": "Computes the N dimensional discrete Fourier transform",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the name of the one dimensional inverse discrete Fourier transform ofinput?",
        "Y": "2 dimensional inverse discrete Fourier transform ofinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the one dimensional Fourier transform of real-valuedinput?",
        "Y": "N dimensional inverse discrete Fourier transform ofinput",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional Fourier transform of what?",
        "Y": "real-valuedinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does Computes the 2 dimensional discrete Fourier transform ofinput?",
        "Y": "Computes the 2 dimensional inverse discrete Fourier transform ofinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes what of real-valuedinput?",
        "Y": "one dimensional Fourier transform",
        "Z": "Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the name of the one dimensional Fourier transform of real-valuedinput?",
        "Y": "N dimensional inverse discrete Fourier transform ofinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What computes the one dimensional Fourier transform of real-valuedinput?",
        "Y": "inverse ofrfft()",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional inverse discrete Fourier transform ofinput.",
        "Y": "2 dimensional discrete Fourier transform",
        "Z": "Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does Computes the one dimensional inverse discrete Fourier transform ofinput?",
        "Y": "Computes the 2 dimensional inverse discrete Fourier transform ofinput",
        "Z": "Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does Computes the one dimensional Fourier transform of real-valuedinput?",
        "Y": "Computes the N dimensional inverse discrete Fourier transform ofinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional Fourier transform of real-valuedinput.",
        "Y": "inverse ofrfft()",
        "Z": "Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the name of the transform that is computed?",
        "Y": "2 dimensional discrete Fourier transform ofinput",
        "Z": "Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does the inverse ofrfft() compute?",
        "Y": "one dimensional Fourier transform of real-valuedinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does the inverse ofrfft2() compute?",
        "Y": "2-dimensional discrete Fourier transform of realinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does Computes the 2 dimensional inverse discrete Fourier transform ofinput?",
        "Y": "Computes the 2 dimensional inverse discrete Fourier transform ofinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional Fourier transform of real-valuedinput. Computes the 2-dimensional discrete Fourier transform of realin",
        "Y": "inverse ofrfft()",
        "Z": "Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the inverse ofrfft2().",
        "Y": "2-dimensional discrete Fourier transform of realinput",
        "Z": "Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the 2-dimensional discrete Fourier transform of realinput.",
        "Y": "inverse ofrfft2()",
        "Z": "Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the name of the two dimensional inverse discrete Fourier transform of realinput?",
        "Y": "N dimensional discrete Fourier transform ofinput",
        "Z": "Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional Fourier transform of real-valuedinput. Computes what?",
        "Y": "inverse ofrfft()",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the inverse ofrfft2(). Computes the inverse ofrfft2().",
        "Y": "2-dimensional discrete Fourier transform of realinput",
        "Z": "Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the N-dimensional discrete Fourier transform of what?",
        "Y": "realinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does Computes the N dimensional inverse discrete Fourier transform ofinput?",
        "Y": "Computes the N dimensional discrete Fourier transform ofinput",
        "Z": "Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the inverse ofrfft2(). Computes the inverse ofrfftn().",
        "Y": "2-dimensional discrete Fourier transform of realinput",
        "Z": "Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the 2-dimensional discrete Fourier transform of realinput. Computes the N-dimensional discrete Fourier transform of realin",
        "Y": "inverse ofrfft2()",
        "Z": "Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the N-dimensional discrete Fourier transform of realinput.",
        "Y": "inverse ofrfftn()",
        "Z": "Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does the inverse ofrfftn() compute?",
        "Y": "N-dimensional discrete Fourier transform of realinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the one dimensional discrete Fourier transform of?",
        "Y": "Hermitian symmetricinputsignal",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional Fourier transform of real-valuedinput. Computes the one dimensional Fourier transform of real-value",
        "Y": "inverse ofrfft()",
        "Z": "Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the inverse ofrfft2(). Computes the N-dimensional discrete Fourier transform of realinput.",
        "Y": "2-dimensional discrete Fourier transform of realinput",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the 2-dimensional discrete Fourier transform of realinput. Computes what?",
        "Y": "inverse ofrfft2()",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal. Computes what?",
        "Y": "inverse ofrfftn()",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional discrete Fourier transform of what?",
        "Y": "Hermitian symmetricinputsignal",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal?",
        "Y": "inverse ofrfftn()",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.",
        "Y": "inverse ofhfft()",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does the inverse ofrfft() do?",
        "Y": "one dimensional Fourier transform of real-valuedinput",
        "Z": "Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What dimension is the discrete Fourier transform of realinput?",
        "Y": "N-dimensional",
        "Z": "Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal. Computes the",
        "Y": "inverse ofrfftn()",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the discrete Fourier Transform sample frequencies for a signal of what?",
        "Y": "sizen",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the 2-dimensional discrete Fourier transform of realinput. Computes the inverse ofrfft2().",
        "Y": "inverse ofrfft()",
        "Z": "Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the inverse ofrfft2(). Computes the inverse ofrfftn(). Computes",
        "Y": "2-dimensional discrete Fourier transform of realinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the 2-dimensional discrete Fourier transform of realinput. Computes the one dimensional discrete Fourier transform of a",
        "Y": "inverse ofrfftn()",
        "Z": "Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional discrete Fourier transform of what symmetricinputsignal?",
        "Y": "Hermitian",
        "Z": "Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the discrete Fourier Transform sample frequencies for a signal of sizen. Computes the discrete Fourier Transform sample",
        "Y": "inverse ofhfft()",
        "Z": "Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes what for a signal of sizen?",
        "Y": "the discrete Fourier Transform sample frequencies",
        "Z": "Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the discrete Fourier transform of realinput?",
        "Y": "2-dimensional",
        "Z": "Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does the inverse ofhfft() compute?",
        "Y": "one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal",
        "Z": "Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the sample frequencies forrfft()with a signal of what?",
        "Y": "sizen",
        "Z": "Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What transform of realinput is computed?",
        "Y": "N-dimensional discrete Fourier transform",
        "Z": "Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the N-dimensional discrete Fourier transform of realinput. Computes what?",
        "Y": "inverse ofrfftn()",
        "Z": "Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the sample frequencies with a signal of sizen.",
        "Y": "forrfft()",
        "Z": "Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does a Hermitian symmetricinputsignal do?",
        "Y": "one dimensional discrete Fourier transform",
        "Z": "Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the N-dimensional discrete Fourier transform of realinput. Computes the one dimensional discrete Fourier transform of",
        "Y": "inverse ofrfft2()",
        "Z": "Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the sample frequencies forrfft() with a signal of what?",
        "Y": "sizen",
        "Z": "Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What provides n-dimensional FFT data to have negative frequency terms first?",
        "Y": "byfftn()",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What is the name of the function that reorders n-dimensional FFT data to have negative frequency terms first?",
        "Y": "Inverse offftshift()",
        "Z": "Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional discrete Fourier transform of a what symmetricinputsignal?",
        "Y": "Hermitian",
        "Z": "Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the discrete sample frequencies for a signal of sizen.",
        "Y": "Fourier Transform",
        "Z": "Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Reorders n-dimensional FFT data, as provided byfftn(), to have what first?",
        "Y": "negative frequency terms",
        "Z": "Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What function reorders n-dimensional FFT data to have negative frequency terms first?",
        "Y": "Inverse offftshift()",
        "Z": "Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the discrete sample frequencies for a signal of sizen?",
        "Y": "Fourier Transform",
        "Z": "Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What provides n-dimensional FFT data?",
        "Y": "byfftn()",
        "Z": "Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the sample frequencies with a signal of sizen?",
        "Y": "forrfft()",
        "Z": "Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.   Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.   Inverse offftshift().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What global function returns a tensor with dtypetorch.int64?",
        "Y": "dtype default",
        "Z": "The shape of the tensor is defined by the variable argumentsize. Note With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What is the default value of the function that returns a tensor filled with random integers?",
        "Y": "0. high(int)",
        "Z": "Returns a tensor filled with random integers generated uniformly\nbetweenlow(inclusive) and high(exclusive). The shape of the tensor is defined by the variable argumentsize. Note With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What variable defines the shape of the tensor?",
        "Y": "argumentsize",
        "Z": "The shape of the tensor is defined by the variable argumentsize. Note With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What is the default value for the lowest integer to be drawn from the distribution?",
        "Y": "0. high(int)",
        "Z": "With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What does torch.float32 return a tensor with?",
        "Y": "global dtype default",
        "Z": "Note With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What is a tuple defining the shape of the output tensor?",
        "Y": "size(tuple)",
        "Z": "The shape of the tensor is defined by the variable argumentsize. Note With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What does this function return with the global dtype default?",
        "Y": "a tensor with dtypetorch.int64",
        "Z": "Note With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "With the global dtype default (torch.float32), this function returns a tensor with what?",
        "Y": "dtypetorch.int64",
        "Z": "With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What is the global name of the function that returns a tensor with dtypetorch.int64?",
        "Y": "dtype default",
        "Z": "With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What does Torch.float32 return a tensor with?",
        "Y": "global dtype default",
        "Z": "With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "How many times above the highest integer to be drawn from the distribution is high(int)?",
        "Y": "One",
        "Z": "high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "The shapes of themasktensor and the input tensor don\u2019t need to match, but what?",
        "Y": "they must bebroadcastable",
        "Z": "Returns a new 1-D tensor which indexes the input tensor according to\nthe boolean maskmaskwhich is a Bool Tensor. The shapes of themasktensor and the input tensor don\u2019t need\nto match, but they must bebroadcastable. Note The returned tensor doesnotuse the same storage\nas the original tensor input(Tensor) \u2013 the input tensor. mask(BoolTensor) \u2013 the tensor containing the binary mask to index with out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select"
    },
    {
        "X": "The returned tensor doesnotuse the same storage as the original tensor?",
        "Y": "input(Tensor)",
        "Z": "Returns a new 1-D tensor which indexes the input tensor according to\nthe boolean maskmaskwhich is a Bool Tensor. The shapes of themasktensor and the input tensor don\u2019t need\nto match, but they must bebroadcastable. Note The returned tensor doesnotuse the same storage\nas the original tensor input(Tensor) \u2013 the input tensor. mask(BoolTensor) \u2013 the tensor containing the binary mask to index with out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select"
    },
    {
        "X": "What is returned that is filled with random numbers from a normal distribution with mean 0 and variance 1?",
        "Y": "a tensor with the same size as input",
        "Z": "Returns a tensor with the same size as inputthat is filled with\nrandom numbers from a normal distribution with mean 0 and variance 1.torch.randn_like(input)is equivalent totorch.randn(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randn_like.html#torch.randn_like"
    },
    {
        "X": "What does dtype(torch.dtype, optional) return?",
        "Y": "the desired data type",
        "Z": "Returns a tensor with the same size as inputthat is filled with\nrandom numbers from a normal distribution with mean 0 and variance 1.torch.randn_like(input)is equivalent totorch.randn(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype ofinput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randn_like.html#torch.randn_like"
    },
    {
        "X": "What does if None do?",
        "Y": "defaults to the dtype ofinput",
        "Z": "Returns a tensor with the same size as inputthat is filled with\nrandom numbers from a normal distribution with mean 0 and variance 1.torch.randn_like(input)is equivalent totorch.randn(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype ofinput.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randn_like.html#torch.randn_like"
    },
    {
        "X": "What does if None default to?",
        "Y": "defaults to the device ofinput",
        "Z": "Returns a tensor with the same size as inputthat is filled with\nrandom numbers from a normal distribution with mean 0 and variance 1.torch.randn_like(input)is equivalent totorch.randn(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randn_like.html#torch.randn_like"
    },
    {
        "X": "What is the PyTorch Timer based on?",
        "Y": "timeit.Timer",
        "Z": "Helper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a significant confounding factor when measuring code?",
        "Y": "run-to-run variation",
        "Z": "For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is median computation?",
        "Y": "more robust than mean",
        "Z": "Helper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does the PyTorch Timer deviate from?",
        "Y": "thetimeitAPI",
        "Z": "For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Where are exact algorithms discussed in the PyTorch Timer?",
        "Y": "method docstrings",
        "Z": "For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is replicated for cases where an adaptive strategy is not desired?",
        "Y": "The timeit method",
        "Z": "Helper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Where are exact algorithms discussed?",
        "Y": "method docstrings",
        "Z": "For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "The timeit method is replicated for cases where what is not desired?",
        "Y": "adaptive strategy",
        "Z": "For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is important because some elements of PyTorch are lazily initialized?",
        "Y": "warmups",
        "Z": "Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does Timer deviate from?",
        "Y": "timeitAPI",
        "Z": "Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What class group and display results for comparison?",
        "Y": "theCompareclass",
        "Z": "When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What class does this class deviate from?",
        "Y": "timeitAPI",
        "Z": "When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What fields can one optionally define when defining a Timer?",
        "Y": "specifylabel,sub_label,description, andenv",
        "Z": "When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Timer specific constructor arguments: stmt,setup,timer,globals, what?",
        "Y": "PyTorch",
        "Z": "When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is used to group and display results for comparison?",
        "Y": "theCompareclass",
        "Z": "When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What are PyTorch Timer specific constructor arguments?",
        "Y": "stmt,setup,timer,globals",
        "Z": "stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is setup code used to define variables used in PyTorch?",
        "Y": "instmt global_setup",
        "Z": "In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What language does global_setup come from?",
        "Y": "C++",
        "Z": "stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Timer\u2013 Callable which returns what?",
        "Y": "current time",
        "Z": "global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What string summarizes stmt?",
        "Y": "label",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Optional setup code is used to define variables used in PyTorch?",
        "Y": "instmt global_setup",
        "Z": "setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the constructor that describes PyTorch?",
        "Y": "sub_label",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Code snippet to be run in a loop and timed?",
        "Y": "stmt",
        "Z": "stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the document that describes PyTorch?",
        "Y": "description",
        "Z": "global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What happens if PyTorch was built without GPU or there is no GPU present?",
        "Y": "synchronize CUDA",
        "Z": "timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a description of PyTorch?",
        "Y": "description",
        "Z": "timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Why would one set label to \"ReLU(x + 1)\"?",
        "Y": "improve readability",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What might a sub_label be?",
        "Y": "float",
        "Z": "stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How would one set description based on the input size to create a table of the form?",
        "Y": "usingCompare",
        "Z": "label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a way to create a table of the form?",
        "Y": "usingCompare",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the sub_label in our example?",
        "Y": "float",
        "Z": "Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "When is description also included?",
        "Y": "when printing a Measurement",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the default size of the PyTorch threadpool?",
        "Y": "one",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the principal use of description to signal toComparethe columns of data?",
        "Y": "usingCompare",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How does PyTorch measure many replicates?",
        "Y": "keeping timer overhead to a minimum",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How does block_autorange measure many replicates?",
        "Y": "keeping timer overhead to a minimum",
        "Z": "Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does blocked_autorange execute in the inner loop?",
        "Y": "variableblock_size",
        "Z": "At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What better amortizes the cost oftimerinvocation?",
        "Y": "A large block size",
        "Z": "A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Block_size is set by running a warmup period, increasing block_size until timer overhead is less than what percentage of the overall computation?",
        "Y": "0.1%",
        "Z": "Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is blocked_autorange's block_size used for?",
        "Y": "main measurement loop",
        "Z": "At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does a small block size result in?",
        "Y": "better statistics",
        "Z": "Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How does blocked_autorange set block_size?",
        "Y": "running a warmup period",
        "Z": "At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Unlike wall times, instruction counts are what?",
        "Y": "deterministic",
        "Z": "Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Instruction counts are deterministic, which makes them ideal for what?",
        "Y": "detailed performance analysis",
        "Z": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What must be installed in order to use this methodvalgrind?",
        "Y": "callgrind_control, andcallgrind_annotate",
        "Z": "Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the measurement object that can be used to compute statistics?",
        "Y": "median",
        "Z": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How are instruction counts different from wall times?",
        "Y": "deterministic",
        "Z": "Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What are instruction counts ideal for?",
        "Y": "detailed performance analysis",
        "Z": "Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What can globalsnot contain?",
        "Y": "arbitrary in-memory data structures",
        "Z": "In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What are globals restricted to?",
        "Y": "builtins,nn.Modules\u2019s, and Torch Scripted functions/modules",
        "Z": "In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What class provides more detail on the subject of globals?",
        "Y": "TheGlobalsBridgeclass",
        "Z": "Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What rely on pickle and you may need to add an import tosetup for them to transfer properly?",
        "Y": "nn.Modules",
        "Z": "In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit().",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What can globals not contain?",
        "Y": "arbitrary in-memory data structures",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What do nn.Modules rely on?",
        "Y": "pickle",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Execute what?",
        "Y": "main statement (stmt)numbertimes",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does timeit.Timer.timeit provide downstream consumers?",
        "Y": "several convenience methods",
        "Z": "By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What method is used for merging replicates?",
        "Y": "Convenience method",
        "Z": "In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does Merge extrapolate times tonumber_per_run=1 and not transfer any metadata?",
        "Y": "Approximate significant figure estimate",
        "Z": "By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What convenience method does this class provide for downstream consumers?",
        "Y": "__repr__",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What property is intended to give a convenient way to estimate the precision of a measurement?",
        "Y": "Approximate significant figure estimate",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Approximate significant figure estimate is intended to give a convenient way what?",
        "Y": "to estimate the precision of a measurement",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the static z value of this property?",
        "Y": "1.645",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the static z value of timeit.Timer.timeit()?",
        "Y": "1.645",
        "Z": "Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does saving and loading tensors do?",
        "Y": "Saves an object to a disk file",
        "Z": "Saves an object to a disk file. See also:Saving and loading tensors obj\u2013 saved object f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note The 1.6 release of PyTorch switchedtorch.saveto use a new\nzipfile-based file format.torch.loadstill retains the ability to\nload files in the old format. If for any reason you wanttorch.saveto use the old format, pass the kwarg_use_new_zipfile_serialization=False. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What does saving and loading tensors preserve?",
        "Y": "views",
        "Z": "Saves an object to a disk file. See also:Saving and loading tensors obj\u2013 saved object f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note The 1.6 release of PyTorch switchedtorch.saveto use a new\nzipfile-based file format.torch.loadstill retains the ability to\nload files in the old format. If for any reason you wanttorch.saveto use the old format, pass the kwarg_use_new_zipfile_serialization=False. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What is an example of a file-like object that can be saved to a disk file?",
        "Y": "Example",
        "Z": "Saves an object to a disk file. See also:Saving and loading tensors obj\u2013 saved object f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note The 1.6 release of PyTorch switchedtorch.saveto use a new\nzipfile-based file format.torch.loadstill retains the ability to\nload files in the old format. If for any reason you wanttorch.saveto use the old format, pass the kwarg_use_new_zipfile_serialization=False. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What is the behavior similar to python\u2019sitertools.combinationswhenwith_replacementis set toFals",
        "Y": "Compute combinations of lengthrrrof the given tensor",
        "Z": "Compute combinations of lengthrrrof the given tensor. The behavior is similar to\npython\u2019sitertools.combinationswhenwith_replacementis set toFalse, anditertools.combinations_with_replacementwhenwith_replacementis set toTrue. input(Tensor) \u2013 1D vector. r(int,optional) \u2013 number of elements to combine with_replacement(boolean,optional) \u2013 whether to allow duplication in combination A tensor equivalent to converting all the input tensors into lists, doitertools.combinationsoritertools.combinations_with_replacementon these\nlists, and finally convert the resulting list into tensor. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations"
    },
    {
        "X": "Compute combinations of lengthrrrof the given tensor. The behavior is similar to what?",
        "Y": "python",
        "Z": "Compute combinations of lengthrrrof the given tensor. The behavior is similar to\npython\u2019sitertools.combinationswhenwith_replacementis set toFalse, anditertools.combinations_with_replacementwhenwith_replacementis set toTrue. input(Tensor) \u2013 1D vector. r(int,optional) \u2013 number of elements to combine with_replacement(boolean,optional) \u2013 whether to allow duplication in combination A tensor equivalent to converting all the input tensors into lists, doitertools.combinationsoritertools.combinations_with_replacementon these\nlists, and finally convert the resulting list into tensor. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations"
    },
    {
        "X": "What does input(Tensor) mean?",
        "Y": "input(Tensor) \u2013 1D vector",
        "Z": "Compute combinations of lengthrrrof the given tensor. The behavior is similar to\npython\u2019sitertools.combinationswhenwith_replacementis set toFalse, anditertools.combinations_with_replacementwhenwith_replacementis set toTrue. input(Tensor) \u2013 1D vector. r(int,optional) \u2013 number of elements to combine with_replacement(boolean,optional) \u2013 whether to allow duplication in combination A tensor equivalent to converting all the input tensors into lists, doitertools.combinationsoritertools.combinations_with_replacementon these\nlists, and finally convert the resulting list into tensor. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations"
    },
    {
        "X": "For backwards compatibility, not providing a value forstepswill what?",
        "Y": "create a tensor with 100 elements",
        "Z": "Warning Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What does torch.get_default_dtype() use?",
        "Y": "global default dtype",
        "Z": "Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What is a tutorial about packaging a Torch Script module?",
        "Y": "first model",
        "Z": "Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a Torch Script module used for?",
        "Y": "Patch code into a package",
        "Z": "Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you do from a package?",
        "Y": "Access package contents",
        "Z": "Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you do to package a Torch Script module?",
        "Y": "Package a Torch Script module",
        "Z": "Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How do I know if a package is executing?",
        "Y": "Test in my source code",
        "Z": "How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How do I access package contents from packaged code?",
        "Y": "Patch code into a package",
        "Z": "How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do I do from a package?",
        "Y": "Access package contents",
        "Z": "How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the purpose of a Torch package?",
        "Y": "See what is inside a package",
        "Z": "See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can I include with my package and access them later?",
        "Y": "arbitrary resources",
        "Z": "See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can I do with a Torch package?",
        "Y": "Customize how a class is packaged",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can I do to a package?",
        "Y": "Patch code into a package",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a package contain?",
        "Y": "Package a Torch Script module",
        "Z": "See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the module that is included in a package?",
        "Y": "Package a Torch Script module",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a way to test if a class is executing inside a package?",
        "Y": "Patch code into a package",
        "Z": "Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a way to test if a package is executing inside a package?",
        "Y": "Patch code into a package",
        "Z": "Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can I do from packaged code?",
        "Y": "Access package contents",
        "Z": "Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a Torch Script module do?",
        "Y": "Package a Torch Script module",
        "Z": "Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Torch distinguish between?",
        "Y": "packaged code and non-packaged code",
        "Z": "Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is one way to distinguish between packaged code and non-packaged code?",
        "Y": "Re-export an imported object",
        "Z": "Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you do to differentiate between packaged code and non-packaged code?",
        "Y": "Package a Torch Script module",
        "Z": "Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What defines your code's dependencies?",
        "Y": "torch.packageFormat",
        "Z": "Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the ZIP format?",
        "Y": "The container format for a torch.package",
        "Z": "Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the package that Torch Script modules use?",
        "Y": "torch.packageFormat",
        "Z": "Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you do with a Torch Script module?",
        "Y": "edit files and :writethem back into the archive",
        "Z": "Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will you be familiar with after completing this tutorial?",
        "Y": "basic API",
        "Z": "torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will return a printable and queryableFolderobject?",
        "Y": "afile_structure()method",
        "Z": "Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you query Folder objects with?",
        "Y": "the has_file()method",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Package Exporter exposes three methods that allow you to save Python objects, text, and binary data to a package.",
        "Y": "save_pickle,save_textandsave_binary",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Package Exporterexposes three methods that allow you to save Python objects, text, and binary data to a package.",
        "Y": "save_pickle,save_textandsave_binary",
        "Z": "The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What exposes complementary methods that allow you to load Python objects, text and binary data from a package?",
        "Y": "Package Importer",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can vim do with ZIP archives?",
        "Y": "edit files and :writethem back into the archive",
        "Z": "vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are the three methods that Package Exporter exposes?",
        "Y": "save_pickle,save_textandsave_binary",
        "Z": "vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are the three methods that allow you to save Python objects, text, and binary data to a package?",
        "Y": "save_pickle,save_textandsave_binary",
        "Z": "Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Package Importerexposes complementary methods called what?",
        "Y": "load_pickle",
        "Z": "Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How is torch.package accessed?",
        "Y": "defining the method__reduce_package__on a class and by defining a corresponding de-packaging function",
        "Z": "You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the step in the Python pickling process?",
        "Y": "Steps",
        "Z": "You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you do with the has_file() method?",
        "Y": "query Folder objects",
        "Z": "You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Define the method__reduce_package__(self,exporter:Package Exporter) on what?",
        "Y": "the target class",
        "Z": "Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who calls this method when it encounters an instance of the target class?",
        "Y": "the Package Exporter",
        "Z": "Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps: Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the Package Exporter define for the class?",
        "Y": "a de-packaging function",
        "Z": "Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should do the work to reconstruct and return an instance of the class?",
        "Y": "de-packaging function",
        "Z": "Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the code behave differently depending on?",
        "Y": "whether it\u2019s imported normally through your Python environment or imported from a torch.package",
        "Z": "Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "In general, it's bad practice to have code that behaves differently depending on what?",
        "Y": "whether it\u2019s packaged or not",
        "Z": "Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is it bad practice to have code that behaves differently depending on whether it's packaged or not?",
        "Y": "behaves differently",
        "Z": "Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What offers asave_source_string() method that allows one to save arbitrary Python source code to a module of your choosing?",
        "Y": "Package Exporter",
        "Z": "Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the package that a Package Importer imports code from?",
        "Y": "a torch.package",
        "Z": "a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What files are placed in the.data/?",
        "Y": "Framework files",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is everything else in the.data/ directory?",
        "Y": "User files",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who owns the.data/directory of a fully packaged ResNet model?",
        "Y": "torch.package",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a list of modules that are consideredextern:class:Package Importer.externmodules?",
        "Y": "module",
        "Z": "extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Why should you consult this essay for a deeper dive in how Python packaging works?",
        "Y": "it\u2019s slightly out of date",
        "Z": "Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you check the implementation details of Python packages?",
        "Y": "Python reference documentation",
        "Z": "extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is stored in the *.storage?",
        "Y": "serialized tensor data",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you check the implementation details of a Python package?",
        "Y": "Python reference documentation",
        "Z": "extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What standard forms does Torch.package look for when a Python module is identified as a dependency?",
        "Y": "fromximporty,importz,fromwimportvasu",
        "Z": "When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When import statements are encountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same walking way",
        "Y": "AST",
        "Z": "When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are the standard forms of import statements that Torch.package looks for when a Python module is identified as a dependency?",
        "Y": "fromximporty,importz,fromwimportvasu",
        "Z": "In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When import statements are encountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in what walking way?",
        "Y": "AST",
        "Z": "When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does AST parsing have limited support for?",
        "Y": "the__import__(...)syntax",
        "Z": "In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are the standard forms of import statements that torch.package looks for?",
        "Y": "fromximporty,importz,fromwimportvasu",
        "Z": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What walking way does torch.package parse dependencies in?",
        "Y": "AST",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What has limited support for the__import__(...)syntax and does not supportimportlib.import_module",
        "Y": "AST parsing",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should not be detected by torch.package?",
        "Y": "dynamic imports",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the process that torch.packageautomatically finds the Python modules that your code and objects depend on?",
        "Y": "dependency resolution",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "AST parsing has limited support for what?",
        "Y": "the__import__(...)syntax",
        "Z": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should not be detected bytorch.package?",
        "Y": "dynamic imports",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What action stubs out a module?",
        "Y": "mock",
        "Z": "The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does depending on this module do during package export?",
        "Y": "raise an error",
        "Z": "Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an action that the dependency resolver can take?",
        "Y": "declare this module as an external dependency of the package",
        "Z": "torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens when you depend on a module?",
        "Y": "raise an error",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are actions defined on?",
        "Y": "actions are only defined on entire Python modules",
        "Z": "torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Is there a way to package just a function or class from module and leave the rest out?",
        "Y": "There is no way to package \u201cjust\u201d a function or class from module and leave the rest out",
        "Z": "torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will dependencies on a module do during package export?",
        "Y": "raise an error",
        "Z": "extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Python does not offer clean boundaries between objects defined in what?",
        "Y": "module",
        "Z": "extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What action removes or changes dependencies in your code?",
        "Y": "Refactoring",
        "Z": "extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Actions are applied to modules using what?",
        "Y": "patterns",
        "Z": "mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens when a module is placed into the package?",
        "Y": "a module is intern-ed",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is included in a package if a module is interned?",
        "Y": "model code",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you find a list of external dependencies for a package?",
        "Y": "package_exporter.extern_modules",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will be raised if Package Importer can't find a module?",
        "Y": "an error",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you depend on from within your package without having to package them too?",
        "Y": "third-party libraries",
        "Z": "In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If you need long-term reproducibility for your package, try to what?",
        "Y": "limit your use of extern",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If any external library changes in a way, your package may fail to load.",
        "Y": "backwards-incompatible",
        "Z": "Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is code that you \"know\" will not be needed in the loaded package?",
        "Y": "initialization/configuration code",
        "Z": "Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is not smart enough to tell that unused imports are indeed unused?",
        "Y": "The dependency resolver",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should be used as a last resort?",
        "Y": "mock",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should we not leave in our code?",
        "Y": "Do not leave unused imports",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Instead of writing import foo and later usingfoo.bar.baz, prefer to write import foo and later using what?",
        "Y": "writefromfoo.barimportbaz",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the dependency resolver do to make sure you don't leave unused imports in your code?",
        "Y": "Qualify your imports",
        "Z": "In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How do you split up large files with unrelated functionality into smaller ones?",
        "Y": "Split up large files with unrelated functionality into smaller ones",
        "Z": "Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does yourutilsmodule contain?",
        "Y": "a hodge-podge of unrelated functionality",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can be: a list of segments separated by a separator string?",
        "Y": "Segments",
        "Z": "Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does thetorch.packageimport use to import externalmodules?",
        "Y": "the loading environment\u2019s system importer",
        "Z": "The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are some examples of standard import statements?",
        "Y": "fromximporty,importz,fromwimportvasu",
        "Z": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you do instead of pulling in lots of unrelated dependencies?",
        "Y": "define single-purpose modules that can be packaged independently of one another",
        "Z": "Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of string matches any string, including the empty string?",
        "Y": "wildcard",
        "Z": "A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When specifying actions, you can pass what patterns?",
        "Y": "multiple",
        "Z": "A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When specifying actions, you can pass multiple patterns, e.g. a module will match against what action?",
        "Y": "if it matches any of the patterns",
        "Z": "Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the module that matchestorchand all its submodules?",
        "Y": "torch",
        "Z": "A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How will a module match against an action?",
        "Y": "if it matches any of the patterns",
        "Z": "A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you specify patterns to?",
        "Y": "exlcude",
        "Z": "A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is code that you \"know\" will not be needed in the loaded package but still want to available for use in non-packaged contents?",
        "Y": "initialization/configuration code",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should mock be used as?",
        "Y": "last resort",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What language makes it really easy to bind objects and run code at module-level scope?",
        "Y": "Python",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state. Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is quite useful when you define an object at module scope with the intention of mutating it?",
        "Y": "Mutable global state",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state. Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors. Any class that you import from a Package Importerwill be a version of the class specific to that importer. For example:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can mutable global state cause when used withtorch.package?",
        "Y": "it can cause complications",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state. Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are some benefits of mutable global state?",
        "Y": "it can reduce boilerplate, allow for open registration into tables",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state. Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does eachPackage Importerinstance create?",
        "Y": "Modules in a package can only import other packaged modules, or modules markedextern",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Importer provide the same core API as?",
        "Y": "the import libimporter",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens when you invokePackage Importer.import_module()?",
        "Y": "Package Importerwill construct and return a new module",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does thatPackage Importerinstancepatches the returned module to?",
        "Y": "use self",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How does thatPackage Importerinstance fulfill future import requests?",
        "Y": "by looking in the package rather than searching the user\u2019s Python environment",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is used to extend Python's import infrastructure?",
        "Y": "custom importer",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Importer invoke?",
        "Y": "Package Importer.import_module()",
        "Z": "This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the module use self(i.e. thatPackage Importerinstance) do instead of searching the user\u2019s Python environment?",
        "Y": "looking in the package",
        "Z": "When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What prefix does Package Importermangles the__name__and__file__of all imported modules?",
        "Y": "a mangle",
        "Z": "This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a name for a module?",
        "Y": "liketorchvision.models.resnet18",
        "Z": "When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What prefix does Package Importermangles the__name__and__file__of all imported modules by adding?",
        "Y": "a mangle",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What prefix does Package Importermangles add to all imported modules?",
        "Y": "a mangle",
        "Z": "To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What becomes torch_package_0>.torchvision/modules/resnet18.py?",
        "Y": "liketorchvision/models/resnet18.py",
        "Z": "To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What helps avoid inadvertent punning of module names between different packages?",
        "Y": "Name mangling",
        "Z": "To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the file format of a PyTorch model code?",
        "Y": "specially organized zip file",
        "Z": "This exception is raised when there is an issue with exporting a package.Package Exporterwill attempt to gather up all the errors and present\nthem to you at once. This is an exception that is thrown when a mock or extern is marked as allow_empty=False, and is not matched with any module during packaging. Exporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package. Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning. The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can future users of a package do?",
        "Y": "unzip the package, and edit the code in order to perform custom modifications to it",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What allows you to write packages of code, pickled Python data, and arbitrary binary and text resources into a self-contained package?",
        "Y": "Exporters",
        "Z": "Exporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package. Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning. The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the file format of PyTorch code?",
        "Y": "specially organized zip file",
        "Z": "Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning. The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When does a package run locally because it is importing a locally-installed package fail?",
        "Y": "when the package is copied to another machine",
        "Z": "Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning. The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How can future users of a package edit the code in order to perform custom modifications to it?",
        "Y": "unzip",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the fileextern_modulesin the zip archive list?",
        "Y": "all the modules that a package externally depends on",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Why does the fileextern_modulesin the zip archive prevent \u201cimplicit\u201d dependencies where the package runs locally?",
        "Y": "it is importing a locally-installed package",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who can optionally scan source code when source code is added to the package?",
        "Y": "the exporter",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is added to a package, the exporter can optionally scan it for further code dependencies?",
        "Y": "source code",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If an importer is passed, use that to search for modules. If a sequence of importers are passedsed, anOrderedIm",
        "Y": "a single Importer",
        "Z": "When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a sequence of importers are passed, what will be constructed out of them?",
        "Y": "an Ordered Importer",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the file to export to?",
        "Y": "Create an exporter",
        "Z": "Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can be used to create an exporter?",
        "Y": "a string/Path object",
        "Z": "Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If what is passed, use that to search for modules?",
        "Y": "a single Importer",
        "Z": "Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the package have to do?",
        "Y": "Write the package to the filesystem",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens to calls afterclose()?",
        "Y": "Any calls afterclose()are now invalid",
        "Z": "verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a dependency on any matching packages is found, what is raised?",
        "Y": "aPackagingErroris",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is include(Union[List[str],str]) a string e.g. \"my_package.",
        "Y": "list of strings for the names of the modules to be externed",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a single Importer is passed, use that to search for modules?",
        "Y": "importer",
        "Z": "importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the package need to do?",
        "Y": "Write the package to the filesystem",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Blocklist modules whose names match what from the list of modules the package can import?",
        "Y": "glob patterns",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can include(Union[List[str],str]] - A string e.g. \"my_package",
        "Y": "glob-style pattern",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Includemodulein the list of external modules the package can import. This will prevent what from saving it in the package?",
        "Y": "dependency discovery",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is included(Union[List[str],str])?",
        "Y": "list of strings for the names of the modules to be externed",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can include(Union[List[str],str]] \u2013 A string e.g. \"my_package",
        "Y": "glob-style pattern",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can also be a glob-style pattern, as described inmock()?",
        "Y": "include",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does include(Union[List[str],str]] mean?",
        "Y": "list of strings for the names of the modules to be externed",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will includemodulein prevent from saving in the package?",
        "Y": "dependency discovery",
        "Z": "Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What flag specifies whether the extern modules specified by this call to theexternmethod must be matched to some module during packaging?",
        "Y": "allow_empty(bool)",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if an extern module glob pattern is added with allow_empty=False?",
        "Y": "an exception is thrown",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a string e.g. \"my_package.my_subpackage\" or list of strings for the names of the",
        "Y": "include",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How often is an id guaranteed to be handed out for a package?",
        "Y": "once",
        "Z": "Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does get an id for a package?",
        "Y": "Specify modules that should be packaged",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If an extern module glob pattern is added with allow_empty=False, andclose()is called (either explicitly or via_",
        "Y": "If allow_empty=True",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a string e.g. \u201cmy_package.my_subpackage\u201d or list of strings for the names of the",
        "Y": "include",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does this id do?",
        "Y": "Specify modules that should be packaged",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What must be specified in order to be included in the package?",
        "Y": "modules that should be packaged",
        "Z": "Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Replace some required modules with what?",
        "Y": "mock implementation",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How do dependency resolution find files that are imported by model files but whose functionality is never used?",
        "Y": "file-by-file",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Use this function to do what without having to modify the original code?",
        "Y": "mock this functionality out",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is another name for union[List[str],str])?",
        "Y": "include",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How does dependency resolution find files that are imported by model files but whose functionality is never used?",
        "Y": "file-by-file",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the function that allows you to mock a module without having to modify the original code?",
        "Y": "include(Union[List[str],str])",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a mock is added with allow_empty=False, andclose()is called and the mock has not been matched to",
        "Y": "If allow_empty=True",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does it do when a module matches against anextern()pattern?",
        "Y": "Registers an extern hook on the exporter",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How are hooks called on the exporter?",
        "Y": "in order of registration",
        "Z": "Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How will hooks be called?",
        "Y": "in order of registration",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a handle that can be used to remove the added hook?",
        "Y": "callinghandle.remove()",
        "Z": "Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a mock hook called each time a module matches against?",
        "Y": "amock()pattern",
        "Z": "Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When will hooks be called on the exporter?",
        "Y": "in order of registration",
        "Z": "Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What saves raw bytes to the package?",
        "Y": "torch.utils.hooks.RemovableHandle",
        "Z": "The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a mock hook match against?",
        "Y": "amock()pattern",
        "Z": "torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When will hooks be called?",
        "Y": "in order of registration",
        "Z": "The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the module package this resource should go it?",
        "Y": "package(str)",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a unique name for the resource, used to identify it to load?",
        "Y": "resource(str)",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the data to save?",
        "Y": "binary(str)",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How many bytes does torch.utils.hooks.RemovableHandle save to the package?",
        "Y": "bytes",
        "Z": "A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the code to save?",
        "Y": "Save the code formoduleinto the package",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Code for the module is resolved using what?",
        "Y": "the importers path",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the module that will be saved to provide code for this package?",
        "Y": "module_name(str)",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can be used to remove the added hook by callinghandle.remove()?",
        "Y": "handle",
        "Z": "A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the data to save?",
        "Y": "binary(str)",
        "Z": "A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is returned when a tensor is filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)?",
        "Y": "tensor with the same shape",
        "Z": "Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns what with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive",
        "Y": "a tensor",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What quantiles of each row of the input tensor along the dimensiondim?",
        "Y": "q-th",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns what value, ignoring NaN values?",
        "Y": "the median of the values ininput",
        "Z": "Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Ifunbiasedis True, Bessel's correction will be used to calculate what?",
        "Y": "standard deviation",
        "Z": "Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   Ifunbiasedis True, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is returned when a named tuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the",
        "Y": "the cumulative product of elements ofinputin the dimensiondim",
        "Z": "Returns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.   Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What does the named tuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim",
        "Y": "the cumulative product of elements ofinputin the dimensiondim",
        "Z": "Do cartesian product of the given sequence of tensors.   Computes batched the p-norm distance between each pair of the two collections of row vectors.   Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    If input is a vector (1-D tensor), then returns a 2-D square tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "If input is a vector, then returns a 2-D square tensor Creates a tensor whose diagonals",
        "Y": "1-D tensor",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the default value for PyTorch Profiler?",
        "Y": "ProfilerActivity.CPU",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Activities(iterable) \u2013 what to use in profiling?",
        "Y": "list of activity groups",
        "Z": "An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the default value for ProfilerActivity.CUDA?",
        "Y": "ProfilerActivity.CPU",
        "Z": "Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are the default values for ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA?",
        "Y": "ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA",
        "Z": "An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is on_trace_ready?",
        "Y": "callable",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What bool is used to record source information for the ops?",
        "Y": "with_stack",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What bool tracks tensor memory allocation/deallocation?",
        "Y": "profile_memory",
        "Z": "record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the file and line number in a bool?",
        "Y": "with_stack",
        "Z": "record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is deprecated since version 1.8.1:useactivitiesinstead?",
        "Y": "use_cuda",
        "Z": "record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What command is used to generate result files for TensorBoard?",
        "Y": "tensorboard--logdirdir_name",
        "Z": "record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What function is deprecated since version 1.8.1?",
        "Y": "use_cuda",
        "Z": "with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Usetensorboard_trace_handler() to generate result files for TensorBoard?",
        "Y": "on_trace_ready",
        "Z": "with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are examples of schedules?",
        "Y": "profiler\u2019sschedule,on_trace_readyandstepfunctions",
        "Z": "with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Using the profiler\u2019sschedule,on_trace_readyandstepfunctions adds a user defined metadata with what",
        "Y": "a string key and a string value",
        "Z": "use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does Usetensorboard_trace_handler() generate?",
        "Y": "result files",
        "Z": "Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the metric to use for self_cpu_time_total or self_cuda_time_total?",
        "Y": "metric",
        "Z": "After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does Python Profiler TensorBoard plugin use?",
        "Y": "tensorboard",
        "Z": "tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is used to save stacks file to this location?",
        "Y": "path(str)",
        "Z": "Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are the members of Profiler actions that can be taken at the specified intervals?",
        "Y": "CPU CUDA",
        "Z": "Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is a member of a profiler that can be taken at the specified intervals?",
        "Y": "CPU CUDA",
        "Z": "metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the file where stack traces are saved?",
        "Y": "path(str)",
        "Z": "Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profilerschedulear",
        "Y": "Profiler actions",
        "Z": "Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does the profiler skip?",
        "Y": "firstskip_firststeps",
        "Z": "Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What grouping of events does perf_viz.svg Averages use?",
        "Y": "profiler.stacks",
        "Z": "./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the directory delivered to tensorboard as?",
        "Y": "logdir",
        "Z": "Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "When dim is specified,torch.unique always sort the tensor at the beginning regardless of thesortargument.",
        "Y": "CUDA implementation and the CPU implementation",
        "Z": "This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What returns the counts for each unique element?",
        "Y": "return_counts",
        "Z": "Returns the unique elements of the input tensor. Note This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "When dim is specified,torch.unique always sort the tensor at the beginning regardless of thesortargument?",
        "Y": "CUDA implementation and the CPU implementation",
        "Z": "Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "Whether to return the counts for each unique element. dim(int) \u2013 the dimension to apply unique. if None, the unique of",
        "Y": "return_counts",
        "Z": "Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "When dim is specified, the tensor is always sort at the beginning regardless of thesortargument?",
        "Y": "CUDA implementation and the CPU implementation",
        "Z": "Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What function returns the indices for where elements in the original input ended up in the returned unique list?",
        "Y": "return_inverse",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the name of the function that returns a single tensor?",
        "Y": "inverse_indices",
        "Z": "input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is an iterable containing the parameters (all should beVariables) to optimize?",
        "Y": "an Optimizer",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are some options that can be specified by an Optimizer?",
        "Y": "learning rate, weight decay",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "When do you need to move a model to GPU via.cuda()?",
        "Y": "before constructing optimizers",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What will the parameters update based on?",
        "Y": "computed gradients",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are some options you can specify in an Optimizer?",
        "Y": "learning rate, weight decay",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "If you need to do what before constructing optimizers for a model, please do so before constructing optimizers for it.",
        "Y": "move a model to GPU via.cuda()",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What should each iterable ofdicts contain?",
        "Y": "aparamskey",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What do you need to do to make sure that optimized parameters live in consistent locations when optimizers are constructed?",
        "Y": "Note",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Options as keyword arguments will be used as what?",
        "Y": "defaults",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "When are keyword arguments used as defaults?",
        "Y": "when you only want to vary a single option, while keeping all others consistent between parameter groups",
        "Z": "Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Parameters of a model will be different objects with those before the call?",
        "Y": "after.cuda()",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What can you still pass options as?",
        "Y": "keyword arguments",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What do you want to specify when you only want to vary a single option, while keeping all others consistent between parameter groups?",
        "Y": "per-layer learning rates",
        "Z": "In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Options as keyword arguments will be used as what in the groups that didn't override them?",
        "Y": "defaults",
        "Z": "Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Who supports specifying per-parameter options?",
        "Y": "Optimizers",
        "Z": "Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What can still be passed as keyword arguments?",
        "Y": "options",
        "Z": "You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Options will be used as what in the groups that didn't override them?",
        "Y": "defaults",
        "Z": "You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "The function can be called once the gradients are computed using what?",
        "Y": "e.g.backward()",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What class's parameters will use a learning rate of1e-3?",
        "Y": "model.classifier",
        "Z": "For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "All optimizers implement what method that updates the parameters?",
        "Y": "a step()method",
        "Z": "All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What class is used for all optimizers?",
        "Y": "Base class",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What class's parameters will use a learning rate of 1e-3?",
        "Y": "model.classifier",
        "Z": "This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is used for all optimizers?",
        "Y": "Base class",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What version of the function can be called once the gradients are computed using e.g.backward()?",
        "Y": "simplified",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Some optimization algorithms such as what need to reevaluate the function multiple times?",
        "Y": "Conjugate Gradient and LBFGS",
        "Z": "For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "The closure that allows them to reevaluate the function multiple times should clear the gradients, what should the closure do?",
        "Y": "compute the loss",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does Optimizer.add_param_group Add a param group to theOptimizersparam_groups?",
        "Y": "Optimizer.load_state_dict",
        "Z": "Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does Optimizer.zero_grad set the gradients of all optimizedtorch.Tensors to zero?",
        "Y": "Adadelta algorithm",
        "Z": "params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are examples of objects that don't satisfy the properties of parameters?",
        "Y": "sets and iterators over values of dictionaries",
        "Z": "Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements Adadelta algorithm?",
        "Y": "Adagrad algorithm",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is L-BFGS algorithm heavily inspired by?",
        "Y": "minFunc",
        "Z": "params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does Adamax algorithm implement?",
        "Y": "Averaged Stochastic Gradient Descent",
        "Z": "All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is implemented optionally with momentum?",
        "Y": "stochastic gradient descent",
        "Z": "Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What provides several methods to adjust the learning rate based on the number of epochs?",
        "Y": "torch.optim.lr_scheduler",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm is implemented?",
        "Y": "resilient backpropagation algorithm",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does Optimizer.step implement?",
        "Y": "Adam algorithm",
        "Z": "Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements the lazy version of Adam algorithm suitable for sparse tensors?",
        "Y": "AdamW algorithm",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm is heavily inspired by minFunc?",
        "Y": "L-BFGS algorithm",
        "Z": "Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements Adam algorithm?",
        "Y": "AdamW algorithm",
        "Z": "Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements the Adadelta algorithm?",
        "Y": "Adagrad",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What should be applied after optimizer's update?",
        "Y": "Learning rate scheduling",
        "Z": "Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What should be applied after the optimizer's update?",
        "Y": "Learning rate scheduling",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What do we use the following template to refer to?",
        "Y": "schedulers algorithms",
        "Z": "Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Prior to PyTorch 1.1.0, what was the learning rate scheduler expected to be called?",
        "Y": "before the optimizer\u2019s update",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the name of the learning rate scheduler?",
        "Y": "lr_scheduler.StepLR",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate scheduler called?",
        "Y": "callingscheduler.step()",
        "Z": "Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does the following template refer to?",
        "Y": "schedulers algorithms",
        "Z": "Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Prior to PyTorch 1.1.0, when was the learning rate scheduler expected to be called?",
        "Y": "before the optimizer\u2019s update",
        "Z": "Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does the following template refer to in PyTorch documentation?",
        "Y": "schedulers algorithms",
        "Z": "In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.StepLR Decay the learning rate of each parameter group by?",
        "Y": "gamma",
        "Z": "In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What language did 1.1.0 change the behavior of the learning rate scheduler?",
        "Y": "BC",
        "Z": "Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate of each parameter group set to?",
        "Y": "the initial lr times a given function",
        "Z": "Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What LR Multiply the learning rate of each parameter group by the factor given in the specified function?",
        "Y": "Multiplicative",
        "Z": "lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What Decays the learning rate of each parameter group by gamma every step_size epochs?",
        "Y": "StepLR",
        "Z": "lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What LR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of",
        "Y": "MultiStep",
        "Z": "lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What LR Decays the learning rate of each parameter group by gamma every epoch?",
        "Y": "Exponential",
        "Z": "lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "OneCycleLR Sets the learning rate of each parameter group according to what policy?",
        "Y": "1cycle learning rate policy",
        "Z": "lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does CosineAnnealingWarmRestarts do?",
        "Y": "CosineAnnealingWarmRestarts",
        "Z": "lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "MultiStepLR Decays the learning rate of each parameter group by what?",
        "Y": "gamma",
        "Z": "lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a learning rate scheduler?",
        "Y": "anneals the learning rate to a fixed value, and then keeps it constant",
        "Z": "Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does update_bn() assume each batch in the dataloaderloaderis?",
        "Y": "tensor",
        "Z": "update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does update_bn() assume each batch in the dataloaderloader is?",
        "Y": "tensors",
        "Z": "Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "By default,torch.optim.swa_utils.AveragedModelcomputes a what?",
        "Y": "running equal average of the parameters that you provide",
        "Z": "Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does the exampleema_modelcompute?",
        "Y": "an exponential moving average",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the model that accumulates the averages of the weights?",
        "Y": "SWA model",
        "Z": "Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How many epochs does the model train for?",
        "Y": "300",
        "Z": "Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a variant oftorch.quantile() that \u201cignores\u201dNaNvalues?",
        "Y": "fortorch.quantile()",
        "Z": "This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues,\ncomputing the quantilesqas ifNaNvalues ininputdid\nnot exist. If all values in a reduced row areNaNthen the quantiles for\nthat reduction will beNaN. See the documentation fortorch.quantile(). input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of quantile values in the range [0, 1] dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile"
    },
    {
        "X": "What does this function return of a real symmetric or complex Hermitian matrixinputor a batch thereof?",
        "Y": "eigenvalues and eigenvectors",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is used if upperis False?",
        "Y": "lower triangular portion",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What function is deprecated in favor oftorch.linalg.eigh()?",
        "Y": "torch.symeig()",
        "Z": "This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What should be replaced with torch.symeig()?",
        "Y": "L,_=torch.symeig(A,upper=upper)",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "If inputis what, then the eigenvalues of each matrix in the batch is returned in ascending order?",
        "Y": "a batch of matrices",
        "Z": "Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What does.stride() do?",
        "Y": "stridesV.contiguous().transpose(-1, -2)",
        "Z": "If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "The eigenvalues are returned in what order?",
        "Y": "ascending order",
        "Z": "Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "In what order are the eigenvalues in a named tuple?",
        "Y": "ascending",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What are the eigenvalues in ascending order?",
        "Y": "eigenvectors",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "If eigenvectors=False, what is it?",
        "Y": "empty tensor",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is an example of a (Tensor, Tensor)?",
        "Y": "Examples",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "Holds submodules in a list. Holds parameters in a list. Holds parameters in a dictionary.",
        "Y": "dictionary",
        "Z": "These are the basic building blocks for graphs: torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the basic building block for graphs?",
        "Y": "nn.Conv1d",
        "Z": "These are the basic building blocks for graphs: torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "A sequential container. Holds submodules in a list. Holds parameters in a dictionary. Holds submodules in",
        "Y": "Base class",
        "Z": "Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a global forward pre-hook common to all modules?",
        "Y": "Global Hooks For Module Registers a forward pre-hook common to all modules",
        "Z": "Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of hook does Global Hooks For Module Register?",
        "Y": "global forward hook for all the modules",
        "Z": "Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a global forward hook common to all the modules?",
        "Y": "nn.Conv1d",
        "Z": "Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does Holds parameters in a dictionary do?",
        "Y": "Holds parameters in a dictionary",
        "Z": "DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What hook is registered for all modules?",
        "Y": "global forward hook",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations",
        "Y": "Pooling layers",
        "Z": "Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds what?",
        "Y": "parameters in a dictionary",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv1d Applies a what convolution over an input signal composed of several input planes?",
        "Y": "1D",
        "Z": "Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for nn.Conv3d?",
        "Y": "nn.ConvTranspose1d",
        "Z": "Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are Transformer Layers?",
        "Y": "Normalization Layers Recurrent Layers",
        "Z": "Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of hook is registered for all modules?",
        "Y": "global forward hook",
        "Z": "Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ConvTranspose1d use?",
        "Y": "nn.ConvTranspose1d",
        "Z": "Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization",
        "Y": "DataParallel Layers",
        "Z": "DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.Conv1d Applies over an input signal composed of several input planes?",
        "Y": "1D convolution",
        "Z": "Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.Conv1d and nn.Conv3d use?",
        "Y": "nn.ConvTranspose2d",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv1d Applies what over an input signal composed of several input planes?",
        "Y": "1D convolution",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv3d Applies what convolution over an input signal composed of several input planes?",
        "Y": "3D",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.Conv1d and nn.Conv2d use?",
        "Y": "nn.ConvTranspose3d",
        "Z": "A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ConvTranspose1d and nn.Conv2d use?",
        "Y": "nn.ConvTranspose3d",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "A sequential container. Holds submodules in what?",
        "Y": "dictionary",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConvTranspose2d Applies what type of transposed convolution operator over an input image composed of several input planes",
        "Y": "2D",
        "Z": "nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for a sequential container?",
        "Y": "nn.LazyConv1d",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What hook does Global Hooks For Module register?",
        "Y": "global forward hook",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of convolution does nn.Conv2d Applies over an input signal composed of several input planes?",
        "Y": "2D",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is nn.LazyConv2d?",
        "Y": "nn.LazyConv3d",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the 3D convolution operator that is inferred from theinput.size(1)?",
        "Y": "nn.LazyConv3d",
        "Z": "Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv2d Applies what over an input signal composed of several input planes?",
        "Y": "2D convolution",
        "Z": "nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is nn.LazyConv1d a torch.nn.Conv3dmodule with lazy initialization of the",
        "Y": "nn.LazyConvTranspose1d",
        "Z": "Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv3d Applies what over an input signal composed of several input planes?",
        "Y": "3D convolution",
        "Z": "nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transposed convolution operator does nn.ConvTranspose2d Applies over an input image composed of several input plane",
        "Y": "2D",
        "Z": "nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is nn.LazyConv3d a torch.nn.Conv2dmodule with lazy initialization of the",
        "Y": "nn.LazyConvTranspose1d",
        "Z": "nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theCon",
        "Y": "nn.LazyConv2d",
        "Z": "nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which a torch.nn.Conv2dmodule has lazy initialization of thein_channelsargument of theConv",
        "Y": "nn.LazyConv3d",
        "Z": "Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is nn.LazyConv1d a torch.nn.Conv2dmodule with lazy initialization of the",
        "Y": "nn.LazyConvTranspose2d",
        "Z": "Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What transposed convolution operator is applied over an input image composed of several input planes?",
        "Y": "1D",
        "Z": "Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument",
        "Y": "nn.LazyConvTranspose1d",
        "Z": "nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of the",
        "Y": "nn.LazyConvTranspose2d",
        "Z": "nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transposed convolution operator does nn.ConvTranspose3d Applies a 3D transposed convolution operator",
        "Y": "2D",
        "Z": "Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channels",
        "Y": "nn.LazyConvTranspose1d",
        "Z": "Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Applies a transposed convolution operator over an input image composed of several input planes?",
        "Y": "3D",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3",
        "Y": "nn.LazyConv3d",
        "Z": "a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of the",
        "Y": "nn.LazyConvTranspose3d",
        "Z": "a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.MaxPool1d Applies what over an input signal composed of several input planes?",
        "Y": "1D max pooling",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.MaxPool3d Applies what type of max pooling over an input signal composed of several input planes?",
        "Y": "3D",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.AvgPool3d Applies what type of average pooling over an input signal composed of several input planes",
        "Y": "3D",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.MaxPool3d Applies a what type of max pooling over an input signal composed of several input planes",
        "Y": "3D",
        "Z": "nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.AvgPool2d Applies a 2D average pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.LPPool1d Applies a what power-average pooling over an input signal composed of several input planes",
        "Y": "1D",
        "Z": "nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.MaxPool3d Applies what over an input signal composed of several input planes?",
        "Y": "3D max pooling",
        "Z": "nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.AvgPool3d Applies a 3D average pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Computes a partial what of MaxPool1d?",
        "Y": "inverse",
        "Z": "Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.AdaptiveMaxPool1d and nn.FractionalMaxPool3d",
        "Y": "nn.AdaptiveMaxPool2d",
        "Z": "Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.FractionalMaxPool2d Applies?",
        "Y": "2D fractional max pooling over an input signal",
        "Z": "Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.AdaptiveMaxPool3d stand for?",
        "Y": "nn.AdaptiveMaxPool3d",
        "Z": "nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.AvgPool1d do?",
        "Y": "Computes a partial inverse ofMaxPool3d",
        "Z": "Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does Applies over an input signal composed of several input planes?",
        "Y": "3D average pooling",
        "Z": "Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of fractional max pooling does nn.FractionalMaxPool2d apply?",
        "Y": "2D",
        "Z": "nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of",
        "Y": "several input planes",
        "Z": "nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.LPPool1d Applies over an input signal composed of several input planes?",
        "Y": "1D power-average pooling",
        "Z": "Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does replicationPad1d Pads the input tensor using?",
        "Y": "replication",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConstantPad3d Pads the input tensor boundaries with what value?",
        "Y": "constant value",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary. nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary. nn.ReplicationPad1d Pads the input tensor using replication of the input boundary. nn.ReplicationPad2d Pads the input tensor using replication of the input boundary. nn.ReplicationPad3d Pads the input tensor using replication of the input boundary. nn.ZeroPad2d Pads the input tensor boundaries with zero. nn.ConstantPad1d Pads the input tensor boundaries with a constant value. nn.ConstantPad2d Pads the input tensor boundaries with a constant value. nn.ConstantPad3d Pads the input tensor boundaries with a constant value.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the Sigmoid Linear Unit (SiLU) function?",
        "Y": "nn.Mish",
        "Z": "nn.Hardshrink Applies the hard shrinkage function element-wise: nn.Hardsigmoid Applies the element-wise function: nn.Hardtanh Applies the HardTanh function element-wise nn.Hardswish Applies the hardswish function, element-wise, as described in the paper: nn.LeakyReLU Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What input does nn.BatchNorm2d Applies Batch Normalization over?",
        "Y": "4D",
        "Z": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What inputs does Batch Normalization apply over?",
        "Y": "2D or 3D",
        "Z": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.BatchNorm2d Applies Batch Normalization over a what input?",
        "Y": "4D",
        "Z": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.BatchNorm2d Applies Batch Normalization over what input?",
        "Y": "4D input",
        "Z": "nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the group that applies Batch Normalization over a 5D input?",
        "Y": "nn.GroupNorm",
        "Z": "nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What input does BatchNorm3d apply Batch Normalization over?",
        "Y": "5D",
        "Z": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatch",
        "Y": "nn.LazyBatchNorm2d",
        "Z": "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatch",
        "Y": "nn.LazyBatchNorm3d",
        "Z": "a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the group that applies Batch Normalization over a 4D input?",
        "Y": "nn.GroupNorm",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Over what input is Batch Normalization applied?",
        "Y": "5D input",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a mini-batch of 3D inputs with additional channel dimension?",
        "Y": "5D input",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the paperGroup Normalization?",
        "Y": "nn.GroupNorm Applies Group Normalization over a mini-batch of inputs",
        "Z": "nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the instanceNorm1d module?",
        "Y": "nn.InstanceNorm1d",
        "Z": "nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatch",
        "Y": "nn.LazyBatchNorm2d",
        "Z": "a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the module that allows Batch Normalization over a N-Dimensional input?",
        "Y": "nn.InstanceNorm1d",
        "Z": "a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the module with lazy initialization of thenum_featuresargument of theBatchNorm2d?",
        "Y": "nn.InstanceNorm2d",
        "Z": "nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the mini-batch of 2D inputs with additional channel dimension described in the paperInstance Normalization: The Missing Ingredient for Fast",
        "Y": "4D input",
        "Z": "a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the module with lazy initialization of thenum_featuresargument of theBatchNorm3d?",
        "Y": "nn.InstanceNorm3d",
        "Z": "nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the module that applies Batch Normalization over a 4D input?",
        "Y": "nn.InstanceNorm3d",
        "Z": "a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a mini-batch of 2D inputs with additional channel dimension described in the paperInstance Normalization: The Missing Ingredient for",
        "Y": "4D input",
        "Z": "Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies Group Normalization over a mini-batch of inputs as described in the paperInstance Normalization: The Missing Ingredient for",
        "Y": "nn.InstanceNorm3d",
        "Z": "Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a mini-batch of [N-2]D inputs with additional channel dimension?",
        "Y": "N-Dimensional input",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does SyncBatchNorm Applies Batch Normalization over a N-Dimensional input?",
        "Y": "nn.LayerNorm",
        "Z": "nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the document that applies Batch Normalization over a N-Dimensional input?",
        "Y": "nn.LayerNorm",
        "Z": "Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does LayerNorm Applies over a mini-batch of inputs?",
        "Y": "Layer Normalization",
        "Z": "nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does AlphaDropout apply over the input?",
        "Y": "Alpha Dropout",
        "Z": "nn.Dropout During training, randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution. nn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 2D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.Dropout3d Randomly zero out entire channels (a channel is a 3D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 3D tensorinput[i,j]\\text{input}[i, j]input[i,j]). nn.AlphaDropout Applies Alpha Dropout over the input.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What two classes does nn.CrossEntropyLoss combine?",
        "Y": "LogSoftmaxandNLLLossin",
        "Z": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What loss is nn.NLLLoss?",
        "Y": "negative log likelihood loss",
        "Z": "nn.L1Loss Creates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy. nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What label does MarginRankingLoss create a criterion that measures the loss given inputsx1x1x1,",
        "Y": "1D mini-batch tensoryy",
        "Z": "Creates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy. nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss",
        "Y": "Poisson",
        "Z": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What criterion optimizes a multi-class multi-classification hinge loss?",
        "Y": "nn.HuberLoss",
        "Z": "Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of loss is associated with Poisson distribution of target?",
        "Y": "Negative log likelihood loss",
        "Z": "Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "If the absolute element-wise error falls below delta, what is the criterion that uses a squared term?",
        "Y": "if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise",
        "Z": "nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the criterion that uses a squared term if the absolute element-wise error falls below delta?",
        "Y": "nn.Smooth",
        "Z": "nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of hinge loss does MultiLabelMarginLoss optimize?",
        "Y": "multi-class multi-classification hinge loss",
        "Z": "Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What measure measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -",
        "Y": "nn.HingeEmbeddingLoss",
        "Z": "The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-",
        "Y": "nn.Smooth",
        "Z": "nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which criterion uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise?",
        "Y": "nn.SoftMarginLoss",
        "Z": "nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What criterion uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise?",
        "Y": "nn.SoftMarginLoss",
        "Z": "Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term",
        "Y": "nn.SoftMarginLoss",
        "Z": "nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of loss is optimized between inputxxx(a 2D mini-batchTensor) and outputyyy?",
        "Y": "multi-class multi-classification hinge loss",
        "Z": "Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What criterion optimizes a two-class classification logistic loss between input tensorxxxand target tensoryy",
        "Y": "MultiLabelSoftMarginLoss",
        "Z": "Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a matrix-based loss between inputxxx(a 2D mini-batchTensor) and outputyyy?",
        "Y": "multi-class multi-classification hinge loss",
        "Z": "Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the value of a labels tensoryyy?",
        "Y": "1 or -1",
        "Z": "Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does a criterion use if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise",
        "Y": "if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise",
        "Z": "Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a criterion that optimizes a multi-class multi-classification hinge loss?",
        "Y": "MultiMarginLoss",
        "Z": "Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What criterion measures the loss given input tensorsx1x_1x1 and target tensoryyy",
        "Y": "MultiMarginLoss",
        "Z": "nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What criterion optimizes a multi-class classification hinge loss?",
        "Y": "TripletMarginLoss",
        "Z": "nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of loss does TripletMarginLoss measure?",
        "Y": "triplet",
        "Z": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does TripletMarginWithDistanceLoss measure?",
        "Y": "triplet loss",
        "Z": "nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "UpsamplingNearest2d Applies a 2D what upsampling to an input signal composed of several input channels?",
        "Y": "nearest neighbor",
        "Z": "nn.PixelShuffle Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is an upscale factor. nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d Applies a 2D bilinear upsampling to an input signal composed of several input channels.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "UpsamplingBilinear2d Applies what upsampling to an input signal composed of several input channels?",
        "Y": "2D bilinear",
        "Z": "nn.PixelShuffle Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is an upscale factor. nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor. nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. nn.UpsamplingBilinear2d Applies a 2D bilinear upsampling to an input signal composed of several input channels.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the entire (currently unpruned) channels in a tensor at random?",
        "Y": "RandomStructured Prune",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.identity?",
        "Y": "CustomFromMask",
        "Z": "Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of Structured Prune entire (currently unpruned) channels in a tensor based on their Ln",
        "Y": "Ln",
        "Z": "Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pruning method does prune.identity apply pruning reparametrization to the tensor corresponding to the parameter callednamein",
        "Y": "random_unstructured",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What structured prune entire channels in a tensor based on their Ln-norm?",
        "Y": "Ln",
        "Z": "Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "CustomFromMask prune.identity Applies what reparametrization to the tensor corresponding to the parameter calledname",
        "Y": "pruning",
        "Z": "Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the Abstract base class for creation of?",
        "Y": "pruning techniques",
        "Z": "Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the",
        "Y": "L1-norm",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does customFromMask prune.identity apply to the tensor corresponding to the parameter callednameinmodule?",
        "Y": "pruning reparametrization",
        "Z": "prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a sequence of pruning methods for?",
        "Y": "iterative pruning",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.L1Unstructured Prune units in a tensor by zeroing out the ones with the lowest?",
        "Y": "L1-norm",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.L1Unstructured Prune?",
        "Y": "L1-norm",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does customFromMask prune.identity Applies to the tensor corresponding to the parameter callednameinmodule without",
        "Y": "pruning reparametrization",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What corresponding to parameter callednameinmodule by removing the specifiedamountof (currently unpruned) units with the lowest",
        "Y": "prune.l1_unstructured Prunes tensor",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the entire channels in a tensor at random?",
        "Y": "RandomStructured Prune",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the term for channels in a tensor that are currently unpruned?",
        "Y": "RandomStructured Prune",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What corresponding to parameter callednameinmodule?",
        "Y": "prune.l1_unstructured Prunes tensor",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the tensor with the lowest L1-norm?",
        "Y": "prune.ln_structured",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the term for current unpruned units in a tensor?",
        "Y": "prune.L1Unstructured Prune",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "At what rate do prune.RandomStructured Prune entire (currently unpruned) channels in a",
        "Y": "random",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What corresponding to parameter callednameinmodule by removing the specifiedamountof (currently unpruned) channels along the specified",
        "Y": "prune.ln_structured Prunes tensor",
        "Z": "Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is done to current unpruned units in a tensor?",
        "Y": "Prune",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.random_unstructured Prunes tensor corresponding to parameter callednameinmodule?",
        "Y": "prune.ln_structured",
        "Z": "Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Prune entire (currently unpruned) channels in a tensor based on their Ln-norm",
        "Y": "prune.LnStructured",
        "Z": "Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.CustomFromMask prune.identity apply to the tensor corresponding to the parameter callednameinmodule",
        "Y": "pruning reparametrization",
        "Z": "prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What tensor corresponding to parameter callednameinmodule removing the specifiedamountof (currently unpruned) units",
        "Y": "prune.l1_unstructured Prunes tensor",
        "Z": "prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the Prunes tensor corresponding to parameter callednameinmodule?",
        "Y": "prune.ln_structured",
        "Z": "prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method?",
        "Y": "global_unstructured",
        "Z": "Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What global prunes tensors corresponding to all parameters inparameters by applying the specifiedpruning_method?",
        "Y": "global_unstructured",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the tensor that prunes tensors corresponding to all parameters inparameters?",
        "Y": "prune.custom_from_mask",
        "Z": "Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is applied to the tensor corresponding to the parameter callednameinmodule without actually pruning any units?",
        "Y": "pruning reparametrization",
        "Z": "Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What tensor corresponding to parameter callednameinmodule by removing the specifiedamountof (currently unpruned)",
        "Y": "prune.l1_unstructured Prunes tensor",
        "Z": "Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the globally prunes tensors corresponding to all parameters inparameters?",
        "Y": "global_unstructured",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is applied to prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmodule?",
        "Y": "pre-computed mask inmask",
        "Z": "prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned",
        "Y": "L1-norm",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the global prunes tensors corresponding to all parameters inparameters?",
        "Y": "global_unstructured",
        "Z": "prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.is_pruned inherit from?",
        "Y": "theBasePruningMethod",
        "Z": "prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.is_pruned do to a parameter in a given module?",
        "Y": "Applies weight normalization",
        "Z": "prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.remove remove from a module?",
        "Y": "forward hook",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is applied to a parameter in a given module?",
        "Y": "weight normalization",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of normalization is applied to a parameter in a given module?",
        "Y": "spectral",
        "Z": "prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Removes what reparameterization from a module?",
        "Y": "spectral normalization",
        "Z": "Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What globally prunes tensors corresponding to all parameters inparameters?",
        "Y": "prune.global_unstructured",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.is_pruned apply to a parameter in the given module?",
        "Y": "weight normalization",
        "Z": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.global_unstructured apply to a parameter in the given module?",
        "Y": "spectral normalization",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is removed from a module?",
        "Y": "spectral normalization reparameterization",
        "Z": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Parametrizations implemented using what new parametrization functionality?",
        "Y": "intorch.nn.utils.parameterize.register_parametrization()",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What applies spectral normalization to a parameter in the given module?",
        "Y": "parametrizations",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to what repository?",
        "Y": "github",
        "Z": "Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility. Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where can you see the full script for Pytorch Hub?",
        "Y": "inpytorch/vision repo",
        "Z": "Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility. Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Pytorch/vision repo dependencies variable is alistof what required for training a model?",
        "Y": "dependencies",
        "Z": "Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility. Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is required for training a model?",
        "Y": "dependencies",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Here we just want to use the expanded version as what to show how it works?",
        "Y": "an example",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is highly recommended to add to the help message?",
        "Y": "a few examples",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What are callables prefixed with?",
        "Y": "underscore",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is alistof package names required toload the model?",
        "Y": "dependencies variable",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What are the allowed arguments for a model?",
        "Y": "positional/keyword",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What are auxiliary tools to make the user workflow smoother?",
        "Y": "tokenizers",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the minimum size of a pretrained weight?",
        "Y": "2GB",
        "Z": "dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What should you do if you have less than 2GB of pretrained weights?",
        "Y": "attach it to aproject release",
        "Z": "argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can't be published models?",
        "Y": "a random commit",
        "Z": "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simplehubconf.pyfile; hubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is an example of auxiliary tools to make the user workflow smoother?",
        "Y": "tokenizers",
        "Z": "Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the example where you can put the following logic in the entrypoint definition?",
        "Y": "abovetorchvision.models.resnet.resnet18handlespretrained",
        "Z": "Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can you put in the entrypoint definition?",
        "Y": "the following logic",
        "Z": "Here is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo dependencies variable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model. argsandkwargsare passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What should you do if your weight is less than 2GB?",
        "Y": "attach it to aproject release",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What can't be included in the published models?",
        "Y": "a random commit",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of all entrypoints available in Pytorch Hub?",
        "Y": "in github hubconf",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What provides convenient APIs to explore all available models in hub throughtorch.hub.list(), show docstring and examples throughtor",
        "Y": "Pytorch Hub",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What branch ismaster if not specified?",
        "Y": "The default branch",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for a list of available entrypoint names entrypoints?",
        "Y": "Default is False",
        "Z": "Pretrained weights can either be stored locally in the github repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition. The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What type of entrypointmodel does Pytorch Hub show?",
        "Y": "docstring",
        "Z": "The published models should be at least in a branch/tag. It can\u2019t be a random commit. Pytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load(). List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does in github hubconf list?",
        "Y": "all entrypoints available",
        "Z": "List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What type of tag/branch does github(string) have?",
        "Y": "optional",
        "Z": "List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel.",
        "Y": "force_reload",
        "Z": "List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is model(string)?",
        "Y": "a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional)",
        "Z": "github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for a string of entrypoint name defined in repo's hubconf.py force_reload(bool",
        "Y": "Default is False",
        "Z": "List all entrypoints available in github hubconf. github(string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default is False?",
        "Y": "force_reload",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is force_reload(bool,optional) set to?",
        "Y": "Default is False",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What ismasterif not specified?",
        "Y": "branch",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is an example of a string of entrypoint name defined in repo's hubconf.py?",
        "Y": "pytorch/vision[:hub]",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does force_reload(bool,optional) default to?",
        "Y": "Default is False",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. a list of available entrypoint names entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the entrypoint defined in repo's hubconf.py?",
        "Y": "pytorch/vision[:hub]",
        "Z": "entrypoints Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for the default value?",
        "Y": "False",
        "Z": "Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does repo_or_diris expect to be of the formrepo_owner/repo_name[:tag_name",
        "Y": "if sourceis'github",
        "Z": "if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the entrypoint name defined in repo's hubconf.py?",
        "Y": "pytorch/vision[:hub]",
        "Z": "Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for the default value?",
        "Y": "False",
        "Z": "Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the entrypoint model defined in repo's hubconf.py force_reload(bool,optional)",
        "Y": "pytorch/vision[:hub]",
        "Z": "Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How do you load a model from a github repo?",
        "Y": "a local directory",
        "Z": "Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for a new download?",
        "Y": "False",
        "Z": "github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Load what from a github repo or a local directory?",
        "Y": "a model",
        "Z": "github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of a callable (entrypoint) defined in the repo/dir\u2019shubconf.py?",
        "Y": "model(string)",
        "Z": "Example Show the docstring of entrypointmodel. github(string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional\ntag/branch. The default branch ismasterif not specified.\nExample: \u2018pytorch/vision[:hub]\u2019 model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "what is a string of entrypoint name defined in repo's hubconf.py force_reload(bool,optional)",
        "Y": "model(string)",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "Y": "source",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Specifies how repo_or_dir is to be interpreted. Default is' what?",
        "Y": "github",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the command that can be used to force a fresh download?",
        "Y": "force_reload",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download. Default is what?",
        "Y": "False",
        "Z": "force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Example Load what from a github repo or a local directory?",
        "Y": "a model",
        "Z": "Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the typical use case of a github repo or a local directory?",
        "Y": "Load a model",
        "Z": "Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default for repo_or_diris?",
        "Y": "github",
        "Z": "if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the output of the modelcallable when called with the given*argsand**kwargs?",
        "Y": "output",
        "Z": "model(string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault is False. Example Load a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc. if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting to force a fresh download of the github repo unconditionally?",
        "Y": "False",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Where is the Example Download object located?",
        "Y": "given URL to a local path",
        "Z": "repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default location to be interpreted?",
        "Y": "github",
        "Z": "model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is at the given URL to a local path?",
        "Y": "Example Download object",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the name of the command to force a fresh download of the github repo unconditionally?",
        "Y": "force_reload",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default setting for the SHA256 downloaded file?",
        "Y": "None",
        "Z": "*args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If False, what happens to messages about hitting local caches?",
        "Y": "mute messages about hitting local caches",
        "Z": "source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value for displaying a progress bar to stderr?",
        "Y": "Default: None",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What message cannot be muted?",
        "Y": "message about first download",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What does the modelcallable callable callable callable callable callable callable callable callable callable callable callable callable call",
        "Y": "Example Download object",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What type of object is at the given URL to a local path?",
        "Y": "Example Download object",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the URL of the object to download dst(string) \u2013 Full path where object will be saved?",
        "Y": "url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved",
        "Z": "verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value of progress(bool,optional) to display a progress bar to stderr?",
        "Y": "True",
        "Z": "hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "If the object is already present, it's deserialized and returned.",
        "Y": "inmodel_dir",
        "Z": "If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value to display a progress bar to stderr?",
        "Y": "True",
        "Z": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What default value is used to load the Torch serialized object at the given URL?",
        "Y": "True",
        "Z": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value of the Torch serialized object?",
        "Y": "True Example Loads the Torch serialized object at the given URL",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value of progress(bool,optional)?",
        "Y": "None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr",
        "Z": "if sourceis'github',repo_or_diris expected to be\nof the formrepo_owner/repo_name[:tag_name]with an optional\ntag/branch. if sourceis'local',repo_or_diris expected to be a\npath to a local directory. repo_or_dir(string) \u2013 repo name (repo_owner/repo_name[:tag_name]),\nif source='github'; or a path to a local directory, if source='local'. model(string) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py. *args(optional) \u2013 the corresponding args for callablemodel. source(string,optional) \u2013'github'|'local'. Specifies how repo_or_dir is to be interpreted. Default is'github'. force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What serialized object does Example Load at the given URL?",
        "Y": "Torch",
        "Z": "Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value of check_hash(bool,optional)?",
        "Y": "True",
        "Z": "force_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect if source='local'. Default is False. verbose(bool,optional) \u2013 If False, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect if source='local'.\nDefault is True. **kwargs(optional) \u2013 the corresponding kwargs for callablemodel. The output of themodelcallable when called with the given*argsand**kwargs. Example Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is used to ensure unique names and to verify the contents of the file?",
        "Y": "The hash",
        "Z": "Download object at the given URL to a local path. url(string) \u2013 URL of the object to download dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is the default value of file_name(string,optional)?",
        "Y": "False file_name(string,optional) \u2013 name for the downloaded file",
        "Z": "dst(string) \u2013 Full path where object will be saved, e.g./tmp/temporary_file hash_prefix(string,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None progress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True Example Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example Note that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is dir(model)to see all available methods of the model. help(model.foo)to check what argumentsmodel.footakes to run To help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example. The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is an example of a filename that will be used if not set?",
        "Y": "Example",
        "Z": "If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. Example",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is optionally set to save downloaded models & weights?",
        "Y": "Torch Hub directory",
        "Z": "The locations are used in the order of Callinghub.set_dir(<PATH_TO_HUB_DIR>) $TORCH_HOME/hub, if environment variableTORCH_HOMEis set. $XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set. ~/.cache/torch/hub Get the Torch Hub cache directory used for storing downloaded models & weights. Ifset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set. Optionally set the Torch Hub directory used to save downloaded models & weights. d(string) \u2013 path to a local folder to save downloaded models & weights.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is normal Python behavior?",
        "Y": "cachessys.modulesandsys.path_importer_cache",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior. A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is a known limitation of importing in Python?",
        "Y": "userCANNOTload two different branches of the same repo in thesame python process",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior. A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How many packages with the same name are installed in Python?",
        "Y": "two",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior. A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "Cache might join the party and give you what if you try that?",
        "Y": "surprise",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior. A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How do you load two different branches of the same repo?",
        "Y": "in separate processes",
        "Z": "Torch hub works by importing the package as if it was installed. There\u2019re some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior. A known limitation that worth mentioning here is userCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "What is an example of a function that computes the element-wise minimum of inputandother?",
        "Y": "Example:",
        "Z": "Computes the element-wise minimum ofinputandother. This is liketorch.minimum()except it handles NaNs differently:\nif exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum.\nOnly if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019sstd::fminand is similar to NumPy\u2019sfminfunction. Supportsbroadcasting to a common shape,type promotion, and integer and floating-point inputs. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "What is the output shape of input.shape[:-1] + other.shape[:-1]?",
        "Y": "Example",
        "Z": "Computes the dot product for 1D tensors. For higher dimensions, sums the product\nof elements frominputandotheralong their last dimension. Note If eitherinputorotheris a scalar, the result is equivalent\ntotorch.mul(input, other). If bothinputandotherare non-scalars, the size of their last\ndimension must match and the result is equivalent totorch.tensordot(input,\nother, dims=([-1], [-1])) input(Tensor) \u2013 First input tensor other(Tensor) \u2013 Second input tensor out(Tensor,optional) \u2013 Optional output tensor to write result into. The output\nshape isinput.shape[:-1] + other.shape[:-1]. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner"
    },
    {
        "X": "Atol(float,optional) \u2013 what is input(Tensor) a second tensor to compare?",
        "Y": "absolute tolerance",
        "Z": "Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.\nCloseness is defined as: whereinputandotherare finite. Whereinputand/orotherare nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other whenequal_nanis True. input(Tensor) \u2013 first tensor to compare other(Tensor) \u2013 second tensor to compare atol(float,optional) \u2013 absolute tolerance. Default: 1e-08 rtol(float,optional) \u2013 relative tolerance. Default: 1e-05 equal_nan(bool,optional) \u2013 If True, then twoNaNs will be considered equal. Default:False Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"
    },
    {
        "X": "How do you find the indices from the innermost dimension ofsorted_sequence?",
        "Y": "Find the indices",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s).",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Ifrightis False (default), then the left boundary ofsorted_sequenceis closed.",
        "Y": "Ifrightis False",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s).",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What does sorted_sequence right return index satisfies?",
        "Y": "1-D False sorted_sequence",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s).",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What right returned index satisfies 1-D False sorted_sequence?",
        "Y": "sorted_sequence",
        "Z": "sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Return index satisfies what type of False sorted_sequence[i-1]values[m][",
        "Y": "1-D",
        "Z": "returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is False sorted_sequence[i-1]values[m][n]?",
        "Y": "1-D",
        "Z": "1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What does the sorted_sequence(Tensor) contain?",
        "Y": "monotonically increasing sequence",
        "Z": "False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is another name for sorted_sequence(Tensor)?",
        "Y": "1-D tensor",
        "Z": "N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "Right(bool,optional) \u2013 if False, return the what?",
        "Y": "first",
        "Z": "Find the indices from the innermost dimension ofsorted_sequencesuch that, if the\ncorresponding values invalueswere inserted before the indices, the order of the\ncorrespondinginnermostdimension withinsorted_sequencewould be preserved.\nReturn a new tensor with the same size asvalues. Ifrightis False (default),\nthen the left boundary ofsorted_sequenceis closed. More formally, the returned index\nsatisfies the following rules: sorted_sequence right returned index satisfies 1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i] 1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i] N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i] N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False. out(Tensor,optional) \u2013 the output tensor, must be the same size asvaluesif provided. Note If your use case is always 1-D sorted sequence,torch.bucketize()is preferred,\nbecause it has fewer dimension checks resulting in slightly better performance. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If False, return the first suitable location that is found.",
        "Y": "return the last such index",
        "Z": "sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If no suitable index found, what is returned for non-numerical value (eg. nan, inf) or the size of",
        "Y": "return 0",
        "Z": "sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is a non-numerical value?",
        "Y": "inf",
        "Z": "sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If False, gets what instead of the lower bound index?",
        "Y": "upper bound index",
        "Z": "sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is the default value of sorted_sequence?",
        "Y": "False",
        "Z": "sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is a sorted_sequence(Tensor) containing monotonically increasing sequence on the innermost dimension?",
        "Y": "N-D or 1-D tensor",
        "Z": "True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "If no suitable index found, what is returned for non-numerical value?",
        "Y": "return 0",
        "Z": "True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What index does False get for each value invalueson the correspondinginnermostdimension of thesorted_sequence?",
        "Y": "lower bound index",
        "Z": "True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is the default value of a sorted_sequence?",
        "Y": "False",
        "Z": "True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "sorted_sequence(Tensor) \u2013 N-D or what?",
        "Y": "1-D tensor",
        "Z": "sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What is the default value for a sorted_sequence?",
        "Y": "False",
        "Z": "sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i] sorted_sequence(Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. values(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.\nDefault value is False, i.e. default output data type is torch.int64. right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the\nlast such index. If no suitable index found, return 0 for non-numerical value\n(eg. nan, inf) or the size ofinnermostdimension withinsorted_sequence(one pass the last index of the innermost dimension). In other words, if False,\ngets the lower bound index for each value invalueson the correspondinginnermostdimension of thesorted_sequence. If True, gets the upper\nbound index instead. Default value is False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "What does PyTorch support on Ampere devices?",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aboolthat controls whether what is enabled in PyTorch?",
        "Y": "cuDNN",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What backend controls if TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does the backend that controls cache capacity of cuFFT plan cache do?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Returns what version of cuDNN?",
        "Y": "version of cuDNN",
        "Z": "torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What backend controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Amper",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does openmp return?",
        "Y": "whether PyTorch is built with CUDA support",
        "Z": "torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is the name of the tensor cores that can be used in matrix multiplications on Ampere devices?",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is returned by torch.backends.cudnn torch.backends.mkl torch.backends.mk",
        "Y": "whether PyTorch is built with CUDA support",
        "Z": "torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is returned when PyTorch is built with CUDA support?",
        "Y": "version of cuDNN",
        "Z": "torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does torch.backends.mkl return?",
        "Y": "whether PyTorch is built with CUDA support",
        "Z": "torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does it do when a PyTorch binary is built with CUDA support?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What is returned by torch.backends.openmp?",
        "Y": "version of cuDNN",
        "Z": "torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does it do when the cuFFT plan cache is cleared?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aboolthat controls whether what may be used in matrix multiplications on Ampere or newer GPUs?",
        "Y": "TensorFloat-32 tensor cores",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Aintthat controls what of the cuFFT plan. Clears the cuFFT plan cache.",
        "Y": "cache capacity",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions",
        "Y": "SeeTensorFloat-32(TF32)",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What type of algorithms does _algorithms_enabled() and torch.use_deterministic_algorith",
        "Y": "deterministic",
        "Z": "Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does cuDNN do if True?",
        "Y": "benchmark multiple convolution algorithms and select the fastest",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "A readonly int that shows the number of plans currently in what?",
        "Y": "cuFFT plan cache",
        "Z": "cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does cufft_plan_cachecache do?",
        "Y": "Clears the cuFFT plan cache",
        "Z": "cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What causes cuDNN to benchmark multiple convolution algorithms and select the fastest?",
        "Y": "Aboolthat",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What caches the cuFFT plans?",
        "Y": "cufft_plan_cache",
        "Z": "Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What type of algorithms does _algorithms_enabled() use?",
        "Y": "deterministic",
        "Z": "torch.backendscontrols the behavior of various backends that PyTorch supports. These backends include: torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp Returns whether PyTorch is built with CUDA support.  Note that this\ndoesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run a machine with working CUDA drivers and devices, we\nwould be able to use it. Aboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. cufft_plan_cachecaches the cuFFT plans A readonly int that shows the number of plans currently in the cuFFT plan cache. Aintthat controls cache capacity of cuFFT plan. Clears the cuFFT plan cache. Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest. Returns whether PyTorch is built with MKL support. Returns whether PyTorch is built with MKL-DNN support. Returns whether PyTorch is built with OpenMP support.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "What does a bool that is True cause cuDNN to do?",
        "Y": "benchmark multiple convolution algorithms and select the fastest",
        "Z": "Returns the version of cuDNN Returns a bool indicating if CUDNN is currently available. Aboolthat controls whether cuDNN is enabled. Aboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. SeeTensorFloat-32(TF32) on Ampere devices. Aboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms(). Aboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.",
        "source": "https://pytorch.org/docs/stable/backends.html"
    },
    {
        "X": "Computes the one dimensional Fourier transform of real-valuedinput. Computes the inverse ofrfft(). Comp",
        "Y": "Computes the N dimensional inverse discrete Fourier transform ofinput",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the 2-dimensional discrete Fourier transform of realinput. Computes the inverse ofrfft2()?",
        "Y": "Computes the inverse ofrfft2()",
        "Z": "Discrete Fourier transforms and related functions.   Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional inverse discrete Fourier transform ofinput. Computes the one dimensional inverse discrete Fourier",
        "Y": "2 dimensional discrete Fourier transform",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional Fourier transform of real-valuedinput. Computes the one dimensional Fourier transform of realinput",
        "Y": "N dimensional inverse discrete Fourier transform",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional Fourier transform of real-valuedinput. Computes the inverse ofrfft2().",
        "Y": "2-dimensional discrete Fourier transform of realinput",
        "Z": "Computes the one dimensional discrete Fourier transform ofinput.   Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "Computes the one dimensional inverse discrete Fourier transform of realinput. Computes the inverse ofrfft2",
        "Y": "2-dimensional discrete Fourier transform of realinput",
        "Z": "Computes the one dimensional inverse discrete Fourier transform ofinput.   Computes the 2 dimensional discrete Fourier transform ofinput.   Computes the 2 dimensional inverse discrete Fourier transform ofinput.   Computes the N dimensional discrete Fourier transform ofinput.   Computes the N dimensional inverse discrete Fourier transform ofinput.   Computes the one dimensional Fourier transform of real-valuedinput.   Computes the inverse ofrfft().   Computes the 2-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfft2().   Computes the N-dimensional discrete Fourier transform of realinput.   Computes the inverse ofrfftn().   Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.   Computes the inverse ofhfft().   Computes the discrete Fourier Transform sample frequencies for a signal of sizen.   Computes the sample frequencies forrfft()with a signal of sizen.",
        "source": "https://pytorch.org/docs/stable/fft.html"
    },
    {
        "X": "What does torch.float32 use?",
        "Y": "global dtype default",
        "Z": "The shape of the tensor is defined by the variable argumentsize. Note With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What is the default value for a global default tensor type?",
        "Y": "if None",
        "Z": "The shape of the tensor is defined by the variable argumentsize. Note With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What does Torch.float32 use?",
        "Y": "global dtype default",
        "Z": "Note With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What does if None use for the default tensor type?",
        "Y": "current device",
        "Z": "With the global dtype default (torch.float32), this function returns\na tensor with dtypetorch.int64. low(int,optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high(int) \u2013 One above the highest integer to be drawn from the distribution. size(tuple) \u2013 a tuple defining the shape of the output tensor. generator(torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "What is the default value for autograd to return a tensor with the same size as input?",
        "Y": "False",
        "Z": "Returns a tensor with the same size as inputthat is filled with\nrandom numbers from a normal distribution with mean 0 and variance 1.torch.randn_like(input)is equivalent totorch.randn(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.randn_like.html#torch.randn_like"
    },
    {
        "X": "Helper class for measuring what of PyTorch statements.",
        "Y": "execution time",
        "Z": "Helper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What fields are included in the representation of result object and by theCompareclass?",
        "Y": "These fields are included in the representation of result object and by theCompareclass",
        "Z": "Helper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What are some examples of PyTorch Timer constructor arguments?",
        "Y": "stmt,setup,timer,globals",
        "Z": "Helper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Setup\u2013 what is used to define variables used instmt global_setup?",
        "Y": "Optional setup code",
        "Z": "Helper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Timer\u2013 Callable which returns the what?",
        "Y": "current time",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What are the fields included in the representation of result object and by theCompareclass to group and display results for comparison?",
        "Y": "These fields are included in the representation of result object and by theCompareclass to group and display results for comparison",
        "Z": "When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Setup\u2013 What is used to define variables used instmt global_setup?",
        "Y": "Optional setup code",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Optional setup code. Used to define variables used instmt what?",
        "Y": "global_setup",
        "Z": "When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "A dict which defines the global variables whenstmtis being executed. This is the other method for providing variables whichstmtn",
        "Y": "globals",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "String which summarizes stmt.",
        "Y": "label",
        "Z": "When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is sub_label?",
        "Y": "Provide supplemental information",
        "Z": "When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a String to distinguish measurements with identical label and sub_label?",
        "Y": "description",
        "Z": "When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What would one set description based on to create a column of data?",
        "Y": "input size",
        "Z": "When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "If PyTorch was built without what, this defaults totimeit.default_timer; otherwise it will synchronize CUDA",
        "Y": "CUDA",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "String which summarizes stmt. For instance, ifstmtis \u201ctorch.nn.functional.rel",
        "Y": "label",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What \u2013 Provide supplemental information to disambiguate measurements with identical stmt or label?",
        "Y": "sub_label",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "String to distinguish measurements with identical label and sub_label. The principal use of descriptionis to signal toComparethe columns of data.",
        "Y": "description",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How would one set it based on the input size to create a table of the form?",
        "Y": "usingCompare",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the other method for providing variables whichstmtneeds?",
        "Y": "globals",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "When is description included in PyTorch?",
        "Y": "when printing a Measurement",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the optional setup code used to define variables used in PyTorch?",
        "Y": "instmt global_setup",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Block_autorange executes the following pseudo-code: Note what in the inner loop?",
        "Y": "variableblock_size",
        "Z": "Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What are examples of statistics that can be collected using Callgrind?",
        "Y": "mean, median, etc.",
        "Z": "A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Who can instrument the program?",
        "Y": "Valgrind",
        "Z": "Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a measurement object that contains measured runtimes and repetition counts?",
        "Y": "median",
        "Z": "At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Globals are restricted to what?",
        "Y": "builtins,nn.Modules\u2019s, and Torch Scripted functions/modules",
        "Z": "blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What provides more detail on the subject of globals?",
        "Y": "TheGlobalsBridgeclass",
        "Z": "blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is collected and cached to indicate how many instructions are from the Python loop which drivesstmt?",
        "Y": "a profile",
        "Z": "A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What are some statistics that can be used by Callgrind?",
        "Y": "mean, median, etc.",
        "Z": "blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Why are globals restricted to builtins,nn.Modules, and Torch Scripted functions/modules?",
        "Y": "globalscannot contain arbitrary in-memory data structures",
        "Z": "blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the result of a Timer measurement?",
        "Y": "timeit.Timer.timeit",
        "Z": "blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What are some statistics that can be computed with A Measurement object?",
        "Y": "mean, median, etc.",
        "Z": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What class provides more detail on globals?",
        "Y": "TheGlobalsBridgeclass",
        "Z": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What provides instruction counts and basic facilities for analyzing and manipulating results?",
        "Y": "ACallgrindStatsobject",
        "Z": "ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does timeit.Timer.timeit() execute?",
        "Y": "main statement (stmt)numbertimes",
        "Z": "In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is instruction counts ideal for?",
        "Y": "detailed performance analysis",
        "Z": "Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does the Timer.timeit class provide for downstream consumers?",
        "Y": "several convenience methods",
        "Z": "By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Merge will extrapolate times tonumber_per_run=1 and will not transfer any metadata.",
        "Y": "Approximate significant figure estimate",
        "Z": "Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What class provides more detail on this subject?",
        "Y": "TheGlobalsBridgeclass",
        "Z": "In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What depend on pickle and you may need to add an import tosetup for them to transfer properly?",
        "Y": "nn.Modules",
        "Z": "In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the purpose of the Approximate significant figure estimate property?",
        "Y": "to give a convenient way to estimate the precision of a measurement",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Why is there a process boundary between the caller and thestmtexecution?",
        "Y": "globalscannot contain arbitrary in-memory data structures",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What are restricted to builtins,nn.Modules, and Torch Scripted functions/modules?",
        "Y": "globals",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What provides more detail on this subject?",
        "Y": "TheGlobalsBridgeclass",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does ACallgrindStats mirror?",
        "Y": "timeit.Timer.timeit()",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does this class provide?",
        "Y": "several convenience methods",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Merge will extrapolate times tonumber_per_run=1 and will not transfer any metadata. (Since it might differ between",
        "Y": "Convenience method for merging replicates",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What region does the Approximate significant figure estimate only use?",
        "Y": "interquartile region",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is used to provide a more human interpretable data summary?",
        "Y": "thetrim_sigfigmethod",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Why does __repr__ not use this method?",
        "Y": "it simply displays raw values",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is significant figure estimation intended for?",
        "Y": "forCompare",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What container is used for Callgrind results collected by Timer?",
        "Y": "Top level container",
        "Z": "Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is executed by timeit.Timer.timeit()?",
        "Y": "main statement (stmt)numbertimes",
        "Z": "By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is provided for merging replicates?",
        "Y": "Convenience method",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What property is used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data summary?",
        "Y": "significant figure estimation",
        "Z": "ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is intended forCompare?",
        "Y": "Significant figure estimation",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is used for Callgrind results collected by Timer?",
        "Y": "Top level container",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Manipulation is generally done using what class?",
        "Y": "FunctionCounts",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the most significant convenience method?",
        "Y": "isCallgrindStats.as_standardized()",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What can be removed from function strings?",
        "Y": "Strip library names and some prefixes",
        "Z": "ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does the timeit.Timer.timeit class provide?",
        "Y": "several convenience methods",
        "Z": "ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What can be a stumbling block when comparing two different sets of instruction counts?",
        "Y": "path prefixes",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does Callgrind include when reporting a function?",
        "Y": "full filepath",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "If a key component such as Python or PyTorch was built in separate locations in the two profiles, what can this cause?",
        "Y": "issues",
        "Z": "ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "If a key component such as Python was built in separate locations in the two profiles, what can result in something resembling?",
        "Y": "PyTorch",
        "Z": "ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the function that mirrors the semantics of?",
        "Y": "timeit.Timer.timeit()",
        "Z": "Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What convenience method does timeit.Timer.timeit provide?",
        "Y": "__repr__",
        "Z": "Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data summary?",
        "Y": "significant figure estimation",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What do you strip from function strings?",
        "Y": "library names and some prefixes",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What can Callgrind include the full filepath when reporting a function?",
        "Y": "issues when diffing profiles",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What can ameliorate this issue by regularizing the strings and causing better cancellation of equivilent call sites when diffing?",
        "Y": "Stripping prefixes",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What convenience method is provided for downstream consumers?",
        "Y": "__repr__",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What can I do with a package?",
        "Y": "Customize how a class is packaged",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the purpose of testing a package?",
        "Y": "Test in my source code whether or not it is executing inside a package",
        "Z": "Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What package does Torch package?",
        "Y": "Torch Script module",
        "Z": "See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the package that finds your code's dependencies?",
        "Y": "torch.packageFormat",
        "Z": "Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is one way to package a package?",
        "Y": "Package a Torch Script module",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the package that Torch.packagefinds your code's dependencies?",
        "Y": "torch.packageFormat",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the module that can be used to package code?",
        "Y": "Package a Torch Script module",
        "Z": "Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the package format used by Torch Script?",
        "Y": "torch.packageFormat",
        "Z": "Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Package Exporterexposes three methods that allow you to save Python objects, text, and binary data to a package?",
        "Y": "save_pickle,save_textandsave_binary",
        "Z": "Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you need to distinguish between?",
        "Y": "packaged code and non-packaged code",
        "Z": "Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a Torch Script module?",
        "Y": "Package a Torch Script module",
        "Z": "Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is similar to defining a corresponding de-packaging function on a class and by defining a corresponding de-packaging",
        "Y": "defining__reduce__for Python\u2019s normal pickling process",
        "Z": "Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What steps are included in the Torch Script module?",
        "Y": "Steps",
        "Z": "Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What steps are included in the Torch.packageFormat Overview?",
        "Y": "Steps",
        "Z": "Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.packageallow for?",
        "Y": "customization",
        "Z": "torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a similar behavior to defining the method__reduce_package__on a class and by defining a",
        "Y": "defining__reduce__for Python\u2019s normal pickling process",
        "Z": "torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What steps are included in the Torch.packageFormat Overview Howtorch.packagekeeps packages isolated from each other?",
        "Y": "Steps",
        "Z": "torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.package allow for?",
        "Y": "customization",
        "Z": "Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Howtorch.package allows for the customization of how classes are packaged?",
        "Y": "defining__reduce__for Python\u2019s normal pickling process",
        "Z": "Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What steps are included in the Torch.packagesharp edges?",
        "Y": "Steps",
        "Z": "Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What steps are included in the Torch.package tutorial?",
        "Y": "Steps",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is similar to defining the method__reduce_package__on a class and by defining a corresponding de-",
        "Y": "defining__reduce__for Python\u2019s normal pickling process",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a tutorial that guides you through packaging and unpackaging a simple model?",
        "Y": "API Reference",
        "Z": "API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the steps that are used to create and use Torch packages?",
        "Y": "Steps",
        "Z": "API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who calls the de-packaging function when it encounters an instance of the target class?",
        "Y": "the Package Exporter",
        "Z": "Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What function should do the work to reconstruct and return an instance of the class?",
        "Y": "de-packaging",
        "Z": "torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps: Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is it bad practice to have code that behaves differently depending on whether it\u2019s packaged or not?",
        "Y": "it\u2019s bad practice",
        "Z": "Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "a torch.packagefile is a what?",
        "Y": "ZIP archive",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are the two types of files inside the ZIP archive?",
        "Y": "Framework files",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is everything else in the ZIP archive?",
        "Y": "User files",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who put all other files in the archive in a torch.packagefile?",
        "Y": "a user",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "The layout of a torch.packagefile is identical to what?",
        "Y": "Pythonregular package",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of everything else in Python?",
        "Y": "User files",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "The layout is identical to what?",
        "Y": "Pythonregular package",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is everything else?",
        "Y": "User files",
        "Z": "User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who owns the data/directory of the ResNet model?",
        "Y": "torch.package",
        "Z": "As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the.data/directory contain?",
        "Y": "version: a version number for the serialized format",
        "Z": "Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the purpose of this essay?",
        "Y": "a deeper dive in how Python packaging works",
        "Z": "version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What describes where to find the implementation of the object\u2019s type?",
        "Y": "a GLOBAL opcode",
        "Z": "version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Python module is identified as a dependency,torch.packagewalks the module\u2019s what representation?",
        "Y": "python AST",
        "Z": "User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When a Python module is identified as a dependency,torch.packageregisters the imported modules as dependencies that are dependencies that",
        "Y": "import statements",
        "Z": "User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "For a deeper dive in how Python packaging works, please consult what?",
        "Y": "this essay",
        "Z": "Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What representation does torch.package walk when a Python module is identified as a dependency?",
        "Y": "python AST",
        "Z": "As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST",
        "Y": "import statements",
        "Z": "As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When a Python module is identified as a dependency,torch.packagewalks the module's what representation?",
        "Y": "python AST",
        "Z": "In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Which package will pickle the object normally when you issue a save_pickle(obj,...)call?",
        "Y": "Package Exporter",
        "Z": "version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s what representation?",
        "Y": "python AST",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Which package automatically finds the Python modules that your code and objects depend on?",
        "Y": "torch.package",
        "Z": "version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "For each module that the dependency resolver finds, you must specify what?",
        "Y": "an action to",
        "Z": "Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What must you do for each module that the dependency resolver finds?",
        "Y": "specify an action to take",
        "Z": "version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the module that the dependency resolver finds?",
        "Y": "intern",
        "Z": "extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the action you can take to put a module into the package?",
        "Y": "intern",
        "Z": "In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the action that removes or changes dependencies in your code that is not technically part oftorch.package?",
        "Y": "Refactoring",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "There is no way to package \u201cjust\u201d a function or class from what?",
        "Y": "module",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What action is allowed to declare a module as an external dependency of the package?",
        "Y": "extern",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are only defined on entire Python modules?",
        "Y": "actions",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does mock do?",
        "Y": "stub out this module",
        "Z": "The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will happen if you depend on this module during package export?",
        "Y": "raise an error",
        "Z": "The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Remove or change the dependencies in your code.",
        "Y": "Refactoring",
        "Z": "The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Actions are only defined on entire what?",
        "Y": "Python modules",
        "Z": "Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Is there a way to package \u201cjust\u201d a function or class from module and leave the rest out?",
        "Y": "no way to package \u201cjust\u201d a function or class from module and leave the rest out",
        "Z": "Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Is it possible to package just a function or class from module and leave the rest out?",
        "Y": "by design",
        "Z": "The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of a module name?",
        "Y": "foo.bar",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When patterns are checked in the order that they were defined, what action will be taken?",
        "Y": "first",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if a module is intern-ed?",
        "Y": "it will be placed into the package",
        "Z": "If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is intern-ed, it will be placed into the package. This action is what?",
        "Y": "your model code, or any related code you want to package",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What module will you need to package a ResNet fromtorchvision?",
        "Y": "torchvision.models.resnet",
        "Z": "This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If Package Importer can't find that module, what will happen?",
        "Y": "an error will be raised",
        "Z": "On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does this ensure?",
        "Y": "eachPackage Importeris isolated from the loading environment",
        "Z": "Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are two examples of modules that will raise an error if you attempt to intern them?",
        "Y": "C extension modules and bytecode modules",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!):",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Refactoring is what action that is not technically part of oftorch.package?",
        "Y": "remove or change the dependencies in your code",
        "Z": "Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When patterns are checked in the order that they were defined, what action is taken?",
        "Y": "first action",
        "Z": "Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is included in the package if a module is intern-ed?",
        "Y": "model code",
        "Z": "If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who will look inside your package for an intern-ed module when your packaged code tries to import an intern-ed module?",
        "Y": "Package Importer",
        "Z": "Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is extern-ed, it will not be packaged. Instead, it will be added to what?",
        "Y": "list of external dependencies",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you find a list of external dependencies for this package?",
        "Y": "on package_exporter.extern_modules",
        "Z": "On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is intern-ed, what happens?",
        "Y": "it will be placed into the package",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "This ensures that what does eachPackage Importeris isolated from?",
        "Y": "eachPackage Importeris isolated from the loading environment",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find",
        "Y": "package import",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if a package import fails to find a module?",
        "Y": "an error will be raised",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you depend on?",
        "Y": "third-party libraries",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Does this ensure that eachPackage Importer is isolated from the loading environment?",
        "Y": "eachPackage Importeris isolated from the loading environment",
        "Z": "On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Only what can be intern-ed?",
        "Y": "Python source modules",
        "Z": "This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of what?",
        "Y": "external dependencies",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!):",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Package Importer will use what to find anextern-ed module?",
        "Y": "default Python importer",
        "Z": "This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will happen if a module isn't found?",
        "Y": "an error will be raised",
        "Z": "If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If any external library changes in a what way, your package may fail to load?",
        "Y": "backwards-incompatible",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is mock-ed, it will what?",
        "Y": "not be packaged",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!):",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is mock-ed, it will not be packaged. Instead a what will be packaged in its place?",
        "Y": "stub module",
        "Z": "On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What module will allow you to retrieve objects from it?",
        "Y": "stub module",
        "Z": "On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the action called?",
        "Y": "your model code, or any related code you want to package",
        "Z": "This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you depend on without having to package them too?",
        "Y": "third-party libraries",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do these kinds of modules need to do?",
        "Y": "be mock-ed or extern-ed",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!):",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Importer use to find anextern-ed module?",
        "Y": "Python importer",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If it can't find that module, what will happen?",
        "Y": "an error will be raised",
        "Z": "On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should be used for code that you \u201cknow\u201d will not be needed in the loaded package?",
        "Y": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package",
        "Z": "On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of code that you \u201cknow\u201d will not be needed in the loaded package?",
        "Y": "initialization/configuration code",
        "Z": "On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you find the list of external dependencies for this package?",
        "Y": "on package_exporter.extern_modules",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!):",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Importer do when a package imports anextern-ed module?",
        "Y": "Package Importerwill use the default Python importer to find that module",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if a package importer can't find a module?",
        "Y": "an error will be raised",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!):",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Code that you \"know\" will not be needed in the loaded package, but you still want to available for use in non-packaged contents. For",
        "Y": "initialization/configuration code",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!):",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is not packaged, it will not be packaged. Instead a stub module will be packaged in its place.",
        "Y": "is mock-ed",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are examples of non-packaged code that mockshould be used for?",
        "Y": "initialization/configuration code, or code only used for debugging/training",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a good practice for writing code with clean dependencies?",
        "Y": "Do not leave unused imports in our code",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if the package importer can't find that module?",
        "Y": "an error will be raised",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should be used for code that you \u201cknow\u201d won't be needed in the loaded package?",
        "Y": "mock",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents. For",
        "Y": "initialization/configuration code",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are some guidelines for writing code with?",
        "Y": "clean dependencies",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused, and will try",
        "Y": "Include only what you use",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is not smart enough to tell that unused imports are indeed unused, and will try to process them?",
        "Y": "dependency resolver",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you do to avoid unused imports?",
        "Y": "Qualify your imports",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Do not leave unused imports in our code?",
        "Y": "Do not leave unused imports in our code",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you want to do with your imports?",
        "Y": "Qualify",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of modules can be packaged independently of one another?",
        "Y": "single-purpose modules",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a code that you know will not be needed in the loaded package?",
        "Y": "initialization/configuration code",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should we not do with unused imports in our code?",
        "Y": "Do not leave unused imports in our code",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you do with your imports?",
        "Y": "Qualify",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the dependency resolver need to do to process unused imports?",
        "Y": "Qualify your imports",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a better way to write import foo and later usefoo.bar.baz?",
        "Y": "writefromfoo.barimportbaz",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is one way to break up large files with unrelated functionality into smaller ones?",
        "Y": "Split up large files",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What contains a hodge-podge of unrelated functionality?",
        "Y": "yourutilsmodule",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Prefer to define what type of modules that can be packaged independently of one another?",
        "Y": "single-purpose modules",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Patterns allow you to specify groups of modules with what?",
        "Y": "convenient syntax",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Segments can be: A what?",
        "Y": "literal string",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What character matches any string, including the empty string?",
        "Y": "wildcard",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "The wildcard matches any string, including what?",
        "Y": "empty string",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "A double wildcard (**) matches against what?",
        "Y": "zero or more complete segments",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of a double wildcard that matches against zero or more complete segments?",
        "Y": "torch",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a ** match?",
        "Y": "matchestorchand all its submodules",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When a Python module is identified as a dependency,torch.packagewalks what module's AST representation?",
        "Y": "python",
        "Z": "When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How does the module use self(i.e. thatPackage Importerinstance) to fulfill future import requests?",
        "Y": "looking in the package",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a name that becomes torch_package_0>.torchvision.models.resnet18",
        "Y": "liketorchvision.models.resnet18",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of a file that becomestorch_package_0>.torchvision/modules/re",
        "Y": "liketorchvision/models/resnet18.py",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can developer-facing details about mangling be found?",
        "Y": "consultmangling.mdintorch/package/",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can future users of a package do to edit the code in order to perform custom modifications to it?",
        "Y": "unzip",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "The importer for packages ensures that code in what module can only be loaded from within the package?",
        "Y": "module can only be loaded from within the package",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Why does the fileextern_modules prevent \u201cimplicit\u201d dependencies where the package runs locally?",
        "Y": "it is importing a locally-installed package",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the exporter look for when source code is added to the package?",
        "Y": "import statements",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is stdout useful for tracking down?",
        "Y": "why certain files get included",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the exporter do to a package?",
        "Y": "Write the package to the filesystem",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What ensures that code in the module can only be loaded from within the package?",
        "Y": "The importer",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the importer do to a package?",
        "Y": "Write the package to the filesystem",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is preferable to use instead of write the package to the filesystem?",
        "Y": "resource guard syntax",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Includemodulein do?",
        "Y": "This will prevent dependency discovery from saving it in the package",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who names match the given glob patterns from the list of modules the package can import?",
        "Y": "Blocklist modules",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if a dependency on any matching packages is found?",
        "Y": "aPackagingErroris raised",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of a string for the names of the modules to be externed?",
        "Y": "my_package.my_subpackage",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How is a glob-style pattern described?",
        "Y": "inmock()",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of a string that specifies the names of the modules to be externald?",
        "Y": "my_package.my_subpackage",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What describes a glob-style pattern?",
        "Y": "inmock()",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Get an id. This id is what?",
        "Y": "guaranteed to only be handed out once for this package",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Specify what?",
        "Y": "modules that should be packaged",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of a string for the names of modules that should be included in the package?",
        "Y": "my_package.my_subpackage",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If an extern module glob pattern is added, andclose() is called (either explicitly or via__exit__) before any modules",
        "Y": "with allow_empty=False",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If an internmodule glob pattern is added with allow_empty=False, andclose()is called (either explicitly or via",
        "Y": "If allow_empty=True",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What must be specified in order to get an id?",
        "Y": "modules that should be packaged",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can include(Union[List[str],str]] \u2013 A string e.g. \u201cmy_package",
        "Y": "glob-style pattern",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If an internmodule glob pattern is added with allow_empty=False, andclose()is called (either explicitly or",
        "Y": "an exception is thrown",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If an intern module glob pattern is added with allow_empty=False, andclose()is called (either explicitly or via_",
        "Y": "If allow_empty=True",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Replace some required modules with a what?",
        "Y": "mock implementation",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How do we copy?",
        "Y": "file-by-file",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Use this function to do what?",
        "Y": "mock this functionality out without having to modify the original code",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If an extern module glob pattern is added, andclose()is called (either explicitly or via__exit__) before any modules",
        "Y": "with allow_empty=False",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Because we copy what, the dependency resolution will sometimes find files that are imported by model files but whose functionality is never used?",
        "Y": "file-by-file",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What function is used to mock modules that are imported by model files but whose functionality is never used?",
        "Y": "include(Union[List[str],str])",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit",
        "Y": "If allow_empty=True",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Registers what?",
        "Y": "an extern hook on the exporter",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "In what order will hooks be called?",
        "Y": "order of registration",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.utils.hooks.RemovableHandle Register?",
        "Y": "an intern hook on the exporter",
        "Z": "A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Which tool saves raw bytes to the package?",
        "Y": "torch.utils.hooks.RemovableHandle",
        "Z": "A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should go it (e.g. \"my_package.my_subpackage\")?",
        "Y": "package(str) \u2013 The name of module package this resource",
        "Z": "The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code for",
        "Y": "resource",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is binary(str)?",
        "Y": "binary(str) \u2013 The data to save",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does binary(str) \u2013 The data to save?",
        "Y": "Save the code formoduleinto the package",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does module_name(str) stand for?",
        "Y": "module_name(str)",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the exporter register on the exporter?",
        "Y": "extern hook",
        "Z": "Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Hooks will be called in what order?",
        "Y": "order of registration",
        "Z": "Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a module match against?",
        "Y": "an intern()pattern",
        "Z": "Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is used to identify it to load?",
        "Y": "resource(str) \u2013 A unique name for the resource",
        "Z": "Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does binary(str) stand for?",
        "Y": "binary(str) \u2013 The data to save",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the code formoduleinto the package?",
        "Y": "Save the code formoduleinto the package",
        "Z": "A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Code will be saved to provide code for this package. dependencies(bool,optional) \u2013 If True, we scan the source for",
        "Y": "module_name(str)",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If True, we scan the source for what?",
        "Y": "dependencies",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What saves a python object to the archive?",
        "Y": "pickle",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the equivalent of pickle to save a python object to the archive?",
        "Y": "totorch.save()",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does not save the code, only the objects. Ifdependenciesis true, this method will also scan the pickled objects for which modules are required",
        "Y": "Stanard pickle",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Ifdependenciesis true, this method will also scan the pickled objects for which modules are required to reconstruct?",
        "Y": "Ifdependenciesis true",
        "Z": "Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can be saved to the archive using pickle?",
        "Y": "python object",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the equivalent of totorch.save()?",
        "Y": "totorch.save()",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does not save the code, only the objects?",
        "Y": "Stanard pickle",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If true, this method will also scan the pickled objects for which modules are required to reconstruct them and save the relevant code.",
        "Y": "Ifdependencies",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Code will be saved to provide code for this package?",
        "Y": "module_name(str)",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a handle that can be used to remove the added hook by?",
        "Y": "callinghandle.remove()",
        "Z": "A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "resource(str) \u2013 A what?",
        "Y": "unique name for the resource",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If true, we scan the source for dependencies. Save a python object to the archive using pickle. Save a",
        "Y": "If True",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does resource(str) stand for?",
        "Y": "A unique name for the resource",
        "Z": "The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Code will be saved to provide code for this package. what is e.g. my_package.my_subpackage?",
        "Y": "module_name(str)",
        "Z": "The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will need to be present in theimporterlist for this to be possible to save objects that have previously been packaged?",
        "Y": "importer\u2019simport_modulemethod",
        "Z": "The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Hooks will be called where?",
        "Y": "in order of registration",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the tool that saves raw bytes to the package?",
        "Y": "torch.utils.hooks.RemovableHandle",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Package(str) \u2013 The name of what module package this resource should go it?",
        "Y": "module",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will need to be present in theimporterlist for this to work?",
        "Y": "importer\u2019simport_modulemethod",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Package(str) \u2013 The name of what package this resource should go in?",
        "Y": "module",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work. package(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. obj(Any) \u2013 The object to save, must be picklable. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is created of sizestepswhose values are evenly spaced from starttoend inclusive?",
        "Y": "one-dimensional tensor",
        "Z": "Random sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.   Constructs a tensor with data.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into a torch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size as input.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning from start.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230a stepend\u2212start\u200b\u230b+1with values from starttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from starttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What type of tensor of sizestepswhose values are evenly spaced from starttoend, inclusive?",
        "Y": "one-dimensional",
        "Z": "Constructs a tensor with data.   Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into a torch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size as input.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning from start.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230a stepend\u2212start\u200b\u230b+1with values from starttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from starttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What type of tensor of sizestepswhose values are evenly spaced from starttoend inclusive?",
        "Y": "one-dimensional tensor",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size as input.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning from start.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230a stepend\u2212start\u200b\u230b+1with values from starttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from starttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with base base.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size as input.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size as inputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the name of the index that returns a new tensor which indexes the input tensor along dimensiondimusing",
        "Y": "a Long Tensor",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the boolean maskmask that returns a new 1-D tensor which indexes the input tensor",
        "Y": "a Bool Tensor",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the name of the index which indexes the input tensor along dimensiondimusing the entries inindex?",
        "Y": "a Long Tensor",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.   Returns a new tensor with a dimension of size one inserted at the specified position.   Splitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.   Stack tensors in sequence vertically (row wise).   Return a tensor of elements selected from eitherxory, depending oncondition.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is returned with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive",
        "Y": "a tensor",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Sets the seed for generating random numbers. Returns the random number generator state as a torch.ByteTensor.",
        "Y": "random number generator state",
        "Z": "Sets the seed for generating random numbers.   Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What does Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)?",
        "Y": "a tensor",
        "Z": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is returned if a tensor is filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)",
        "Y": "a tensor",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution torch.Tensor.normal_()- in-place version oftorch.normal() torch.Tensor.random_()- numbers sampled from the discrete uniform distribution torch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution quasirandom.SobolEngine Thetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What does Alias fortorch.special.erf() do?",
        "Y": "Alias fortorch.special.erfc()",
        "Z": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.   Returns a new tensor with the hyperbolic cosine  of the elements ofinput.   Returns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.   Divides each element of the inputinputby the corresponding element ofother.   Alias fortorch.div().   Computes the logarithmic derivative of the gamma function oninput.   Alias fortorch.special.erf().   Alias fortorch.special.erfc().   Alias fortorch.special.erfinv().   Returns a new tensor with the exponential of the elements of the input tensorinput.   Alias fortorch.special.exp2().   Alias fortorch.special.expm1().   Returns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.   Returns a new tensor with the data ininputfake quantized usingscale,zero_point,quant_minandquant_max.   Alias fortorch.trunc()",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim. Returns the mean",
        "Y": "p-norm of (input-other) Returns the log of summed exponentials",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Computes what quantiles of each row of the input tensor along the dimensiondim?",
        "Y": "q-th",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the result of Computes the q-th quantiles of each row of the input tensor along the dimensiondim",
        "Y": "variant oftorch.quantile()that \u201cignores\u201dNaNvalues",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Ifunbiasedis True, what will be used?",
        "Y": "Bessel\u2019s correction",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns the input tensor. Eliminates all but the first element from every consecutive group of equivalent elements. Ifunbiasedis True,",
        "Y": "unique elements",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Ifunbiasedis True, Bessel\u2019s correction will be used.",
        "Y": "Ifunbiasedis True",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the name of the list of activity groups (CPU, CUDA) to use in profiling?",
        "Y": "activities(iterable)",
        "Z": "Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are the default values for PyTorch Profiler?",
        "Y": "ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is a callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profile",
        "Y": "schedule(callable)",
        "Z": "Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation.",
        "Y": "record_shapes",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Use formula to estimate the FLOPS of specific operators (matrix multiplication and 2D convolution). use_cuda(bool",
        "Y": "with_flops",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "An earlier version of what is considered legacy and will be deprecated?",
        "Y": "API intorch.autogradmodule",
        "Z": "Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are the default values for Profiler context manager?",
        "Y": "ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA",
        "Z": "Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Profile_memory(bool) \u2013 what?",
        "Y": "track tensor memory allocation/deallocation",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "With_flops(bool) \u2013 use formula to estimate what of specific operators (matrix multiplication and 2D convolution)?",
        "Y": "FLOPS",
        "Z": "Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is bool deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the call",
        "Y": "use_cuda",
        "Z": "Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "For more information, see what?",
        "Y": "PyTorch Profiler TensorBoard",
        "Z": "Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Profiler is what?",
        "Y": "context manager",
        "Z": "Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Profiler context manager. Activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling.",
        "Y": "Examples: Using the profiler",
        "Z": "Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Use_cuda(bool) \u2013 Deprecated since version 1.8.1: what?",
        "Y": "useactivitiesinstead",
        "Z": "with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Usetensorboard_trace_handler()to generate result files for what?",
        "Y": "TensorBoard",
        "Z": "with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifie",
        "Y": "profiler action",
        "Z": "schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location;",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number)",
        "Y": "profile_memory",
        "Z": "record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Use formula to estimate FLOPS of specific operators (matrix multiplication and 2D convolution). use_cuda(bool)",
        "Y": "with_flops",
        "Z": "profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What adds a user defined metadata with a string key and a string value into the trace file?",
        "Y": "Using the profiler\u2019sschedule,on_trace_readyandstepfunctions",
        "Z": "schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location;",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "The default schedule simply records all the events continuously for what?",
        "Y": "the duration of the context manager",
        "Z": "Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Save stack traces in a file in what format?",
        "Y": "format suitable for visualization",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What path(str) \u2013 save stacks file to this?",
        "Y": "path(str) \u2013 save stacks file to this",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does record_shapes(bool) do?",
        "Y": "save information about operator\u2019s input shapes",
        "Z": "record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FL",
        "Y": "with_stack",
        "Z": "record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does path(str) do to save stacks file to this location?",
        "Y": "path(str) \u2013 save stacks file to this location",
        "Z": "tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "profile_memory(bool) \u2013 what is profile_memory?",
        "Y": "track tensor memory allocation/deallocation",
        "Z": "profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does path(str) do to save stacks file to?",
        "Y": "path(str) \u2013 save stacks file to this location",
        "Z": "Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does with_flops(bool) do?",
        "Y": "use formula to estimate the FLOPS of specific operators",
        "Z": "with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Deprecated since version 1.8.1: what?",
        "Y": "useactivitiesinstead",
        "Z": "Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "To use shape/stack functionality make sure to set what?",
        "Y": "record_shapes/with_stack",
        "Z": "Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does Useschedule() generate?",
        "Y": "callable schedule",
        "Z": "Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does Usetensorboard_trace_handler generate?",
        "Y": "result files",
        "Z": "Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What should you set to use shape/stack functionality?",
        "Y": "record_shapes/with_stack",
        "Z": "Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What Signals the profiler?",
        "Y": "Signals the profiler",
        "Z": "Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the result file generated for?",
        "Y": "TensorBoard",
        "Z": "Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Signals the profiler that what has happened?",
        "Y": "next profiling step has started",
        "Z": "tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What member returns a callable that can be used as profiler schedule argument?",
        "Y": "CPU CUDA",
        "Z": "tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "The profiler will skip what?",
        "Y": "firstskip_firststeps",
        "Z": "Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What should you set when creating profiler context manager?",
        "Y": "record_shapes/with_stack",
        "Z": "tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What will the profiler skip?",
        "Y": "firstskip_firststeps",
        "Z": "tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "After profiling, what can be found in the specified directory?",
        "Y": "result files",
        "Z": "After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is specified with the repeat parameter?",
        "Y": "optional number of cycles",
        "Z": "tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does --logdirdir_name to see the results in TensorBoard?",
        "Y": "tensorboard",
        "Z": "tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does the optimizer update the parameters based on?",
        "Y": "computed gradients",
        "Z": "torch.optimis a package implementing various optimization algorithms.\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture. To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Optimizers also support specifying what?",
        "Y": "per-parameter options",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a keyword argument useful when you only want to do?",
        "Y": "vary a single option",
        "Z": "torch.optimis a package implementing various optimization algorithms.\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture. To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What model's parameters will use a learning rate of1e-3?",
        "Y": "model.classifier",
        "Z": "torch.optimis a package implementing various optimization algorithms.\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture. To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does torch.optim implement that updates the parameters?",
        "Y": "a step()method",
        "Z": "torch.optimis a package implementing various optimization algorithms.\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture. To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What do you have to do to usetorch.optim?",
        "Y": "construct an optimizer object",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "To construct what you have to give it an iterable containing the parameters (all should beVariables) to optimize?",
        "Y": "an Optimizer",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Instead of passing an iterable ofVariables, what is used to specify per-parameter options?",
        "Y": "pass in an iterable ofdicts",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is this useful when you want to specify?",
        "Y": "per-layer learning rates",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is this useful when you only want to do?",
        "Y": "vary a single option",
        "Z": "Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Some optimization algorithms need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute",
        "Y": "Conjugate Gradient and LBFGS",
        "Z": "You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Instead of passing an iterable ofVariables, what do optimizers use to specify per-parameter options?",
        "Y": "pass in an iterable ofdicts",
        "Z": "Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a keyword argument useful for?",
        "Y": "when you only want to vary a single option, while keeping all others consistent between parameter groups",
        "Z": "You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is this useful when you only want to vary a single option, while keeping all others consistent between parameter groups?",
        "Y": "per-layer learning rates",
        "Z": "You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does iterable oftorch.Tensors ordicts do?",
        "Y": "Specifies what Tensors should be optimized",
        "Z": "In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Option arguments will be used as what in the groups that didn\u2019t override them?",
        "Y": "defaults",
        "Z": "Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is useful when one wants to specify?",
        "Y": "per-layer learning rates",
        "Z": "For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How many ways can a step()method be used?",
        "Y": "two ways",
        "Z": "This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Some optimization algorithms such as Conjugate Gradient and LBFGS need to what?",
        "Y": "reevaluate the function multiple times",
        "Z": "Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What class is used by all optimizers?",
        "Y": "Base class",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Examples of objects that don\u2019t satisfy those properties are what?",
        "Y": "sets and iterators over values of dictionaries",
        "Z": "This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is an iterable oftorch.Tensors ordicts?",
        "Y": "params",
        "Z": "Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "defaults\u2013 (dict): a dict containing what?",
        "Y": "default values of optimization options",
        "Z": "Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Optimizer.add_param_group <sep>",
        "Y": "Add a param group to theOptimizersparam_groups",
        "Z": "Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Sets the gradients of all optimizedtorch.Tensors to zero. Implements Adadelta algorithm. Implements Ad",
        "Y": "Optimizer.zero_grad",
        "Z": "Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Implements what algorithm?",
        "Y": "Adadelta algorithm",
        "Z": "Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm was heavily inspired?",
        "Y": "L-BFGS algorithm",
        "Z": "For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters.",
        "Y": "model.classifier",
        "Z": "This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "When can the function be called?",
        "Y": "once the gradients are computed using e.g.backward()",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What Sets the gradients of all optimizedtorch.Tensors to zero?",
        "Y": "Optimizer.zero_grad",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements the resilient back?",
        "Y": "RMSprop algorithm",
        "Z": "This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is implemented in the resilient backpropagation algorithm?",
        "Y": "resilient backpropagation",
        "Z": "This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a closure that allows algorithms to reevaluate the function multiple times?",
        "Y": "recompute your model",
        "Z": "All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements a lazy version of Adam algorithm suitable for sparse tensors?",
        "Y": "AdamW algorithm",
        "Z": "Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does the optimizer implement?",
        "Y": "resilient backpropagation algorithm",
        "Z": "All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is supported by most optimizers?",
        "Y": "a simplified version",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "The closure should clear the gradients, what should the closure do?",
        "Y": "compute the loss",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are examples of objects that don\u2019t satisfy those properties?",
        "Y": "sets and iterators over values of dictionaries",
        "Z": "Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Params is an iterable oftorch.Tensors ordicts. Specifies what Tensors should",
        "Y": "iterable",
        "Z": "Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does defaults\u2013 (dict) contain?",
        "Y": "default values of optimization options",
        "Z": "Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Optimizer.load_state_dict Loads what?",
        "Y": "optimizer state",
        "Z": "Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer",
        "Y": "Optimizer.state_dict",
        "Z": "Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements stochastic gradient descent?",
        "Y": "resilient backpropagation algorithm",
        "Z": "Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "ReduceLROnPlateauallows what?",
        "Y": "dynamic learning rate",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What Returns the state of the optimizer as adict?",
        "Y": "Optimizer.state_dict",
        "Z": "Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does Adadelta algorithm implement?",
        "Y": "Adagrad algorithm",
        "Z": "Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is implemented (optionally with momentum)?",
        "Y": "stochastic gradient descent",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does defaults- (dict) contain?",
        "Y": "default values of optimization options",
        "Z": "Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Learning rate scheduling should be applied when?",
        "Y": "after optimizer\u2019s update",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What happens when a learning rate scheduler is called back-to-back?",
        "Y": "each scheduler is applied one after the other on the learning rate obtained by the one preceding it",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "In many places in the documentation, we will use what?",
        "Y": "template",
        "Z": "Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is params?",
        "Y": "iterable",
        "Z": "Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does Optimizer.add_param_group implement?",
        "Y": "Adam algorithm",
        "Z": "Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does the param group do?",
        "Y": "Add a param group to theOptimizersparam_groups",
        "Z": "Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is implemented?",
        "Y": "Averaged Stochastic Gradient Descent",
        "Z": "Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "When was the learning rate scheduler expected to be called?",
        "Y": "before the optimizer\u2019s update",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What happens if you use the learning rate scheduler before the optimizer\u2019s update?",
        "Y": "this will skip the first value of the learning rate schedule",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Are you unable to reproduce results after upgrading to PyTorch 1.1.0?",
        "Y": "unable to reproduce results",
        "Z": "Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How did 1.1.0 change the behavior of the learning rate scheduler?",
        "Y": "BC-breaking way",
        "Z": "Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the result of upgrading to PyTorch 1.1.0?",
        "Y": "unable to reproduce results",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.Lamb do?",
        "Y": "lr_scheduler.Lamb",
        "Z": "Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does PyTorch implement?",
        "Y": "resilient backpropagation algorithm",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer's update?",
        "Y": "BC",
        "Z": "Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What will the learning rate scheduler skip if you use the learning rate scheduler before the optimizer's update?",
        "Y": "first value of the learning rate schedule",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What Sets the learning rate of each parameter group to the initial tensors?",
        "Y": "lr_scheduler.LambdaLR",
        "Z": "Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements?",
        "Y": "Adam",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What Sets the learning rate of each parameter group to the initial lr times a given lr times?",
        "Y": "lr_scheduler.LambdaLR",
        "Z": "Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What Sets the learning rate of each parameter group to the initial lr times a given function?",
        "Y": "lr_scheduler.LambdaLR",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is lr_scheduler?",
        "Y": "Multiplicative",
        "Z": "Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Learning rate schedulers can be called back-to-back to refer to what?",
        "Y": "schedulers algorithms",
        "Z": "Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements Optimizer.zero_grad?",
        "Y": "Adadelta algorithm",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What version of Adam algorithm suitable for sparse tensors?",
        "Y": "lazy version",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.LambdaLR stand for?",
        "Y": "MultiplicativeLR",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements the gradients of all optimizedtorch.Tensors to zero?",
        "Y": "Adadelta algorithm",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function?",
        "Y": "lr",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Learning rate scheduling should be applied one after the other on the learning rate obtained by the one preceding it.",
        "Y": "schedulers algorithms",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the",
        "Y": "lr_scheduler.StepLR",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "In what book has SWA been proposed?",
        "Y": "Averaging Weights Leads to Wider Optima and Better Generalization",
        "Z": "torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What class serves to compute the weights of the SWA model?",
        "Y": "AveragedModelclass",
        "Z": "torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How can you create an averaged model?",
        "Y": "running",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is used to update the running averages of the parameters of the model?",
        "Y": "theupdate_parameters()function",
        "Z": "Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate that the following code creates a scheduler that linearly anneals the learning rate from its initial value to?",
        "Y": "0.05 in 5 epochs",
        "Z": "torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does update_bn() assume that each batch in the dataloaderloaderis either a tensors or a list",
        "Y": "each batch in the dataloaderloaderis either a tensors or a list of tensors",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What can you update if your dataloader has a different structure?",
        "Y": "If your dataloader has a different structure",
        "Z": "torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What serves to compute the weights of the SWA model?",
        "Y": "AveragedModelclass",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "The following code creates a learning rate scheduler that linearly anneals the learning rate from its initial value to what?",
        "Y": "0.05",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What can you do if your dataloader has a different structure?",
        "Y": "update the batch normalization statistics",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "By default,torch.optim.swa_utils.AveragedModelcomputes what of the parameters that you provide",
        "Y": "running equal average",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the SWA model that accumulates the averages of the weights?",
        "Y": "swa_model",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How many weights does the model train for?",
        "Y": "300",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are computed if the boolean argumenteigenvectors is False?",
        "Y": "eigenvalues",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What portion of the matrix is used by default?",
        "Y": "upper triangular portion",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is used if the input matrixinputis supposed to be symmetric or Hermitian?",
        "Y": "lower triangular portion",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "In what order are the eigenvalues of each matrix in a batch of matrices returned?",
        "Y": "ascending order",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What happens to the returned matrix regardless of the original strides?",
        "Y": "matrixVwill be transposed",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "How is the matrixVtransposed?",
        "Y": "stridesV.contiguous().transpose(-1, -2).stride()",
        "Z": "This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "When is such operation only stable?",
        "Y": "when all eigenvalues are distinct",
        "Z": "This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What are computed if the argumenteigenvectors is False?",
        "Y": "eigenvalues",
        "Z": "This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "If inputis what, then the eigenvalues of each matrix in the batch are returned in ascending order?",
        "Y": "a batch of matrices",
        "Z": "This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "A parameter that is what?",
        "Y": "not initialized",
        "Z": "Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds submodules in a dictionary. Holds parameters in a dictionary. Holds submodules in a dictionary. Hold",
        "Y": "Holds submodules in a list",
        "Z": "These are the basic building blocks for graphs: torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds parameters in a list. Holds parameters in a dictionary. Holds parameters in a dictionary. Holds parameters in a dictionary",
        "Y": "Holds parameters in a list",
        "Z": "These are the basic building blocks for graphs: torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Registers a forward pre-hook common to all modules. Registers a global forward hook for all the modules Registers a backward hook",
        "Y": "Global Hooks For Module",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Global Hooks For Module Registers a forward pre-hook common to all modules. Registers a what?",
        "Y": "global forward hook",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor. nn.MaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv2d Applies a what convolution over an input signal composed of several input planes?",
        "Y": "2D",
        "Z": "These are the basic building blocks for graphs: torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv3d Applies a what type of convolution over an input signal composed of several input planes?",
        "Y": "3D",
        "Z": "These are the basic building blocks for graphs: torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConvTranspose1d Applies a what transposed convolution operator over an input image composed of several input planes",
        "Y": "1D",
        "Z": "These are the basic building blocks for graphs: torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transposed convolution operator does nn.ConvTranspose2d Applies?",
        "Y": "2D",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.LazyConv2d a torch.nn.Conv2dmodule have?",
        "Y": "lazy initialization of thein_channelsargument of theConv2d",
        "Z": "torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other)?",
        "Y": "torch.nn Containers Convolution Layers Pooling layers Padding Layers",
        "Z": "torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds submodules in a dictionary. Holds parameters in a list. Holds submodules in a dictionary. What",
        "Y": "Holds submodules in a list",
        "Z": "torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds submodules in a dictionary <sep>",
        "Y": "Holds submodules in a dictionary",
        "Z": "A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds parameters in a dictionary. Holds parameters in a dictionary. Holds parameters in a dictionary. Holds parameters in a dictionary",
        "Y": "Holds parameters in a list",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds parameters in a dictionary. Holds parameters in a dictionary. Holds parameters in a dictionary <sep>",
        "Y": "Holds parameters in a dictionary",
        "Z": "torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What convolution does nn.Conv1d apply over an input signal composed of several input planes?",
        "Y": "1D",
        "Z": "torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transposed convolution operator does nn.ConvTranspose2d use?",
        "Y": "2D",
        "Z": "torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transposed convolution operator does nn.ConvTranspose3d use?",
        "Y": "3D",
        "Z": "Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds submodules in a list <sep>",
        "Y": "Holds submodules in a list",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds parameters in a list <sep>",
        "Y": "Holds parameters in a list",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What transposed convolution operator does nn.ConvTranspose1d Applies?",
        "Y": "1D",
        "Z": "Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Normalization Layers Recurrent Layers Transformer Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shu",
        "Y": "Non-linear Activations",
        "Z": "Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is nn.LazyConv1d a torch.nn.Conv1dmodule?",
        "Y": "nn.LazyConv1d a torch.nn.Conv1dmodule",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What module has lazy initialization of thein_channelsargument of theConv2d?",
        "Y": "nn.LazyConv2d a torch.nn.Conv2dmodule",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Global Hooks For Module Registers is what?",
        "Y": "a forward pre-hook common to all modules",
        "Z": "Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What module has lazy initialization of thein_channelsargument of theConv1d?",
        "Y": "nn.LazyConv1d a torch.nn.Conv1dmodule",
        "Z": "Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is not initialized. Base class for all neural network modules. A sequential container. Holds submodules in a list. Holds",
        "Y": "A buffer",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds parameters in a dictionary. Holds submodules in a dictionary. Holds parameters in a dictionary <sep>",
        "Y": "Holds parameters in a dictionary",
        "Z": "Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.ConvTranspose1d Applies over an input image composed of several input planes?",
        "Y": "1D transposed convolution operator",
        "Z": "A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds parameters in a dictionary. Holds parameters in a dictionary <sep>",
        "Y": "Holds parameters in a dictionary",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What module has lazy initialization of thein_channelsargument of theConv2d that is inferred from theinput.",
        "Y": "nn.LazyConv2d a torch.nn.Conv2d",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Global Hooks For Module Registers What is a forward pre-hook common to all modules?",
        "Y": "a forward pre-hook common to all modules",
        "Z": "Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What module with lazy initialization of thein_channelsargument of theConv3d is inferred from theinput?",
        "Y": "nn.LazyConv3d a torch.nn.Conv3d",
        "Z": "Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization?",
        "Y": "DataParallel Layers",
        "Z": "DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization What is the",
        "Y": "nn.LazyConvTranspose",
        "Z": "DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "A kind of Tensor that is to be considered a module parameter. A parameter that is not initialized. A buffer that is not initialized",
        "Y": "Quantized Functions Lazy Modules Initialization",
        "Z": "Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds submodules in a dictionary. Holds parameters in a list. Holds parameters in a dictionary. Holds submodul",
        "Y": "Holds submodules in a list",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv2d Applies what convolution over an input signal composed of several input planes?",
        "Y": "2D",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor. nn.MaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the module that Quantized Functions Lazy Modules Initialization is to be considered a module parameter?",
        "Y": "nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d",
        "Z": "Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds submodules in a list. Holds parameters in a dictionary. Holds submodules in a dictionary. Hold",
        "Y": "Holds submodules in a list",
        "Z": "Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What module has lazy initialization of thein_channelsargument of theConv3d?",
        "Y": "nn.LazyConv3d a torch.nn.Conv3d",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What module has lazy initialization?",
        "Y": "nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d",
        "Z": "Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "A kind of Tensor that is to be considered a what?",
        "Y": "module parameter",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a parameter that is?",
        "Y": "not initialized",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "For all neural network modules. A sequential container. Holds submodules in a list. Holds parameters in a dictionary. Holds",
        "Y": "Base class",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds submodules in a dictionary. Holds parameters in a list. Holds parameters in a dictionary.",
        "Y": "Holds submodules in a dictionary",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What module has lazy initialization of thein_channels?",
        "Y": "nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "A buffer that is not initialized. Base class for all neural network modules. A sequential container. Holds submodules in a list.",
        "Y": "parameter that is not initialized",
        "Z": "A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "A sequential container. Holds submodules in a list. Holds submodules in a dictionary. Holds parameters in",
        "Y": "Base class for all neural network modules",
        "Z": "A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What container holds submodules in a list?",
        "Y": "sequential container",
        "Z": "A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.ConvTranspose2d Applies a what type of transposed convolution operator over an input image composed of several input",
        "Y": "2D",
        "Z": "A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What module with lazy initialization of thein_channelsargument of theConv3d is inferred from theinput.size",
        "Y": "nn.LazyConv3d a torch.nn.Conv3d",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What module has lazy initialization of thein_channelsargument of theConvTranspose1d?",
        "Y": "nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds submodules in a list. Holds parameters in a dictionary. Holds parameters in a dictionary. Holds parameters in",
        "Y": "sequential container",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Global Hooks For Module Registers what?",
        "Y": "a forward pre-hook common to all modules",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor. nn.MaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the nn.LazyConvTranspose2d?",
        "Y": "Ator",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds submodules in a list. Holds parameters in a list. Holds submodules in a dictionary.",
        "Y": "Holds submodules in a list",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor. nn.MaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds parameters in a list. Holds submodules in a dictionary. Holds parameters in a dictionary.",
        "Y": "Holds parameters in a list",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor. nn.MaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds what in a dictionary?",
        "Y": "parameters",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor. nn.MaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of transposed convolution operator over an input image composed of several input planes?",
        "Y": "3D",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule",
        "Y": "lazy initialization of thein_channelsargument of theConvTranspose1d",
        "Z": "Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor. nn.MaxPool1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Conv2d Applies a 2D convolution over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What module with lazy initialization of thein_channelsargument of theConvTranspose1d is inferred from thein",
        "Y": "nn.LazyConvTranspose1d a torch.nn.ConvTranspose1d",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the module with lazy initialization of the nn.LazyConvTranspose3d a torch.nn.Con",
        "Y": "nn.LazyConvTranspose3d a torch.nn.ConvTranspose3d",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from theinput.size(1). nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from theinput.size(1). nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from theinput.size(1). nn.Unfold Extracts sliding local blocks from a batched input tensor. nn.Fold Combines an array of sliding local blocks into a large containing tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.MaxPool1d Apply?",
        "Y": "1D max pooling over an input signal",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.MaxPool2d Applies a 2D max pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.MaxPool3d Applies a 3D max pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "MaxUnpool1d Computes a partial what of MaxPool1d?",
        "Y": "inverse",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of pooling does AdaptiveMaxPool3d apply?",
        "Y": "3D adaptive max",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes. nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes. nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes. nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d. nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d. nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d. nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes. nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes. nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes. nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes. nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes. nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes. nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes. nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What Allows the model to jointly attend to information from different representation subspaces?",
        "Y": "MultiheadAttention",
        "Z": "nn.ELU Applies the element-wise function: nn.Hardshrink Applies the hard shrinkage function element-wise: nn.Hardsigmoid Applies the element-wise function: nn.Hardtanh Applies the HardTanh function element-wise nn.Hardswish Applies the hardswish function, element-wise, as described in the paper: nn.LeakyReLU Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh Applies the element-wise function: nn.Tanhshrink Applies the element-wise function: nn.Threshold Thresholds each element of the input Tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function:",
        "Y": "nn",
        "Z": "nn.ELU Applies the element-wise function: nn.Hardshrink Applies the hard shrinkage function element-wise: nn.Hardsigmoid Applies the element-wise function: nn.Hardtanh Applies the HardTanh function element-wise nn.Hardswish Applies the hardswish function, element-wise, as described in the paper: nn.LeakyReLU Applies the element-wise function: nn.LogSigmoid Applies the element-wise function: nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces. nn.PReLU Applies the element-wise function: nn.ReLU Applies the rectified linear unit function element-wise: nn.ReLU6 Applies the element-wise function: nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: nn.SELU Applied element-wise, as: nn.CELU Applies the element-wise function: nn.GELU Applies the Gaussian Error Linear Units function: nn.Sigmoid Applies the element-wise function: nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise. nn.Mish Applies the Mish function, element-wise. nn.Softplus Applies the element-wise function: nn.Softshrink Applies the soft shrinkage function elementwise: nn.Softsign Applies the element-wise function: nn.Tanh Applies the element-wise function: nn.Tanhshrink Applies the element-wise function: nn.Threshold Thresholds each element of the input Tensor.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a mini-batch of 2D inputs with additional channel dimension?",
        "Y": "4D input",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule?",
        "Y": "nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of thenum_featuresargument of theBatchNorm3d that is inferred from theinput",
        "Y": "nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What input is a mini-batch of [N-2]D inputs with additional channel dimension?",
        "Y": "N-Dimensional",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a mini-batch of 1D inputs with optional additional channel dimension?",
        "Y": "3D input",
        "Z": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What inputs does the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift cover?",
        "Y": "2D or 3D",
        "Z": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lazy initialization of thenum_featuresargument of theBatchNorm1d that is inferred from theinput",
        "Y": "nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "InstanceNorm1d Applies Instance Normalization over a what input?",
        "Y": "3D",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is an example of a mini-batch of 2D inputs with additional channel dimension?",
        "Y": "4D input",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Who Applies Instance Normalization over a 4D input?",
        "Y": "nn.InstanceNorm2d",
        "Z": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from theinput.size(1). nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from theinput.size(1). nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from theinput.size(1). nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization. nn.LayerNorm",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another name for mean absolute error?",
        "Y": "MAE",
        "Z": "nn.L1Loss Creates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy. nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which two classes does nn.CrossEntropyLoss combine?",
        "Y": "LogSoftmaxandNLLLossin",
        "Z": "nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "MultiLabelMarginLoss creates a criterion that optimizes what?",
        "Y": "multi-class multi-classification hinge loss",
        "Z": "This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the mean absolute error?",
        "Y": "MAE",
        "Z": "Creates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy. nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does NLLLoss stand for?",
        "Y": "negative log likelihood loss",
        "Z": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion that uses a criterion that uses a criterion that uses a cri",
        "Y": "nn.HuberLoss",
        "Z": "Creates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy. nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does nn.MSELoss measure between each element in inputxxxand targetyyy?",
        "Y": "mean squared error",
        "Z": "nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "NLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Le",
        "Y": "Gaussian",
        "Z": "This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "If the absolute element-wise error falls below delta, what is a squared term?",
        "Y": "if the absolute element-wise error falls below delta",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is measured between each element in inputxxxand targetyyy?",
        "Y": "mean squared error",
        "Z": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion that measures the mean squared error between each element in the inputxxxand targetyyy?",
        "Y": "nn.SmoothL1Loss",
        "Z": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy. nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "PoissonNLLLoss Negative log likelihood loss with what?",
        "Y": "Poisson distribution of target",
        "Z": "nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a criterion that uses a squared term if the absolute element-wise error falls below delta and an L1 term",
        "Y": "nn.SoftMargin",
        "Z": "nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "This criterion combines what two single classes?",
        "Y": "LogSoftmaxandNLLLossin",
        "Z": "This criterion combinesLogSoftmaxandNLLLossin one single class. nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What loss is nn.CTCLoss?",
        "Y": "Connectionist Temporal Classification loss",
        "Z": "nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121):",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion?",
        "Y": "nn.CosineEmbeddingLoss",
        "Z": "Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the term for Gaussian negative log likelihood loss?",
        "Y": "Gaussian",
        "Z": "nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121):",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "MultiLabelMarginLoss creates a criterion that optimizes what type of hinge loss?",
        "Y": "multi-class multi-classification hinge loss",
        "Z": "nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121):",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "When does nn.SmoothL1Loss create a criterion that uses a squared term?",
        "Y": "if the absolute element-wise error falls below beta and an L1 term otherwise",
        "Z": "nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand input",
        "Y": "nn.SoftMarginLoss",
        "Z": "nn.CTCLoss The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121):",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the loss caused by the Connectionist Temporal Classification loss?",
        "Y": "Connectionist Temporal Classification loss",
        "Z": "The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121):",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "NLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss nn.BCE",
        "Y": "Gaussian",
        "Z": "The Connectionist Temporal Classification loss. nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121):",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "NLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gau",
        "Y": "Poisson",
        "Z": "nn.NLLLoss The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "NLLLoss Gaussian negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss",
        "Y": "Gaussian",
        "Z": "nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the loss that occurs with Poisson distribution of target?",
        "Y": "negative log likelihood loss",
        "Z": "The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "nn.PoissonNLLLoss Negative log likelihood loss with what?",
        "Y": "Poisson distribution of target",
        "Z": "The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tens",
        "Y": "nn.MultiLabelSoftMarginLoss",
        "Z": "The negative log likelihood loss. nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target. nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What creates a criterion that optimizes a multi-label one-versus-all loss?",
        "Y": "nn.MultiLabelSoftMarginLoss",
        "Z": "nn.GaussianNLLLoss Gaussian negative log likelihood loss. nn.KLDivLoss The Kullback-Leibler divergence loss measure nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does the criterion measure between the target and the output?",
        "Y": "Binary Cross Entropy",
        "Z": "Creates a criterion that measures the Binary Cross Entropy between the target and the output: nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What two single classes does the BCEWithLogitsLoss combine?",
        "Y": "aSigmoidlayer and theBCELossin",
        "Z": "nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "This loss combines what two single classes?",
        "Y": "aSigmoidlayer and theBCELossin",
        "Z": "This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "When does a SmoothL1Loss create a criterion that uses a squared term?",
        "Y": "if the absolute element-wise error falls below beta",
        "Z": "This loss combines aSigmoidlayer and theBCELossin one single class. nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1). nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of hinge loss does Multi LabelMarginLoss optimize?",
        "Y": "multi-class multi-classification hinge loss",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1). nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices). nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1). nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C). nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1. nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121): nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000. nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Clips gradient of an iterable of parameters at what?",
        "Y": "specified value",
        "Z": "Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Convert parameters to what?",
        "Y": "one vector",
        "Z": "Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "PruningContainer Container holding a sequence of pruning methods for what?",
        "Y": "iterative pruning",
        "Z": "Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "At what rate are channels in a tensor pruned?",
        "Y": "random",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "LnStructured Prune entire (currently unpruned) channels in a tensor based on their",
        "Y": "Ln-norm",
        "Z": "Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually",
        "Y": "prune.CustomFromMask",
        "Z": "Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Prunes tensor corresponding to parameter callednameinmoduleby removing what (currently unpruned) units selected at",
        "Y": "specifiedamountof",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof",
        "Y": "L1-norm",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "How many channels are currently unpruned?",
        "Y": "specifiedamountof",
        "Z": "Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizationstutorial\nfor more information on how to implement your own parametrizations. parametrize.register_parametrization Adds a parametrization to a tensor in a module. parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module. parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization(). parametrize.is_parametrized ReturnsTrueif module has an active parametrization. parametrize.ParametrizationList",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which tensor corresponding to parameter callednameinmodule is prune.random_structured Prunes tensor?",
        "Y": "prune.ln_structured Prunes tensor",
        "Z": "From thetorch.nn.utilsmodule   Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does not prune any units but generates the pruning parametrization with a mask of ones?",
        "Y": "prune.Identity Utility pruning method",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lowest L1Unstructured Prune units in a tensor by zeroing out the ones with the lowest?",
        "Y": "L1-norm",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is removed from current unpruned channels along the specifieddimselected at random?",
        "Y": "specifiedamountof",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is prune.global_unstructured Prune?",
        "Y": "prune.global_unstructured",
        "Z": "Clips gradient norm of an iterable of parameters.   Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What gradient of an iterable of parameters at specified value. Convert parameters to one vector Convert one vector to the parameters prune.BaseP",
        "Y": "Clips",
        "Z": "Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unprune",
        "Y": "prune.ln_structured",
        "Z": "Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the tree that prunes tensors corresponding to tensors?",
        "Y": "global_unstructured",
        "Z": "Clips gradient of an iterable of parameters at specified value.   Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Convert parameters to how many vectors?",
        "Y": "one vector",
        "Z": "Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "LnStructured Prune entire (currently unpruned) channels in a tensor based on what",
        "Y": "Ln-norm",
        "Z": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the lowest L1-norm?",
        "Y": "L1-norm",
        "Z": "Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Which Prunes tensor corresponding to parameter callednameinmodule by removing the specifiedamountof (currently unprune",
        "Y": "prune.ln_structured",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the tree that globally prunes tensors corresponding to all parameters inparameters?",
        "Y": "prune.global_unstructured",
        "Z": "Convert parameters to one vector   Convert one vector to the parameters prune.BasePruningMethod Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Abstract base class for what?",
        "Y": "creation of new pruning techniques",
        "Z": "Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "PruningContainer holds a sequence of pruning methods for what?",
        "Y": "iterative pruning",
        "Z": "Abstract base class for creation of new pruning techniques. prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning. prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random. prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random. prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does prune.customFromMask prune.identity Applies to the tensor corresponding to the parameter callednameinmodul",
        "Y": "pruning reparametrization",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm. prune.CustomFromMask  prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units. prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random. prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm. prune.random_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimselected at random. prune.ln_structured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) channels along the specifieddimwith the lowest L``n``-norm. prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method. prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask. prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook. prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod.   Applies weight normalization to a parameter in the given module.   Removes the weight normalization reparameterization from a module.   Applies spectral normalization to a parameter in the given module.   Removes the spectral normalization reparameterization from a module. Parametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization(). parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    }
]